"def create_string_buffer(init, size=None):
    # Check if the initializer `init` is of type `bytes`
    if isinstance(init, bytes):
        # If `size` is not provided, calculate the buffer size as the length of `init` + 1 for null terminator
        if size is None:
            size = len(init) + 1
        
        # Perform a system audit for security with event type `ctypes.create_string_buffer`
        _sys.audit(""ctypes.create_string_buffer"", init, size)
        
        # Define a new character buffer type with the determined size
        buftype = c_char * size
        
        # Create an instance of the buffer
        buf = buftype()
        
        # Set the buffer's value to the initial `bytes` data
        buf.value = init
        
        # Return the initialized buffer
        return buf
    
    # Check if the initializer `init` is of type `int`
    elif isinstance(init, int):
        # Perform a system audit for security with event type `ctypes.create_string_buffer`
        _sys.audit(""ctypes.create_string_buffer"", None, init)
        
        # Define a new character buffer type with the size specified by `init`
        buftype = c_char * init
        
        # Create and return an instance of the buffer
        buf = buftype()
        return buf
    
    # Raise a TypeError if `init` is neither `bytes` nor `int`
    raise TypeError(init)
","The create_string_buffer function is a utility provided by the Python ctypes library to create mutable character buffers (arrays of type c_char). It allows creating these buffers either from an existing bytes object or by specifying a fixed size as an integer. When initialized with a bytes object, the function ensures the buffer size accommodates the data and includes an extra null byte for C-style string termination. If an integer is provided, it initializes an empty buffer of the specified size. The function raises a TypeError if the input type is neither bytes nor int. It also performs a system audit for security and compliance checks."
"def CFUNCTYPE(restype, *argtypes, **kw):   
    # Default flag for CDECL calling convention
    flags = _FUNCFLAG_CDECL

    # Check if ""use_errno"" keyword argument is set; modify flags accordingly
    if kw.pop(""use_errno"", False):
        flags |= _FUNCFLAG_USE_ERRNO

    # Check if ""use_last_error"" keyword argument is set; modify flags accordingly
    if kw.pop(""use_last_error"", False):
        flags |= _FUNCFLAG_USE_LASTERROR

    # Raise an error if there are any unexpected keyword arguments left
    if kw:
        raise ValueError(""unexpected keyword argument(s) %s"" % kw.keys())

    # Try to fetch a cached function prototype for the given signature and flags
    try:
        return _c_functype_cache[(restype, argtypes, flags)]
    except KeyError:
        # If not cached, proceed to create a new prototype
        pass

    # Define a dynamic class representing the C function prototype
    class CFunctionType(_CFuncPtr):
        # Set the argument types for the function
        _argtypes_ = argtypes
        # Set the return type for the function
        _restype_ = restype
        # Set the calling convention flags
        _flags_ = flags

    # Cache the newly created prototype for future reuse
    _c_functype_cache[(restype, argtypes, flags)] = CFunctionType

    # Return the dynamically created function prototype
    return CFunctionType","The CFUNCTYPE function is a factory for creating function prototypes in the Python ctypes library. It dynamically defines C-style function pointers based on a specified return type (restype) and argument types (argtypes). It can also accept keyword arguments (kw) to modify behavior, such as enabling use_errno or use_last_error. Internally, it uses caching to avoid recreating prototypes for the same signature and flags. The generated prototype can be used to wrap or call foreign functions, create callable C functions from Python callables, or interact with shared libraries and COM methods. The function raises a ValueError for unexpected keyword arguments."
"def _check_size(typ, typecode=None):
    # Import the calcsize function from the `struct` module to calculate expected size
    from struct import calcsize
    
    # If no typecode is provided, use the `_type_` attribute of the ctypes type
    if typecode is None:
        typecode = typ._type_
    
    # Get the actual size of the ctypes type using `sizeof`
    actual = sizeof(typ)
    
    # Calculate the required size based on the typecode using `struct.calcsize`
    required = calcsize(typecode)
    
    # Compare the actual size and the required size
    if actual != required:
        # Raise an error if there is a mismatch
        raise SystemError(
            ""sizeof(%s) wrong: %d instead of %d"" % (typ, actual, required)
        )","The _check_size function verifies the memory size of a given ctypes type against the expected size calculated using Python's struct module. It ensures that the sizeof value for a ctypes type matches the value returned by struct.calcsize for a corresponding typecode. This provides a safeguard against potential misconfigurations or inconsistencies in the underlying libffi library, which ctypes relies on for interfacing with C libraries. If a mismatch is found, the function raises a SystemError indicating the discrepancy."
"def _reset_cache():
    # Clear the cache for pointer type information
    _pointer_type_cache.clear()
    
    # Clear the cache for function type information
    _c_functype_cache.clear()
    
    # If running on Windows, clear the Windows-specific function type cache
    if _os.name == ""nt"":
        _win_functype_cache.clear()
    
    # Restore the default behavior for converting wide-character pointers (c_wchar_p)
    # When passing a `POINTER(c_wchar)` as a parameter, use the behavior of `c_wchar_p.from_param`
    POINTER(c_wchar).from_param = c_wchar_p.from_param
    
    # Restore the default behavior for converting character pointers (c_char_p)
    # When passing a `POINTER(c_char)` as a parameter, use the behavior of `c_char_p.from_param`
    POINTER(c_char).from_param = c_char_p.from_param
    
    # Explicitly map `None` to `c_void_p` in the pointer type cache
    # This ensures that passing `None` as a pointer translates to a void pointer
    _pointer_type_cache[None] = c_void_p
","The _reset_cache function is used internally to reset various caches and update type mappings in the Python ctypes library. It clears caches that store ctypes function and pointer type information (_pointer_type_cache, _c_functype_cache, and _win_functype_cache on Windows). Additionally, it restores default behavior for pointer type conversion by reassigning methods that determine how c_char and c_wchar pointers are processed. Finally, it explicitly maps None to the c_void_p type in the pointer type cache, maintaining compatibility with functions that expect void pointers."
"def create_unicode_buffer(init, size=None):
    if isinstance(init, str):
        # Handle the case where the input is a string
        if size is None:
            # Calculate buffer size based on the size of `c_wchar`
            if sizeof(c_wchar) == 2:
                # For UTF-16, account for surrogate pairs (2 `wchar_t` per non-BMP character)
                # Add 1 for the trailing null character
                size = sum(2 if ord(c) > 0xFFFF else 1 for c in init) + 1
            else:
                # For UTF-32, each character is 1 `wchar_t`; add 1 for the null terminator
                size = len(init) + 1

        # Audit the creation of the Unicode buffer for security/logging purposes
        _sys.audit(""ctypes.create_unicode_buffer"", init, size)

        # Create a ctypes `c_wchar` array type of the specified size
        buftype = c_wchar * size
        # Initialize the buffer with the string
        buf = buftype()
        buf.value = init  # Assign the string to the buffer
        return buf

    elif isinstance(init, int):
        # Handle the case where the input is an integer (specifying buffer size)
        _sys.audit(""ctypes.create_unicode_buffer"", None, init)

        # Create a ctypes `c_wchar` array type of the specified size
        buftype = c_wchar * init
        buf = buftype()  # Create the buffer (uninitialized)
        return buf

    # Raise an error if the input type is invalid
    raise TypeError(init)
","The create_unicode_buffer function is part of the Python ctypes library, designed to create mutable buffers for storing Unicode strings (wchar_t arrays). It supports two main use cases: initializing a buffer from a string or allocating a buffer of a specific size in wchar_t units. When a string is provided, the function calculates the appropriate buffer size to accommodate the string, taking into account the encoding of wide characters (UTF-16 or UTF-32) and adding space for a null terminator. If an integer is provided, it creates an uninitialized buffer of the specified size. The function ensures proper auditing and raises a TypeError if the input type is invalid."
"def SetPointerType(pointer, cls):
    import warnings

    # Issue a deprecation warning for this function
    warnings._deprecated(""ctypes.SetPointerType"", remove=(3, 15))
    
    # Check if the new type is already present in the cache
    if _pointer_type_cache.get(cls, None) is not None:
        raise RuntimeError(""This type already exists in the cache"")
    
    # Verify that the pointer is in the cache; raise an error if not found
    if id(pointer) not in _pointer_type_cache:
        raise RuntimeError(""What's this???"")
    
    # Update the pointer object to use the new type
    pointer.set_type(cls)
    
    # Add the new type to the pointer type cache
    _pointer_type_cache[cls] = pointer
    
    # Remove the old type mapping from the cache
    del _pointer_type_cache[id(pointer)]
","The SetPointerType function modifies the type associated with a ctypes pointer object and updates the internal pointer type cache. It is primarily used to redefine the behavior of an existing pointer type by associating it with a new class. The function first checks if the new type is already in the cache, raising a RuntimeError if it exists, to prevent conflicts. It also validates the provided pointer by ensuring it is in the cache. The pointer's type is updated, and the cache is updated accordingly by removing the old mapping and adding the new one. This function is deprecated as of Python 3.15 and should be used cautiously."
"def DllGetClassObject(rclsid, riid, ppv):
    try:
        # Attempt to import the in-process server implementation from comtypes
        ccom = __import__(""comtypes.server.inprocserver"", globals(), locals(), ['*'])
    except ImportError:
        # Return CLASS_E_CLASSNOTAVAILABLE if the module cannot be imported
        return -2147221231  # COM error code indicating the class is not available
    else:
        # Delegate the request to the DllGetClassObject function in the inprocserver module
        return ccom.DllGetClassObject(rclsid, riid, ppv)
","The DllGetClassObject function is a COM server entry point typically used by in-process COM servers. It facilitates the creation of a class factory object, which can then create instances of a specified COM class. The function imports the comtypes.server.inprocserver module to handle the request. If the module is unavailable, it returns the standard COM error code CLASS_E_CLASSNOTAVAILABLE (-2147221231), indicating that the requested class cannot be provided. Otherwise, it delegates the handling of the request to the DllGetClassObject function in the inprocserver module."
"def DllCanUnloadNow():
    try:
        # Attempt to import the in-process server implementation from comtypes
        ccom = __import__(""comtypes.server.inprocserver"", globals(), locals(), ['*'])
    except ImportError:
        # Return S_OK if the module cannot be imported, indicating the server can be unloaded
        return 0  # S_OK
    # Delegate the unload check to the DllCanUnloadNow function in the inprocserver module
    return ccom.DllCanUnloadNow()
","The DllCanUnloadNow function determines whether an in-process COM server can be unloaded from memory. It is a standard COM entry point used by COM clients to check if the server has no active objects or references. The function tries to import the comtypes.server.inprocserver module, which contains the implementation of DllCanUnloadNow. If the module is not available, it returns S_OK (0), signaling that the server can be unloaded. Otherwise, it delegates the call to the DllCanUnloadNow function in the imported module.

"
"def _last_version(libnames, sep):
    def _num_version(libname):
        # Split the library name into parts using the specified separator
        parts = libname.split(sep)
        nums = []
        try:
            # Extract and convert the numeric components, starting from the end
            while parts:
                nums.insert(0, int(parts.pop()))
        except ValueError:
            # Ignore parts that cannot be converted to integers
            pass
        # If no numeric parts are found, return a high default value ([maxsize])
        return nums or [maxsize]

    # Find the library name with the maximum version using the custom key function
    return max(reversed(libnames), key=_num_version)","The _last_version function identifies the ""latest version"" of a library from a list of library names based on version numbers embedded in their filenames. It takes two arguments: a list of library names and a separator string (sep) used to split the library names into parts. Internally, it defines a helper function _num_version that extracts and converts version components from the library name into a list of integers, which is then used to determine the order of versions. The function returns the library name with the highest version number, using Python's max with a custom key function."
"def get_ld_header(p):
    # Initialize the ld_header variable to store the matching line
    ld_header = None
    
    # Iterate over each line in the process's standard output
    for line in p.stdout:
        # Check if the line starts with a relative or absolute path
        if line.startswith(('/', './', '../')):
            ld_header = line  # Found a possible ld header
        # If the line contains the word ""INDEX"", return the last found ld header
        elif ""INDEX"" in line:
            return ld_header.rstrip('\n')  # Return the path-like line, stripping trailing newline
        
    # If no matching line is found, return None
    return None","The get_ld_header function reads the output of a process (typically from the standard output stdout) and attempts to extract the first line that starts with a path indicator (/, ./, ../), which is treated as the ""ld header"". The function iterates through each line of the output, identifying the line that contains the path. It returns this line, stripped of trailing newlines, once it encounters a line containing the word ""INDEX"", signaling the end of the relevant section. If no such lines are found, it returns None."
"def get_ld_header_info(p):
    # Initialize the list to store lines that start with a digit
    info = []
    
    # Iterate over each line in the process's standard output
    for line in p.stdout:
        # Check if the line starts with a digit
        if re.match(""[0-9]"", line):
            info.append(line)  # Add the matching line to the list
        else:
            # If a blank line (separator) is encountered, break the loop
            break
    
    # Return the list of lines that match the pattern
    return info","The get_ld_header_info function extracts lines from the standard output of a process that begin with a digit, assuming these lines represent information related to known paths, archives, or members in the context of the ld header. The function processes each line from the output and appends those that start with a digit to a list called info. The iteration stops when a blank line (separator) is encountered. The function then returns the list of lines that match the digit-starting pattern, which can be used to gather specific details about paths or archives relevant to the header."
"def get_ld_headers(file):
    # Initialize an empty list to store the tuples of ld_header and ld_header_info
    ldr_headers = []
    
    # Create a subprocess to run the 'dump' command with the specified arguments
    # The subprocess will capture the output and suppress stderr
    p = Popen([""/usr/bin/dump"", f""-X{AIX_ABI}"", ""-H"", file],
              universal_newlines=True, stdout=PIPE, stderr=DEVNULL)
    
    # Continuously extract headers and associated info using the get_ld_header function
    # Stop when no more headers are found
    while ld_header := get_ld_header(p):
        # Append the tuple (ld_header, ld_header_info) to the list
        ldr_headers.append((ld_header, get_ld_header_info(p)))
    
    # Close the standard output of the subprocess
    p.stdout.close()
    
    # Wait for the subprocess to complete
    p.wait()
    
    # Return the list of (ld_header, ld_header_info) tuples
    return ldr_headers
","The get_ld_headers function is designed to parse the header section of an executable or archive file. It does this by invoking the /usr/bin/dump command with specific options to retrieve the file's header information. The function iterates over the output of the dump -H command, extracting each ""ld header"" (path-like lines) along with related information (lines starting with digits). The headers and associated information are stored as tuples in a list, which is then returned. The process stops when no more headers are found. This function is useful for analyzing executable or archive files, especially in environments using the AIX ABI.
"
"def get_shared(ld_headers):
    # Initialize an empty list to store the shareable objects
    shared = []
    
    # Iterate over each (line, _) tuple in the ld_headers list
    for (line, _) in ld_headers:
        # Check if the line contains the character ""["", indicating a potential shareable object
        if ""["" in line:
            # Extract the portion of the line from the ""["" to the last character before ""]""
            # Strip off the trailing colon (:) by using slicing
            shared.append(line[line.index(""[""):-1])
    
    # Return the list of shareable objects
    return shared
","The get_shared function extracts shareable objects from a list of ld_headers, which are typically parsed from the output of the dump -H command. It searches for lines containing the character [ (which is used to denote shareable objects in the dump -H output), and for each matching line, it extracts the substring between [ and the final ]. This allows the function to capture the paths or names of shareable objects while leaving the bracketed information intact. The function returns a list of these extracted shareable objects."
"def get_one_match(expr, lines):
    # Prepare the regular expression pattern to match the expression within square brackets
    # 'expr' will be matched between ""["" and ""]""
    expr = rf'\[({expr})\]'
    
    # Search each line for matches of the expression
    # Use filter and re.search to generate a list of matches
    matches = list(filter(None, (re.search(expr, line) for line in lines)))
    
    # If there is exactly one match, return the captured group (inside the brackets)
    if len(matches) == 1:
        return matches[0].group(1)
    else:
        # If there are no matches or multiple matches, return None
        return None
","The get_one_match function is designed to search through a list of strings (lines) for a single match of a regular expression (expr). The match must be enclosed in square brackets ([ and ]). If exactly one match is found, the function returns the matched string, stripped of the enclosing brackets. If there are no matches or multiple matches, the function returns None. This is particularly useful when searching for a unique item in a structured text output, where the item is expected to be uniquely identified within square brackets."
"def get_legacy(members):
    # Check if the system architecture is 64-bit (AIX_ABI is assumed to be defined elsewhere)
    if AIX_ABI == 64:
        # For AIX 64-bit binaries, look for legacy member names: shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'
        member = get_one_match(expr, members)  # Search for the pattern in the members list
        if member:  # If a match is found, return the member name
            return member
    else:
        # For 32-bit binaries, the legacy names are shr.o and shr4.o
        # We prefer shr.o, so check for it first
        for name in ['shr.o', 'shr4.o']:
            member = get_one_match(re.escape(name), members)  # Search for each name in the list
            if member:  # If a match is found, return the member name
                return member
    
    # If no legacy member name is found, return None
    return None
","The get_legacy function is used to identify legacy naming schemes for shared library member names, specifically for AIX systems. It distinguishes between 32-bit and 64-bit binaries and looks for specific member names (e.g., shr.o for 32-bit and shr4.o, shr_64.o, or shr4_64.o for 64-bit binaries) within a list of members. The function first checks if the system is running with a 64-bit ABI (AIX_ABI == 64) and searches for relevant 64-bit member names. If the system is running with a 32-bit ABI, it checks for the shr.o name first, followed by shr4.o. The function returns the appropriate member name if found, or None if no match is found."
"def get_version(name, members):
    # Define regular expressions to match versioned library names in both standard and AIX-specific formats
    exprs = [
        rf'lib{name}\.so\.[0-9]+[0-9.]*',        # Matches libFOO.so.X, libFOO.so.X.Y, libFOO.so.X.Y.Z
        rf'lib{name}_?64\.so\.[0-9]+[0-9.]*'    # Matches libFOO_64.so.X, libFOO64.so.X, etc.
    ]
    
    # Iterate over the regular expressions to find matching library members
    for expr in exprs:
        versions = []  # List to store found versions
        for line in members:
            m = re.search(expr, line)  # Search for the version pattern in each member name
            if m:
                versions.append(m.group(0))  # If a match is found, add it to the versions list
        
        # If versions were found, return the highest version
        if versions:
            return _last_version(versions, '.')
    
    # If no versions were found, return None
    return None
","The function get_version is designed to extract and return the highest version number from a list of member names, which typically represent shared library files (e.g., .so files) in the context of GNU and AIX naming conventions. The function checks for versions following the standard GNU versioning (such as libFOO.so.X, libFOO.so.X.Y, libFOO.so.X.Y.Z) and AIX-specific variations that include the 64 suffix in 64-bit libraries (e.g., libFOO_64.so.X.Y.Z)."
"def get_legacy(members):
    # Check if the system architecture is 64-bit (AIX_ABI is assumed to be defined elsewhere)
    if AIX_ABI == 64:
        # For AIX 64-bit binaries, look for legacy member names: shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'  # Regular expression to match shr64.o, shr_64.o, or shr4_64.o
        member = get_one_match(expr, members)  # Search for the pattern in the members list
        if member:  # If a match is found, return the member name
            return member
    else:
        # For 32-bit binaries, the legacy names are shr.o and shr4.o
        # We prefer shr.o, so check for it first
        for name in ['shr.o', 'shr4.o']:
            member = get_one_match(re.escape(name), members)  # Search for each name in the list
            if member:  # If a match is found, return the member name
                return member
    
    # If no legacy member name is found, return None
    return None
","The function get_legacy is designed to identify legacy library member names, particularly for AIX systems. It handles the differences in naming conventions for 32-bit and 64-bit binaries, where 32-bit libraries might use shr.o and shr4.o as member names, while 64-bit libraries might use names like shr64.o, shr_64.o, or shr4_64.o. The function searches through a list of member names to find the appropriate legacy name based on the system architecture (32-bit or 64-bit)."
"def get_libpaths():
    # Check for the LD_LIBRARY_PATH environment variable, or fall back to LIBPATH
    libpaths = environ.get(""LD_LIBRARY_PATH"")
    if libpaths is None:
        libpaths = environ.get(""LIBPATH"")
    if libpaths is None:
        libpaths = []  # If neither is defined, start with an empty list
    else:
        libpaths = libpaths.split("":"")  # Split colon-separated paths into a list

    # Get loader header information from the executable
    objects = get_ld_headers(executable)  # This function is assumed to extract header info

    for (_, lines) in objects:
        for line in lines:
            # The second (optional) argument in the header is the path, if it includes a ""/""
            path = line.split()[1]
            if ""/"" in path:  # Only add to libpaths if the path is absolute
                libpaths.extend(path.split("":""))  # Split colon-separated paths and append

    # Return the combined list of library paths
    return libpaths
","The get_libpaths function is designed to extract library paths on an AIX system based on information stored in the executable's ""loader header."" The function uses /usr/bin/dump -H to extract loader header information, which contains paths to libraries that the executable will use. These paths are then combined with the LD_LIBRARY_PATH or LIBPATH environment variables, depending on which is available, and returned as a list of library paths. The goal is to mimic the behavior of dlopen() in AIX, which resolves libraries based on these paths."
"def get_legacy(members):
    # Check if the system architecture is 64-bit (AIX_ABI is assumed to be defined elsewhere)
    if AIX_ABI == 64:
        # For AIX 64-bit binaries, look for legacy member names: shr64.o, shr_64.o, or shr4_64.o
        expr = r'shr4?_?64\.o'  # Regular expression to match shr64.o, shr_64.o, or shr4_64.o
        member = get_one_match(expr, members)  # Search for the pattern in the members list
        if member:  # If a match is found, return the member name
            return member
    else:
        # For 32-bit binaries, the legacy names are shr.o and shr4.o
        # We prefer shr.o, so check for it first
        for name in ['shr.o', 'shr4.o']:
            member = get_one_match(re.escape(name), members)  # Search for each name in the list
            if member:  # If a match is found, return the member name
                return member
    
    # If no legacy member name is found, return None
    return None
","The get_legacy function is designed to identify and return legacy shared library member names, specifically for AIX systems. These legacy names were introduced in AIX4 to support shared libraries with distinct naming schemes for different binary architectures (32-bit vs. 64-bit). This function helps locate the appropriate member name in the library archive (.a) based on system architecture."
"def find_library(name):
    # Retrieve the library search paths (either LD_LIBRARY_PATH or LIBPATH)
    libpaths = get_libpaths()

    # Search for an archive with a suitable member using find_shared
    (base, member) = find_shared(libpaths, name)
    if base is not None:
        # If a suitable archive member is found, return it in the format ""archive(member)""
        return f""{base}({member})""

    # If no suitable archive member is found, look for a .so file
    soname = f""lib{name}.so""
    for dir in libpaths:
        # Skip the /lib directory, as it is a symbolic link to /usr/lib
        if dir == ""/lib"":
            continue
        shlib = path.join(dir, soname)
        if path.exists(shlib):
            # If a .so file is found, return the .so file name
            return soname

    # If no matching library or .so file is found, return None
    return None
","The find_library function is an AIX-specific implementation of ctypes.util.find_library(), used to locate shared libraries for dynamic linking (such as those used by dlopen). It first attempts to find a suitable member in an archive (.a file), and if it can't find one, it looks for a shared object file (.so). This function is particularly useful for handling the two different library formats on AIX: the archive format with .a extensions and the shared object format with .so extensions."
"def _other_endian(typ):
    # Check for the '_OTHER_ENDIAN' attribute (present if 'typ' is a primitive type)
    if hasattr(typ, _OTHER_ENDIAN):
        return getattr(typ, _OTHER_ENDIAN)

    # If 'typ' is an array, recursively handle the element type
    if isinstance(typ, _array_type):
        # Get the type of elements and handle each element in the array
        return _other_endian(typ._type_) * typ._length_

    # If 'typ' is a structure or union, return it as-is (as these may already handle endianness for fields)
    if issubclass(typ, (Structure, Union)):
        return typ
    
    # Raise an error if 'typ' does not support endianness conversion
    raise TypeError(f""This type does not support other endian: {typ}"")
","The _other_endian function is designed to return the type of a given value, but with the byte order swapped to the ""other"" endianness. It can handle simple types (e.g., integers), arrays, and structures or unions. This is useful when dealing with systems that might use different endianness (byte order), allowing you to convert types for compatibility across platforms."
"def __setattr__(self, attrname, value):
    # Check if the attribute being set is '_fields_'
    if attrname == ""_fields_"":
        # Create a new list to hold the processed field definitions
        fields = []
        
        # Iterate over the list of field descriptions (e.g., ('field1', c_int))
        for desc in value:
            name = desc[0]  # The field name
            typ = desc[1]   # The field type
            rest = desc[2:] # Any additional descriptors (e.g., default values)
            
            # Modify the type to its ""other endian"" version
            fields.append((name, _other_endian(typ)) + rest)
        
        # Set 'value' to the new list of processed fields
        value = fields
    
    # Set the attribute using the parent class's __setattr__
    super().__setattr__(attrname, value)","The __setattr__ method you've provided is a custom implementation of the attribute setter method for a class. This method is typically used in Python to control how an attribute is set when it is assigned a value. In your case, the method handles setting the _fields_ attribute, which is common in ctypes.Structure classes.
"
"def round_down(n, multiple):
    # Assert that the number n is non-negative
    assert n >= 0
    
    # Assert that multiple is positive
    assert multiple > 0
    
    # Perform integer division (round down n to the nearest multiple of 'multiple')
    return (n // multiple) * multiple","The round_down function takes two arguments: n, the number to be rounded, and multiple, the value to which n will be rounded down. The function rounds down the number n to the nearest multiple of multiple. It uses integer division (//) to divide n by multiple, truncating any remainder, and then multiplies the result by multiple to get the rounded-down value. The function also includes assertions to ensure that n is non-negative and that multiple is positive."
"def round_up(n, multiple):
    # Assert that the number n is non-negative
    assert n >= 0
    
    # Assert that multiple is positive
    assert multiple > 0
    
    # Add (multiple - 1) to n to ensure rounding up, and then perform integer division
    return ((n + multiple - 1) // multiple) * multiple","The round_up function rounds the number n up to the nearest multiple of multiple. It ensures that n is non-negative and multiple is positive by using assertions. The function works by adding multiple - 1 to n before performing integer division. This ensures that any remainder in n is accounted for, pushing the result up to the next multiple of multiple. The function then multiplies the result by multiple to get the rounded-up value."
"def BUILD_SIZE(bitsize, offset):
    # Assert that the offset is non-negative
    assert 0 <= offset, offset
    
    # Assert that the offset is less than or equal to 0xFFFF (65535)
    assert offset <= 0xFFFF, offset
    
    # Assert that the bitsize is positive (we don't support zero-length bitfields)
    assert bitsize > 0, bitsize
    
    # Encode the bitsize and offset into a single integer
    result = (bitsize << 16) + offset
    
    # Assert that the bitsize extracted from the result matches the original bitsize
    assert bitsize == NUM_BITS(result), (bitsize, result)
    
    # Assert that the offset extracted from the result matches the original offset
    assert offset == LOW_BIT(result), (offset, result)
    
    # Return the encoded result
    return result","The BUILD_SIZE function constructs a value that encodes both the size of a bitfield (bitsize) and its offset (offset). The function first validates the offset to ensure it lies within the range of 0 to 0xFFFF, inclusive, and that the bitsize is positive (since a zero-length bitfield is not supported). The result is created by shifting the bitsize left by 16 bits and then adding the offset. After constructing this encoded value, it performs assertions to check that the size and offset extracted from the result match the original values, ensuring the encoding is correct. The function returns the encoded result as a single integer."
"def build_size(bit_size, bit_offset, big_endian, type_size):
    # Check if the data is stored in big-endian format
    if big_endian:
        # For big-endian, adjust the offset: subtract bit_offset and bit_size from the total size (8 * type_size)
        return BUILD_SIZE(bit_size, 8 * type_size - bit_offset - bit_size)
    
    # For little-endian, use the provided bit_offset directly
    return BUILD_SIZE(bit_size, bit_offset)","The build_size function constructs an encoded size based on the bit_size, bit_offset, endianness (big_endian), and type_size. It adjusts the bit_offset depending on whether the data is stored in big-endian or little-endian format. For big-endian format, the function calculates the offset by subtracting the sum of the bit_offset and bit_size from the total size of the type (i.e., 8 * type_size). If the data is little-endian, the function directly uses the provided bit_offset. The result is then passed to the BUILD_SIZE function to encode both the bit_size and adjusted bit_offset."
"def get_layout(cls, input_fields, is_struct, base): 
    # Retrieve the layout type, defaulting to None if not set
    layout = getattr(cls, '_layout_', None)
    
    # Determine layout type based on class attribute or platform
    if layout is None:
        if sys.platform == 'win32' or getattr(cls, '_pack_', None):
            gcc_layout = False  # Use MS layout for Windows or if _pack_ is set
        else:
            gcc_layout = True  # Use GCC SysV layout for others
    elif layout == 'ms':
        gcc_layout = False  # MS layout
    elif layout == 'gcc-sysv':
        gcc_layout = True  # GCC SysV layout
    else:
        raise ValueError(f'unknown _layout_: {layout!r}')  # Error for unknown layout type

    # Get the alignment attribute, defaulting to 1
    align = getattr(cls, '_align_', 1)
    
    # Validate alignment; must be non-negative
    if align < 0:
        raise ValueError('_align_ must be a non-negative integer')
    elif align == 0:
        # If align is 0, use default alignment
        align == 1

    # Adjust alignment based on base type if provided
    if base:
        align = max(ctypes.alignment(base), align)

    # Check if the class has byte-swapped fields
    swapped_bytes = hasattr(cls, '_swappedbytes_')
    if swapped_bytes:
        # Set big_endian based on system byte order
        big_endian = sys.byteorder == 'little'
    else:
        big_endian = sys.byteorder == 'big'

    # Validate _pack_ attribute if present
    pack = getattr(cls, '_pack_', None)
    if pack is not None:
        try:
            pack = int(pack)  # Ensure _pack_ is an integer
        except (TypeError, ValueError):
            raise ValueError(""_pack_ must be an integer"")
        if pack < 0 or pack > _INT_MAX:
            raise ValueError(""_pack_ must be a non-negative integer and within range"")
        if gcc_layout:
            raise ValueError('_pack_ is not compatible with gcc-sysv layout')

    result_fields = []

    # Set initial format specifier for structure or union
    if is_struct:
        format_spec_parts = [""T{""]
    else:
        format_spec_parts = [""B""]

    last_field_bit_size = 0  # Used in MS layout for bit-field packing

    # Initialize offsets
    next_bit_offset = 0
    next_byte_offset = 0

    # Initialize sizes for struct and union
    struct_size = 0
    union_size = 0

    # Adjust struct size based on base type if provided
    if base:
        struct_size = ctypes.sizeof(base)
        if gcc_layout:
            next_bit_offset = struct_size * 8  # For GCC layout, in bits
        else:
            next_byte_offset = struct_size  # For MS layout, in bytes

    last_size = struct_size
    for i, field in enumerate(input_fields):
        # Reset offsets for unions
        if not is_struct:
            last_field_bit_size = 0
            next_bit_offset = 0
            next_byte_offset = 0

        # Unpack the field tuple (name, ctype, [bit_size])
        field = tuple(field)
        try:
            name, ctype = field  # Standard field with name and type
        except (ValueError, TypeError):
            try:
                name, ctype, bit_size = field  # Bit-field with bit size
            except (ValueError, TypeError) as exc:
                raise ValueError(
                    '_fields_ must be a sequence of (name, C type) pairs '
                    + 'or (name, C type, bit size) triples') from exc
            is_bitfield = True
            if bit_size <= 0:
                raise ValueError(f'number of bits invalid for bit field {name!r}')
            type_size = ctypes.sizeof(ctype)
            if bit_size > type_size * 8:
                raise ValueError(f'number of bits invalid for bit field {name!r}')
        else:
            is_bitfield = False
            type_size = ctypes.sizeof(ctype)
            bit_size = type_size * 8  # Default bit size for non-bitfields

        # Determine type size in bits and alignment
        type_bit_size = type_size * 8
        type_align = ctypes.alignment(ctype) or 1
        type_bit_align = type_align * 8

        if gcc_layout:
            # For GCC layout, check if the bit-field fits in a slot
            assert pack is None
            assert next_byte_offset == 0

            slot_start_bit = round_down(next_bit_offset, type_bit_align)
            slot_end_bit = slot_start_bit + type_bit_size
            field_end_bit = next_bit_offset + bit_size
            if field_end_bit > slot_end_bit:
                # Add padding if bit-field doesn't fit in the slot
                next_bit_offset = round_up(next_bit_offset, type_bit_align)

            offset = round_down(next_bit_offset, type_bit_align) // 8
            if is_bitfield:
                effective_bit_offset = next_bit_offset - 8 * offset
                size = build_size(bit_size, effective_bit_offset, big_endian, type_size)
                assert effective_bit_offset <= type_bit_size
            else:
                assert offset == next_bit_offset / 8
                size = type_size

            next_bit_offset += bit_size
            struct_size = round_up(next_bit_offset, 8) // 8
        else:
            # Handle MS layout (byte-based packing)
            if pack:
                type_align = min(pack, type_align)

            if (0 < next_bit_offset + bit_size) or (type_bit_size != last_field_bit_size):
                next_byte_offset = round_up(next_byte_offset, type_align)
                next_byte_offset += type_size
                last_field_bit_size = type_bit_size
                next_bit_offset = -last_field_bit_size

            offset = next_byte_offset - last_field_bit_size // 8
            if is_bitfield:
                assert 0 <= (last_field_bit_size + next_bit_offset)
                size = build_size(bit_size, last_field_bit_size + next_bit_offset, big_endian, type_size)
            else:
                size = type_size

            next_bit_offset += bit_size
            struct_size = next_byte_offset

        # Validate bitfield size (if present)
        assert (not is_bitfield) or (LOW_BIT(size) <= size * 8)

        # Add format specifier for structure or union
        if is_struct:
            padding = offset - last_size
            format_spec_parts.append(padding_spec(padding))

            fieldfmt, bf_ndim, bf_shape = buffer_info(ctype)
            if bf_shape:
                format_spec_parts.extend((
                    ""("",
                    ','.join(str(n) for n in bf_shape),
                    "")"",
                ))

            if fieldfmt is None:
                fieldfmt = ""B""
            format_spec_parts.append(f""{fieldfmt}:{name}:"")

        # Add field details to result
        result_fields.append(CField(
            name=name,
            type=ctype,
            size=size,
            offset=offset,
            bit_size=bit_size if is_bitfield else None,
            index=i,
        ))
        align = max(align, type_align)
        last_size = struct_size
        if not is_struct:
            union_size = max(struct_size, union_size)

    # Calculate final aligned size
    total_size = struct_size if is_struct else union_size
    aligned_size = round_up(total_size, align)

    if is_struct:
        padding = aligned_size - total_size
        format_spec_parts.append(padding_spec(padding))
        format_spec_parts.append(""}"")

    return StructUnionLayout(
        fields=result_fields,
        size=aligned_size,
        align=align,
        format_spec="""".join(format_spec_parts),
    )
","The get_layout function is responsible for returning the layout of a structure or union, based on its class attributes and the specified input fields. It determines the layout style (gcc-sysv or ms) and performs various checks for alignment, packing, bit-field handling, and endianness. The function supports both structure and union layouts, adjusting offsets and sizes of fields according to the layout type and alignment constraints. It computes the total size of the structure or union, applies any necessary padding, and returns a StructUnionLayout object, which contains information about the fields, total size, alignment, and format specification. The layout process ensures that the structure or union adheres to the rules of bit-field packing, alignment boundaries, and endianness, depending on the platform and layout settings."
"def padding_spec(padding):
    # If padding is less than or equal to zero, no padding is required, so return an empty string
    if padding <= 0:
        return """"
    
    # If padding is exactly 1, return ""x"" to represent 1 byte of padding
    if padding == 1:
        return ""x""
    
    # If padding is greater than 1, return a formatted string indicating how many bytes of padding are needed
    return f""{padding}x""","The padding_spec function generates a format string that represents padding in a memory layout, specifically for low-level structures such as structs or unions. It takes a single integer input padding, which indicates the number of padding bytes required.

If the padding value is less than or equal to zero (padding <= 0), the function returns an empty string (""""), meaning no padding is needed.
If the padding value is exactly 1 (padding == 1), the function returns the string ""x"", which represents a single byte of padding.
If the padding value is greater than 1, it returns a string formatted as ""{padding}x"", where {padding} is the number of padding bytes (e.g., for 4 bytes of padding, it returns ""4x"").
This function is used to specify the number of bytes to be inserted as padding between fields in memory layouts, ensuring proper alignment in data structures."
"def _get_build_version():
    # Define the prefix that identifies the MSVC version in the sys.version string
    prefix = ""MSC v.""
    
    # Search for the prefix ""MSC v."" in the Python version string
    i = sys.version.find(prefix)
    
    # If the prefix is not found, return 6, assuming MSVC 6 was used (for versions before Python 2.3)
    if i == -1:
        return 6
    
    # Move past the prefix to extract the version number
    i = i + len(prefix)
    
    # Split the remaining string to separate the version number from the rest of the text
    s, rest = sys.version[i:].split("" "", 1)
    
    # Extract the major version number, adjusting it based on the MSVC version
    majorVersion = int(s[:-2]) - 6
    
    # For MSVC versions 13 and above, adjust the major version
    if majorVersion >= 13:
        majorVersion += 1
    
    # Extract the minor version number and divide by 10.0 to get a float
    minorVersion = int(s[2:3]) / 10.0
    
    # For MSVC 6, set minor version to 0 (no change in paths)
    if majorVersion == 6:
        minorVersion = 0
    
    # If the major version is 6 or above, return the combined version number
    if majorVersion >= 6:
        return majorVersion + minorVersion
    
    # Otherwise, return None, as we can't determine the version
    return None
","The _get_build_version function determines the version of the Microsoft Visual C++ (MSVC) compiler that was used to build the currently running Python interpreter. The function works by examining the version string of Python, which includes information about the compiler for Python versions 2.3 and later.

For Python versions 2.3 and above, the MSVC version is embedded in sys.version, and the function extracts and parses this information to return the version number.
The version is assumed to follow the format ""MSC v.X.Y"", where X is the major version and Y is the minor version.
For Python versions earlier than 2.3, it assumes MSVC 6 was used for the build.
The function adjusts for minor version quirks by manipulating the extracted values (e.g., increasing the major version for MSVC versions 13 and higher).
If the version cannot be determined (for example, if the MSC v. string is not found in sys.version), it returns None.
This function is particularly useful for identifying the compiler version used in Python's build process, which may affect binary compatibility with C extensions."
"def find_msvcrt():
    # Get the version of the MSVC compiler used to build Python
    version = _get_build_version()
    
    # If the version cannot be determined, return None
    if version is None:
        # Better be safe than sorry
        return None
    
    # For MSVC versions <= 6, use the 'msvcrt' library name
    if version <= 6:
        clibname = 'msvcrt'
    
    # For MSVC versions 7 to 13, use the 'msvcrXX' library name
    elif version <= 13:
        clibname = 'msvcr%d' % (version * 10)
    
    # For MSVC versions > 13, the CRT is no longer directly loadable
    else:
        # See issue23606 for more details
        return None

    # Check if Python was built in debug mode (by looking for '_d.pyd' in EXTENSION_SUFFIXES)
    import importlib.machinery
    if '_d.pyd' in importlib.machinery.EXTENSION_SUFFIXES:
        clibname += 'd'  # Append 'd' for debug mode

    # Return the full DLL name (e.g., 'msvcrt.dll', 'msvcr90.dll', or 'msvcr90d.dll')
    return clibname + '.dll'
","The find_msvcrt function is used to return the name of the Microsoft Visual C++ runtime (MSVCRT) dynamic-link library (DLL) based on the version of the compiler (MSVC) used to build Python.

Version Check: It uses the _get_build_version function to determine the MSVC version used to build Python.

If the version is None (unable to determine the version), the function returns None to indicate that it cannot find the runtime.
For MSVC versions 6 or below, it returns the library name as 'msvcrt.dll'.
For MSVC versions 7-13, it returns the runtime as 'msvcrXX.dll', where XX corresponds to the version number multiplied by 10 (e.g., msvcr90.dll for MSVC 9).
For MSVC versions higher than 13, the CRT is no longer directly loadable, and the function returns None as no suitable library can be determined.
Debug Mode Check: If Python was built in debug mode (indicated by the presence of the _d.pyd extension), it appends a 'd' to the library name to reflect the debug version of the runtime (e.g., msvcr90d.dll).

Return: It returns the name of the corresponding MSVCRT DLL or None if it cannot be determined.

This function is useful in scenarios where Python needs to load or interact with the appropriate version of the MSVC runtime library, especially for working with C extensions that require it."
"def find_library(name):
    # Check if the library name is 'c' or 'm', which are special cases
    # If it is, delegate the search to the find_msvcrt function
    if name in ('c', 'm'):
        return find_msvcrt()
    
    # Iterate through directories listed in the system's PATH environment variable
    # Split the PATH using the OS-specific path separator
    for directory in os.environ['PATH'].split(os.pathsep):
        # Join the current directory with the library name to form the full path
        fname = os.path.join(directory, name)
        
        # Check if the formed file path is a valid file
        if os.path.isfile(fname):
            return fname
        
        # If the filename ends with "".dll"" (case insensitive), continue to the next directory
        if fname.lower().endswith("".dll""):
            continue
        
        # Append "".dll"" to the filename and check again if it's a valid file
        fname = fname + "".dll""
        
        # If the new filename is a valid file, return its path
        if os.path.isfile(fname):
            return fname
    
    # If no valid library was found, return None
    return None","The find_library(name) function searches for a dynamic link library (DLL) file by looking through directories listed in the system's PATH environment variable. If the provided name is either 'c' or 'm', it delegates the search to another function, find_msvcrt(). The function iterates over all directories in PATH, checking for the existence of a file that matches the given name (with and without the .dll extension). It first checks if a file with the exact name exists, and if not, it appends .dll to the name and checks again. If the library is found, its path is returned; otherwise, it returns None, indicating that the library was not found."
"def find_library(name):
    # Escape special characters in the library name to safely use it in a regular expression
    ename = re.escape(name)
    
    # Create a regular expression pattern to match the library's name and version
    # This pattern looks for library paths in the form of ': -l<name>.<version> => <path>'
    expr = r':-l%s\.\S+ => \S*/(lib%s\.\S+)' % (ename, ename)
    
    # Encode the expression to bytes using the file system's encoding
    expr = os.fsencode(expr)

    try:
        # Attempt to run the 'ldconfig -r' command to retrieve library cache data
        proc = subprocess.Popen(('/sbin/ldconfig', '-r'),
                                stdout=subprocess.PIPE,
                                stderr=subprocess.DEVNULL)
    except OSError:  # If 'ldconfig' command is not found or fails (e.g., not installed)
        data = b''  # Set data to empty byte string in case of failure
    else:
        # If 'ldconfig' succeeds, read its output into 'data'
        with proc:
            data = proc.stdout.read()

    # Use the regular expression to find matches in the 'data' output
    res = re.findall(expr, data)
    
    # If no matches are found, fallback to searching using '_get_soname' and '_findLib_gcc'
    if not res:
        return _get_soname(_findLib_gcc(name))
    
    # Sort the list of matched results based on version number (using '_num_version' key function)
    res.sort(key=_num_version)
    
    # Return the most recent version of the library (last item after sorting)
    return os.fsdecode(res[-1])
","The find_library(name) function searches for the shared library file associated with the specified name. It uses the system's ldconfig utility to search the library cache and locate the corresponding library file. The function constructs a regular expression pattern based on the library name and attempts to execute the ldconfig -r command to retrieve the library data. If the command is not found or fails, the function proceeds to another method (_findLib_gcc(name)) to search for the library using a different approach. If valid results are found from ldconfig, they are sorted by version number, and the most recent version is returned. If no results are found, it returns None."
"def _findLib_ld(name):
    # Construct a regular expression to match library names in the form 'lib<name>.<version>'
    # The expression ensures that the library name is properly escaped to handle special characters
    expr = r'[^\(\)\s]*lib%s\.[^\(\)\s]*' % re.escape(name)
    
    # Prepare the 'ld' command with the '-t' flag, which queries the linker for the specified library
    cmd = ['ld', '-t']
    
    # Retrieve the 'LD_LIBRARY_PATH' environment variable, which specifies directories to search
    libpath = os.environ.get('LD_LIBRARY_PATH')
    
    if libpath:
        # If 'LD_LIBRARY_PATH' is set, extend the command to include the directories
        # by adding '-L <directory>' for each directory in the path
        for d in libpath.split(':'):
            cmd.extend(['-L', d])
    
    # Add the library name to the command, specifying it with the '-l<name>' flag
    cmd.extend(['-o', os.devnull, '-l%s' % name])
    
    # Initialize 'result' to None; this will be used if no library is found
    result = None
    
    try:
        # Execute the 'ld' command, capturing both standard output and error
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE,
                             universal_newlines=True)
        out, _ = p.communicate()  # Read the command's output
        
        # Use regular expression to find library files in the output
        res = re.findall(expr, os.fsdecode(out))
        
        for file in res:
            # Check if the matched file is an ELF file (valid shared object)
            # The _is_elf function ensures that the file is not a linker script
            if not _is_elf(file):
                continue
            # If valid, return the path to the ELF file
            return os.fsdecode(file)
    
    except Exception:
        pass  # If any exception occurs, result will remain None
    
    # Return None if no library is found or an error occurs
    return result","The _findLib_ld(name) function is responsible for locating a shared library (name) using the linker (ld). It constructs a command to query the linker (ld -t), optionally including directories from the LD_LIBRARY_PATH environment variable. The function uses regular expressions to match library files in the output of the linker. If a match is found, it checks whether the matched file is an ELF file (ensuring it is an actual shared object and not a linker script). If the file is valid, its path is returned. If no valid library is found or if an error occurs, the function returns None."
"def find_library(name):
    # Attempt to find the library by first calling _findSoname_ldconfig(name)
    # If this fails (returns None), try to find it using _findLib_gcc(name) and retrieve its SONAME
    # If that also fails, try using _findLib_ld(name) to locate the library
    # The function returns the first valid library path it finds or None if no library is found
    return _findSoname_ldconfig(name) or \
           _get_soname(_findLib_gcc(name)) or _get_soname(_findLib_ld(name))","The find_library(name) function attempts to locate a shared library file by calling a series of other functions. It first tries to find the library using the _findSoname_ldconfig(name) method, which uses ldconfig to search the library. If that fails, it then calls _findLib_gcc(name) and retrieves the library's SONAME (shared object name) via _get_soname(). If that also fails, it proceeds to try the _findLib_ld(name) function, which queries the system's linker for the library. The function returns the path of the found library or None if none of the methods successfully locate the library."
"def has_key(ch):
    # If 'ch' is a string (i.e., a character), convert it to its ASCII value (keycode) using ord()
    if isinstance(ch, str):
        ch = ord(ch)

    # Retrieve the capability name corresponding to the keycode from the _capability_names dictionary
    capability_name = _capability_names.get(ch)
    
    # If the capability name is not found, return False (the key is not supported)
    if capability_name is None:
        return False

    # Check if the terminal description supports the capability (using tigetstr from the curses library)
    # If it does, return True, indicating the key is supported
    if _curses.tigetstr(capability_name):
        return True
    else:
        # If the capability is not found or not supported, return False
        return False","The has_key(ch) function checks if a specified key (given as a character or keycode) is recognized by the current terminal's capabilities. If ch is provided as a string, it is converted to its ASCII value using ord(). The function then attempts to find the capability name corresponding to the keycode from the _capability_names dictionary. If the capability is found, the function uses tigetstr() from the curses library to check if the terminal supports this capability. If the terminal supports it, the function returns True; otherwise, it returns False. If the capability name is not found, it returns False."
"def rectangle(win, uly, ulx, lry, lrx):
    """"""Draw a rectangle with corners at the provided upper-left
    and lower-right coordinates.
    """"""
    # Draw the vertical lines of the rectangle on the left and right sides
    # 'vline' draws a vertical line from (uly+1, ulx) down to 'lry' with a height of 'lry - uly - 1'
    win.vline(uly+1, ulx, curses.ACS_VLINE, lry - uly - 1)
    win.vline(uly+1, lrx, curses.ACS_VLINE, lry - uly - 1)

    # Draw the horizontal lines of the rectangle on the top and bottom sides
    # 'hline' draws a horizontal line from (uly, ulx+1) to 'lrx' with a width of 'lrx - ulx - 1'
    win.hline(uly, ulx+1, curses.ACS_HLINE, lrx - ulx - 1)
    win.hline(lry, ulx+1, curses.ACS_HLINE, lrx - ulx - 1)

    # Draw the corners using the appropriate corner characters
    # 'addch' places a character at the specified coordinates (top-left, top-right, bottom-left, bottom-right)
    win.addch(uly, ulx, curses.ACS_ULCORNER)  # Upper-left corner
    win.addch(uly, lrx, curses.ACS_URCORNER)  # Upper-right corner
    win.addch(lry, lrx, curses.ACS_LRCORNER)  # Lower-right corner
    win.addch(lry, ulx, curses.ACS_LLCORNER)  # Lower-left corner","The rectangle(win, uly, ulx, lry, lrx) function draws a rectangle on a specified window (win) in a terminal interface using the curses library. It takes five parameters: win (the window object), uly (the y-coordinate of the upper-left corner), ulx (the x-coordinate of the upper-left corner), lry (the y-coordinate of the lower-right corner), and lrx (the x-coordinate of the lower-right corner). The function uses the vline() and hline() methods of the curses library to draw vertical and horizontal lines for the sides of the rectangle. It also adds corner characters using addch() to place the four corners of the rectangle, using the appropriate curses.ACS_* characters (such as ACS_VLINE for vertical lines and ACS_HLINE for horizontal lines)."
"def _insert_printable_char(self, ch):
    # Update the maximum window size, ensuring the window's dimensions are accounted for
    self._update_max_yx()
    
    # Get the current cursor position (y, x)
    (y, x) = self.win.getyx()
    
    # Initialize 'backyx' to store the position where the cursor should return
    backyx = None
    
    # Continue inserting characters until reaching the end of the window (maxy, maxx)
    while y < self.maxy or x < self.maxx:
        # If in insert mode, save the current character at the current cursor position
        if self.insert_mode:
            oldch = self.win.inch()
        
        # Try to insert the character 'ch' at the current cursor position
        # If an error occurs (e.g., trying to insert at the bottom-right of the window),
        # it will be silently ignored by the try-except block.
        try:
            self.win.addch(ch)
        except curses.error:
            pass
        
        # If insert mode is off or the old character is not printable, break the loop
        if not self.insert_mode or not curses.ascii.isprint(oldch):
            break
        
        # If the old character is printable and insert mode is on, shift characters
        ch = oldch
        
        # Get the new cursor position after shifting
        (y, x) = self.win.getyx()
        
        # Remember the original cursor position before we move it (if insert mode is active)
        if backyx is None:
            backyx = y, x
    
    # If we moved the cursor during insertion, return to the original position
    if backyx is not None:
        self.win.move(*backyx)","The _insert_printable_char(self, ch) function inserts a printable character (ch) into the window of a curses-based terminal user interface (TUI). It manages the insertion mode and adjusts the cursor position accordingly. The function first updates the maximum window size (_update_max_yx()) and retrieves the current cursor position (y, x). It then enters a loop, where it attempts to insert the character at the current cursor location. If the terminal is in insert mode, it saves the current character (oldch) before writing ch. If the cursor reaches the end of the window (or if the terminal's version causes an error when attempting to insert at the lowest-rightmost position), the loop terminates. If the insertion mode is active and the old character is printable, the function performs character shifting, moving the cursor as necessary. The function ensures that the cursor returns to its original position if it was moved during the insertion."
"def do_command(self, ch):
    ""Process a single editing command.""
    
    # Update the maximum window dimensions
    self._update_max_yx()

    # Get the current cursor position (y, x)
    (y, x) = self.win.getyx()

    # Store the last command issued for future reference
    self.lastcmd = ch

    # If the character is printable, insert it at the current position
    if curses.ascii.isprint(ch):
        if y < self.maxy or x < self.maxx:
            self._insert_printable_char(ch)
    
    # Handle the 'home' command (^a), which moves the cursor to the beginning of the line
    elif ch == curses.ascii.SOH:  # ^a
        self.win.move(y, 0)

    # Handle the 'left arrow', 'backspace', and 'delete' commands
    elif ch in (curses.ascii.STX, curses.KEY_LEFT, curses.ascii.BS, curses.KEY_BACKSPACE, curses.ascii.DEL):
        if x > 0:  # Move left by one character if possible
            self.win.move(y, x-1)
        elif y == 0:  # Don't move past the top row
            pass
        elif self.stripspaces:  # Move to the end of the previous line if stripspaces is enabled
            self.win.move(y-1, self._end_of_line(y-1))
        else:  # Move to the far right of the previous line
            self.win.move(y-1, self.maxx)
        
        # If the command is a backspace or delete, delete the character at the cursor
        if ch in (curses.ascii.BS, curses.KEY_BACKSPACE, curses.ascii.DEL):
            self.win.delch()

    # Handle the 'delete character' command (^d)
    elif ch == curses.ascii.EOT:  # ^d
        self.win.delch()

    # Handle the 'end of line' command (^e)
    elif ch == curses.ascii.ENQ:  # ^e
        if self.stripspaces:
            self.win.move(y, self._end_of_line(y))  # Move to the end of the line, ignoring trailing spaces
        else:
            self.win.move(y, self.maxx)  # Move to the far right of the window

    # Handle the 'right arrow' and 'forward' commands (^f)
    elif ch in (curses.ascii.ACK, curses.KEY_RIGHT):  # ^f
        if x < self.maxx:  # Move right by one character if possible
            self.win.move(y, x+1)
        elif y == self.maxy:  # Don't move past the bottom row
            pass
        else:  # Otherwise, move to the beginning of the next line
            self.win.move(y+1, 0)

    # Handle th","The do_command(self, ch) function processes a single user input command (ch) and performs the corresponding action in the terminal user interface (TUI) managed by curses. The function updates the window's maximum dimensions, retrieves the current cursor position, and tracks the last command issued (self.lastcmd). The function interprets various control characters (such as arrow keys, backspace, and newline) to perform different editing tasks, such as moving the cursor, deleting characters, inserting new characters, or refreshing the screen. It uses different conditions to handle printable characters, cursor movement commands (e.g., ^a, ^d, ^f), and screen manipulation commands (e.g., ^l, ^k, ^o). The function also interacts with the window to modify the screen content and the cursor's position accordingly, ensuring smooth text editing operations in the terminal environment."
"def gather(self):
    ""Collect and return the contents of the window.""
    result = """"  # Initialize an empty string to accumulate the window contents.
    
    # Update the maximum dimensions of the window (maxy, maxx).
    self._update_max_yx()
    
    # Iterate through each row (y) of the window.
    for y in range(self.maxy + 1):
        self.win.move(y, 0)  # Move to the beginning of the row.
        
        # Get the position where the current line ends.
        stop = self._end_of_line(y)
        
        # Skip the row if it's empty or if spaces should be stripped.
        if stop == 0 and self.stripspaces:
            continue
        
        # Iterate through each column (x) in the row.
        for x in range(self.maxx + 1):
            # Break if the column exceeds the end of the line, considering the stripspaces flag.
            if self.stripspaces and x > stop:
                break
            
            # Add the character at (y, x) to the result string.
            result = result + chr(curses.ascii.ascii(self.win.inch(y, x)))
        
        # If it's not the last row, add a newline to separate the lines.
        if self.maxy > 0:
            result = result + ""\n""
    
    # Return the accumulated result as the contents of the window.
    return result","The gather(self) function collects and returns the contents of the window as a string. It iterates over each line of the window, fetching each character and constructing a result string that represents the current state of the window. The function takes into account whether to strip trailing spaces and handles the window's dimensions, moving across rows and columns. Each character is retrieved using self.win.inch(y, x) and converted to a string using chr(curses.ascii.ascii(...)). The result is returned as a multi-line string, which can represent the content of the window or screen."
"def edit(self, validate=None):
    ""Edit in the widget window and collect the results.""
    
    while 1:  # Enter an infinite loop to handle user input.
        ch = self.win.getch()  # Wait for a key press from the user.
        
        # If a validate function is provided, call it to validate the character.
        if validate:
            ch = validate(ch)
        
        # If no character is received, skip to the next iteration.
        if not ch:
            continue
        
        # Process the character as a command (e.g., insert, delete, move cursor).
        # If the command is not valid (returns False), break the loop.
        if not self.do_command(ch):
            break
        
        # Refresh the window to reflect any changes made during the command processing.
        self.win.refresh()

    # After the loop ends (i.e., user has finished editing), collect and return the contents of the window.
    return self.gather()","The edit(self, validate=None) function provides an interactive editing session in a widget window. It listens for user input, processes the input as commands, and continuously updates the window until the user signals the end of the editing process. The function can optionally apply a validate function to each input character before processing it. Once the editing session concludes, the function returns the collected contents of the window, which represents the final state of the editing session."
"def _create(self, flag):
    # If the flag is 'n', delete the data file, backup file, and directory file if they exist.
    if flag == 'n':
        for filename in (self._datfile, self._bakfile, self._dirfile):
            try:
                _os.remove(filename)  # Attempt to remove the file.
            except OSError:  # If the file doesn't exist or cannot be removed, ignore the error.
                pass
    
    # Mod by Jack: create the data file if needed (flag not 'c' or 'n')
    try:
        # Try opening the data file in read mode with Latin-1 encoding.
        f = _io.open(self._datfile, 'r', encoding=""Latin-1"")
    except OSError:
        # If opening the file fails (e.g., file doesn't exist or has issues), and flag is not 'c' or 'n', raise an error.
        if flag not in ('c', 'n'):
            raise
        # If the flag is 'c' or 'n', create the file and ensure correct permissions.
        with _io.open(self._datfile, 'w', encoding=""Latin-1"") as f:
            self._chmod(self._datfile)  # Set the permissions for the newly created file.
    else:
        # If the file opens successfully, close the file handle.
        f.close()","The _create(self, flag) function handles the creation or cleanup of files associated with a data structure. Depending on the flag passed to the function, it either removes existing files or creates a new data file. The function handles different file states and ensures proper permissions are set on the newly created file."
"def _update(self, flag):
    # Initialize state: mark the object as unmodified and reset the index.
    self._modified = False
    self._index = {}

    try:
        # Try to open the directory file (_dirfile) in read mode with Latin-1 encoding.
        f = _io.open(self._dirfile, 'r', encoding=""Latin-1"")
    except OSError:
        # If opening the file fails, handle depending on the flag.
        if flag not in ('c', 'n'):
            raise  # Raise an error if flag is not 'c' or 'n'.
        
        # If the flag is 'c' or 'n', create the directory file by opening it in write mode.
        with self._io.open(self._dirfile, 'w', encoding=""Latin-1"") as f:
            self._chmod(self._dirfile)  # Set file permissions for the new file.
    else:
        # If the file opens successfully, read each line.
        with f:
            for line in f:
                line = line.rstrip()  # Remove trailing whitespace from the line.
                key, pos_and_siz_pair = _ast.literal_eval(line)  # Safely evaluate the line into a tuple.
                
                key = key.encode('Latin-1')  # Ensure the key is in the correct encoding.
                self._index[key] = pos_and_siz_pair  # Update the internal index with the key and position/size pair.","The _update(self, flag) function updates the index for a data structure. It opens a directory file, processes its contents, and updates an internal index. If the file does not exist or cannot be opened, it either creates the file (based on the provided flag) or raises an error. This function also manages file permissions after file creation."
"def __getitem__(self, key):
    # Check if the provided key is a string and encode it to UTF-8 if true
    if isinstance(key, str):
        key = key.encode('utf-8')
    
    # Ensure the file is open before performing any operations
    self._verify_open()

    # Retrieve the position (pos) and size (siz) of the requested item from the index
    pos, siz = self._index[key]     # May raise KeyError if the key is not found in the index
    
    # Open the data file in binary read mode to fetch the item
    with _io.open(self._datfile, 'rb') as f:
        # Move the file pointer to the position of the item
        f.seek(pos)
        # Read the item data based on its size
        dat = f.read(siz)

    # Return the retrieved data
    return dat","The __getitem__ method allows for accessing items stored in a data structure using a key, similar to dictionary lookups. It first checks if the key is a string, and if so, encodes it to UTF-8. Then, it verifies that the data structure is open for operations. It retrieves the position and size of the requested item from an index, which is assumed to be pre-populated. The method then opens the data file in binary read mode, seeks to the correct position, and reads the items data based on the recorded size. If the key is not found in the index, a KeyError is raised. This method returns the retrieved data as a byte sequence."
"def _addval(self, val):
    # Open the data file in read-write binary mode ('rb+')
    with _io.open(self._datfile, 'rb+') as f:
        # Move the file pointer to the end of the file to append data
        f.seek(0, 2)
        # Get the current position of the file pointer (end of the file)
        pos = int(f.tell())
        # Calculate the next block position to ensure alignment to _BLOCKSIZE
        npos = ((pos + _BLOCKSIZE - 1) // _BLOCKSIZE) * _BLOCKSIZE
        
        # Write padding (null bytes) to the file to align the data to the next block size
        f.write(b'\0'*(npos - pos))
        
        # Update the position after padding and write the actual value
        pos = npos
        f.write(val)
    
    # Return the position where the data was added and the length of the data written
    return (pos, len(val))","The _addval method appends a value to a data file, ensuring proper alignment to a specified block size (_BLOCKSIZE). The function first opens the data file in read-write binary mode. It moves the file pointer to the end of the file and retrieves the current position. The method calculates the next available position that aligns with the block size, then writes null bytes (b'\0') as padding to fill the space between the current file position and the aligned position. After this, the actual value is written to the file. The method returns the new position where the value was added, along with the length of the written value. This ensures the file remains correctly padded and aligned to the specified block size."
"def _setval(self, pos, val):
    # Open the data file in read-write binary mode ('rb+')
    with _io.open(self._datfile, 'rb+') as f:
        # Move the file pointer to the specified position (pos)
        f.seek(pos)
        # Write the provided value at the specified position in the file
        f.write(val)
    
    # Return the position where the value was written and the length of the value
    return (pos, len(val))","The _setval method writes a specified value (val) to a given position (pos) in a data file. It opens the file in read-write binary mode ('rb+'), then moves the file pointer to the specified position using seek(). The provided value is written to the file at that position using write(). The method returns the position where the value was written and the length of the value. This function is useful for updating specific positions in a file without altering the rest of its contents."
"def _addkey(self, key, pos_and_siz_pair):
    # Add the key and its associated position and size pair to the index dictionary
    self._index[key] = pos_and_siz_pair
    
    # Open the directory file in append mode ('a') with 'Latin-1' encoding
    with _io.open(self._dirfile, 'a', encoding=""Latin-1"") as f:
        # Set appropriate file permissions for the directory file
        self._chmod(self._dirfile)
        
        # Write the key (decoded to 'Latin-1') and its position and size pair to the file
        f.write(""%r, %r\n"" % (key.decode(""Latin-1""), pos_and_siz_pair))","The _addkey method adds a key and its associated position and size pair to an internal index dictionary. After updating the index, it opens the directory file (_dirfile) in append mode ('a') with Latin-1 encoding, ensuring that the file permissions are set correctly by calling _chmod. The key and its position-size pair are written to the file in a specific format (as a string representation). This method is used to update both the in-memory index and the corresponding directory file, which tracks key-location mappings."
"def __delitem__(self, key):
    # Check if the database is in read-only mode, raise an error if it is
    if self._readonly:
        raise error('The database is opened for reading only')
    
    # If the key is a string, convert it to bytes (UTF-8 encoding)
    if isinstance(key, str):
        key = key.encode('utf-8')
    
    # Verify if the database is open for operations
    self._verify_open()
    
    # Mark the database as modified to track changes
    self._modified = True
    
    # Delete the key from the index, which will remove its associated value
    del self._index[key]
    
    # It's unclear why this commit is needed here, but we commit to persist changes
    # It ensures the changes are saved, even though __setitem__ doesn't handle this synchronization
    self._commit()
","The __delitem__ method is responsible for deleting a key and its associated value from the database. First, it checks if the database is in read-only mode (via self._readonly). If it is, an error is raised, preventing any modifications. If the key is a string, it is encoded into bytes using UTF-8 encoding. The method then verifies that the database is open for operations using the _verify_open method. After marking the database as modified (via self._modified), it deletes the specified key from the index dictionary (self._index), effectively removing the key-value association. Finally, the method calls _commit to ensure that changes are persisted, although the exact reason for this commit remains unclear, as other operations like __setitem__ do not synchronize the directory file in the same way."
"def open(file, flag='c', mode=0o666):
    """"""Open the database file, filename, and return corresponding object.

    The flag argument, used to control how the database is opened in the
    other DBM implementations, supports only the semantics of 'c' and 'n'
    values.  Other values will default to the semantics of 'c' value:
    the database will always opened for update and will be created if it
    does not exist.

    The optional mode argument is the UNIX mode of the file, used only when
    the database has to be created.  It defaults to octal code 0o666 (and
    will be modified by the prevailing umask).

    """"""

    # Modify the file mode based on the system's umask to respect the file permissions
    try:
        um = _os.umask(0)  # Set the umask to 0 to get the current umask
        _os.umask(um)      # Restore the original umask
    except AttributeError:
        pass  # If umask is not available, proceed without modifying the mode
    else:
        # Turn off any bits that are set in the umask (ensure the file has the desired permissions)
        mode = mode & (~um)  # Remove bits set by the umask from the mode
    
    # Validate the flag argument, ensuring it's one of the allowed values ('r', 'w', 'c', 'n')
    if flag not in ('r', 'w', 'c', 'n'):
        raise ValueError(""Flag must be one of 'r', 'w', 'c', or 'n'"")
    
    # Return a new _Database object","The open function opens a database file and returns a corresponding _Database object. The flag argument controls how the database is opened and supports the values 'c' (create if not exist) and 'n' (create new). If an invalid flag is provided, a ValueError is raised. The mode argument specifies the UNIX file mode, used only when the database file is created. It defaults to 0o666 and is adjusted according to the systems umask to ensure appropriate file permissions. The function then validates the flag parameter to ensure it is one of the accepted values. Finally, it creates and returns a new _Database object with the specified file path, mode, and flag, which handles further database operations."
"def decode_b(encoded):
    # First, attempt to decode the encoded string, fixing the padding if needed.
    # The padding is necessary to make the length of the encoded string a multiple of 4.
    pad_err = len(encoded) % 4  # Calculate the number of characters missing for padding
    missing_padding = b'==='[:4-pad_err] if pad_err else b''  # Add appropriate padding if needed
    
    try:
        # Attempt to decode with correct padding and validate the base64 characters
        return (
            base64.b64decode(encoded + missing_padding, validate=True),
            [errors.InvalidBase64PaddingDefect()] if pad_err else [],
        )
    except binascii.Error:
        # If padding was correct, it indicates an invalid character error.
        # The non-alphabet characters are ignored for padding purposes, but
        # we can't determine how many of them there are. Try decoding without the padding.
        try:
            return (
                base64.b64decode(encoded, validate=False),
                [errors.InvalidBase64CharactersDefect()],
            )
        except binascii.Error:
            # If the previous attempts fail, add as much padding as could possibly be necessary.
            # Extra padding is ignored, but we attempt to add the minimum required to make the
            # base64 string valid.
            try:
                return (
                    base64.b64decode(encoded + b'==', validate=False),
                    [errors.InvalidBase64CharactersDefect(),
                     errors.InvalidBase64PaddingDefect()],
                )
            except binascii.Error:
                # If the length of the encoded string is 1 more than a multiple of 4, it's invalid.
                # No valid decoding can be done in this case.
                #
                # bpo-27397: Return the encoded string as-is and indicate an invalid length defect.
                return encoded, [errors.InvalidBase64LengthDefect()]","The decode_b function attempts to decode a base64-encoded string with various levels of error handling and padding corrections. It first checks for any padding errors and attempts to add the necessary padding if required. If this decoding attempt fails due to invalid characters in the base64 string, the function then tries to decode the string without padding. If this also fails, it attempts to add just enough padding to make the string valid. Finally, if the string's length is deemed invalid (i.e., it has 1 more character than a multiple of 4), it returns the encoded string itself and flags the error as an invalid length defect. Throughout these steps, the function also tracks and returns specific errors related to padding, characters, or length."
"def decode(ew):
    # Split the encoded word into its components (charset, lang, cte, and the encoded string)
    _, charset, cte, cte_string, _ = ew.split('?')
    
    # Extract the language from the charset part (if it exists, otherwise it's empty)
    charset, _, lang = charset.partition('*')
    
    # Convert the content transfer encoding (cte) to lowercase
    cte = cte.lower()
    
    # Recover the original bytes from the encoded string and perform Content Transfer Encoding (CTE) decoding
    bstring = cte_string.encode('ascii', 'surrogateescape')  # Encode the string as bytes using 'surrogateescape'
    bstring, defects = _cte_decoders[cte](bstring)  # Decode using the appropriate CTE decoder
    
    # Try to decode the CTE-decoded bytes into a string using the specified charset
    try:
        string = bstring.decode(charset)  # Attempt to decode using the specified charset
    except UnicodeDecodeError:
        # If the bytes cannot be decoded with the given charset, add a defect and use 'surrogateescape'
        defects.append(errors.UndecodableBytesDefect(""Encoded word ""
            f""contains bytes not decodable using {charset!r} charset""))
        string = bstring.decode(charset, 'surrogateescape')  # Use surrogateescape to handle unknown characters
    except (LookupError, UnicodeEncodeError):
        # If there's an error looking up or encoding with the charset, fallback to 'ascii' decoding
        string = bstring.decode('ascii', 'surrogateescape')
        # If the charset is not recognized, add a defect
        if charset.lower() != 'unknown-8bit':
            defects.append(errors.CharsetError(f""Unknown cha
","The decode function is designed to decode an RFC 2047/2243 encoded word, commonly used in email headers to represent non-ASCII characters. The function splits the encoded word into its components, such as the charset (character set), lang (language, which is optional), cte (Content Transfer Encoding), and the actual encoded string. The function then decodes the content using the appropriate CTE decoder and attempts to convert the resulting bytes into a Unicode string using the specified charset. If the decoding fails, the function appends defects to a list, replacing any undecodable characters with a fallback value (typically using 'surrogateescape' to avoid loss of data). If an unknown charset is detected, another defect is added. The function returns the decoded string, charset, language, and any defects that occurred during the decoding process."
"def encode(string, charset='utf-8', encoding=None, lang=''):
    # If the charset is 'unknown-8bit', encode the string using 'surrogateescape' to handle unknown characters
    if charset == 'unknown-8bit':
        bstring = string.encode('ascii', 'surrogateescape')
    else:
        # Encode the string using the specified charset (default is utf-8)
        bstring = string.encode(charset)
    
    # If no encoding is specified, calculate the encoded length for both 'q' and 'b' encodings
    if encoding is None:
        qlen = _cte_encode_length['q'](bstring)  # Calculate length for 'q' encoding
        blen = _cte_encode_length['b'](bstring)  # Calculate length for 'b' encoding
        
        # Select the encoding that produces the shorter result. Bias toward 'q' if the difference is less than 5.
        encoding = 'q' if qlen - blen < 5 else 'b'
    
    # Perform the actual CTE encoding using the chosen encoding ('q' or 'b')
    encoded = _cte_encoders[encoding](bstring)
    
    # If a language is provided, add the language specification to the encoded word
    if lang:
        lang = '*' + lang
    
    # Return the RFC 2047/2243 encoded word in the format: =?charset*lang?cte?encoded_string?=
    return ""=?{}{}?{}?{}?="".format(charset, lang, encoding, encoded)","The encode function is used to encode a given string into an RFC 2047/2243 encoded word, which is commonly used in email headers to represent non-ASCII characters. The function first encodes the string using the specified charset (defaulting to UTF-8). If the charset is unknown-8bit, it uses the surrogateescape encoding to handle non-ASCII characters. The function then determines which Content Transfer Encoding (CTE) to use (either 'q' or 'b') based on the length of the encoded result. If no encoding is specified, the function compares the lengths of the two encoding methods and prefers the shorter one, with a slight bias toward 'q' if the difference is small. The encoded string is then generated using the chosen CTE and formatted according to RFC 2047/2243 standards. The resulting encoded word includes the charset, optional language (if provided), encoding type, and the encoded string itself, and is returned in the format: =?charset*lang?cte?encoded_string?=."
"def parsedate_tz(data):
    """"""Convert a date string to a time tuple.

    Accounts for military timezones.
    """"""
    # Parse the date string using the helper function _parsedate_tz
    res = _parsedate_tz(data)
    
    # If the date string could not be parsed, return None
    if not res:
        return
    
    # If the timezone information is missing (res[9] is None), set it to 0
    if res[9] is None:
        res[9] = 0
    
    # Return the parsed date as a tuple
    return tuple(res)
","The parsedate_tz function is designed to parse a date string into a time tuple, which includes various date and time components (such as year, month, day, hour, minute, second, etc.). The function uses a helper function _parsedate_tz to perform the parsing. If the parsing is successful and a result is obtained, the function checks if the timezone information (represented by the 10th element of the result tuple) is missing. If so, it sets the timezone offset to 0. The function then returns the parsed date as a tuple, which includes all the components of the date, with the timezone information appropriately handled. If parsing fails, the function returns None. This function also accounts for military timezones by ensuring that any missing timezone information is set to 0 (UTC)."
"def _parsedate_tz(data):
    if not data:
        return None  # Return None if the input data is empty

    data = data.split()  # Split the date string by whitespace
    if not data:  # If data is only whitespace, return None
        return None

    # Check for and remove optional day name at the beginning of the string
    if data[0].endswith(',') or data[0].lower() in _daynames:
        del data[0]
    else:
        i = data[0].rfind(',')
        if i >= 0:
            data[0] = data[0][i+1:]

    # Handle RFC 850 deprecated date format (only 3 parts)
    if len(data) == 3:
        stuff = data[0].split('-')
        if len(stuff) == 3:
            data = stuff + data[1:]

    # Handle timezone formatting
    if len(data) == 4:
        s = data[3]
        i = s.find('+')
        if i == -1:
            i = s.find('-')
        if i > 0:
            data[3:] = [s[:i], s[i:]]
        else:
            data.append('')  # Add dummy timezone

    # Return None if there are insufficient components to form a valid date
    if len(data) < 5:
        return None

    data = data[:5]  # Only keep the first 5 components (day, month, year, time, tz)
    [dd, mm, yy, tm, tz] = data

    # Return None if day, month, or year are missing
    if not (dd and mm and yy):
        return None

    mm = mm.lower()  # Convert month to lowercase for consistency

    # Check if the month is valid; if not, swap day and month
    if mm not in _monthnames:
        dd, mm = mm, dd.lower()
        if mm not in _monthnames:
            return None

    mm = _monthnames.index(mm) + 1  # Convert month name to month number
    if mm > 12:
        mm -= 12  # Ensure month is valid (1-12)

    if dd[-1] == ',':
        dd = dd[:-1]  # Remove trailing comma from day

    # Handle cases where year or time format is swapped
    i = yy.find(':')
    if i > 0:
        yy, tm = tm, yy

    if yy[-1] == ',':
        yy = yy[:-1]
        if not yy:
            return None

    if not yy[0].isdigit():
        yy, tz = tz, yy  # Swap year and timezone if necessary

    if tm[-1] == ',':
        tm = tm[:-1]  # Remove trailing comma from time
    tm = tm.split(':')  # Split time into hours, minutes, and seconds

    # Handle cases where time components are missing
    if len(tm) == 2:
        [thh, tmm] = tm
        tss = '0'  # Default seconds to 0
    elif len(tm) == 3:
        [thh, tmm, tss] = tm
    elif len(tm) == 1 and '.' in tm[0]:
        tm = tm[0].split('.')
        if len(tm) == 2:
            [thh, tmm] = tm
            tss = 0  # Default seconds to 0
        elif len(tm) == 3:
            [thh, tmm, tss] = tm
        else:
            return None
    else:
        return None

    # Convert string values to integers
    try:
        yy = int(yy)
        dd = int(dd)
        thh = int(thh)
        tmm = int(tmm)
        tss = int(tss)
    except ValueError:
        return None  # Return None if conversion fails

    # Handle two-digit year conversion to four-digit year
    if yy < 100:
        if yy > 68:
            yy += 1900
        else:
            yy += 2000

    tzoffset = None  # Default timezone offset to None
    tz = tz.upper()  # Normalize timezone to uppercase

    # Check if timezone is valid and look it up in _timezones
    if tz in _timezones:
        tzoffset = _timezones[tz]
    else:
        try:
            tzoffset = int(tz)  # Try converting timezone to integer offset
        except ValueError:
            pass

        if tzoffset == 0 and tz.startswith('-'):
            tzoffset = None  # Special case for UTC timestamps with -0000 timezone

    # Convert timezone offset into seconds (e.g., -0500 -> -18000)
    if tzoffset:
        if tzoffset < 0:
            tzsign = -1
            tzoffset = -tzoffset
        else:
            tzsign = 1
        tzoffset = tzsign * ((tzoffset // 100) * 3600 + (tzoffset % 100) * 60)

    # Return the extended time tuple including timezone offset
    return [yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset]
","The _parsedate_tz function is responsible for parsing a date string and returning an extended time tuple, which includes detailed information such as the year, month, day, time, and timezone offset in seconds. It accounts for various date formats, including those with or without a day name, and handles timezones with both numeric offsets (like -0500) and abbreviations (like EST).

The function normalizes the date components, converts them into integers, and adjusts for common inconsistencies in date formats. It also handles the special case of UTC timestamps where the timezone is explicitly set to -0000. If the function encounters invalid date components or formats that cannot be parsed, it returns None.

The resulting time tuple consists of the parsed date and time components, followed by a timezone offset in seconds (if available), allowing for precise handling of timezones and conversions between them."
"def parsedate(data):
    t = parsedate_tz(data)  # Call parsedate_tz to get the extended time tuple
    if isinstance(t, tuple):  # If parsing was successful and returned a tuple
        return t[:9]  # Return only the first 9 elements (ignoring the timezone offset)
    else:
        return t  # Return the result (None if parsing failed)
","The parsedate function is designed to convert a time string into a 9-element time tuple, which includes year, month, day, hour, minute, second, weekday, yearday, and daylight savings flag. It utilizes the parsedate_tz function for parsing the input date string.

The function calls parsedate_tz, which returns a more detailed time tuple that includes the timezone offset as its 10th element. The parsedate function then discards this 10th element (the timezone offset) and returns the first 9 elements of the tuple, which represent the standard components of a date and time.

If the parsedate_tz function returns a valid tuple, parsedate will return the 9-element time tuple. If parsing fails (i.e., the result is not a tuple), parsedate simply returns the result from parsedate_tz (which could be None). This allows for handling various formats of date strings with optional timezone offsets."
"def mktime_tz(data):
    if data[9] is None:
        # No timezone info (None), assume localtime
        return time.mktime(data[:8] + (-1,))  # Use localtime
    else:
        # Import calendar only if necessary (since mktime_tz is rarely used)
        import calendar

        t = calendar.timegm(data)  # Convert to UTC timestamp
        return t - data[9]  # Adjust for timezone offset
","The mktime_tz function is designed to convert a 10-tuple, such as the one returned by parsedate_tz, into a POSIX timestamp. A POSIX timestamp represents the number of seconds since January 1, 1970 (the Unix epoch).

The function checks whether the 10th element (the timezone offset) is None. If there is no timezone information, it assumes the date and time are in local time, so it calls time.mktime to convert the time tuple into a POSIX timestamp based on local time.

If the timezone offset is present, the function imports the calendar module and uses calendar.timegm to convert the time tuple to a UTC timestamp. It then adjusts this timestamp by subtracting the timezone offset (in seconds) to get the correct POSIX timestamp.

Thus, the function handles both local time and UTC time by adjusting for the timezone offset, if provided."
"def gotonext(self):
    wslist = []  # Initialize an empty list to store whitespace characters
    while self.pos < len(self.field):
        # If the current character is white space or newline/carriage return, continue processing
        if self.field[self.pos] in self.LWS + '\n\r':
            # If it's not a newline/carriage return, add it to the wslist
            if self.field[self.pos] not in '\n\r':
                wslist.append(self.field[self.pos])
            self.pos += 1  # Move to the next character
        # If we encounter an opening parenthesis, it's the start of a comment
        elif self.field[self.pos] == '(':
            # Get the comment and append it to the comment list
            self.commentlist.append(self.getcomment())
        else:
            # Stop when a non-whitespace, non-comment character is encountered
            break
    # Join the list of whitespace characters into a string and return it
    return EMPTYSTRING.join(wslist)
","The gotonext method is part of a class that processes an input string, presumably in the context of parsing or tokenization. It handles two main tasks:

Skipping Whitespace: The method looks for whitespace characters (including spaces, tabs, newlines, and carriage returns) and adds them to the list wslist. It continues processing until a non-whitespace character is found.

Extracting Comments: If a parenthesis ( is encountered, the method identifies it as the beginning of a comment. It uses a method (getcomment()) to extract the comment and appends it to a list of comments (commentlist).

Once all whitespace characters and comments are processed, the method returns a string composed of the accumulated whitespace characters (ignoring comments). The method stops when a non-whitespace, non-comment character is encountered, signaling the end of the parsing process for this segment.

The function is designed to be part of a larger parser or tokenizer that processes a field of input data, skipping irrelevant spaces and extracting comments for further handling."
"def getaddrlist(self):
    """"""Parse all addresses.

    Returns a list containing all of the addresses.
    """"""
    # Initialize an empty list to store the results.
    result = []

    # Loop through the field (presumably an email field) until the end of the string is reached.
    while self.pos < len(self.field):
        # Get the address using the getaddress() method.
        ad = self.getaddress()

        # If the address is valid (non-empty), add the address to the result list.
        if ad:
            result += ad  # Adds the returned address to the result list.

        # If no address is found (empty or None), append a tuple with two empty strings.
        else:
            result.append(('', ''))

    # Return the final list containing all the parsed addresses.
    return result","The getaddrlist() function parses a field (presumably containing email addresses) and returns a list of all parsed addresses. It loops through the field and uses the getaddress() method to retrieve individual addresses. If a valid address is found, it is added to the result list. If an address is invalid or not found, an empty tuple ('', '') is appended instead. The function continues this process until it has processed all the content in the field, at which point it returns the complete list of parsed addresses."
"def getrouteaddr(self):
    """"""Parse a route address (Return-path value).

    This method just skips all the route stuff and returns the addrspec.
    """"""
    # Check if the current position in the field points to an opening angle bracket ('<').
    # If not, return (indicating an invalid or missing route address).
    if self.field[self.pos] != '<':
        return

    # Set expectroute flag to False initially.
    expectroute = False
    # Move the position to the next character (after '<').
    self.pos += 1
    # Skip over any leading whitespace or comments.
    self.gotonext()

    # Initialize an empty string to store the address specification (adlist).
    adlist = ''

    # Start looping through the field to parse the route address.
    while self.pos < len(self.field):
        # If expectroute is True, get the domain part of the address.
        if expectroute:
            self.getdomain()
            expectroute = False  # Reset expectroute flag after processing domain.
        
        # If the current character is a closing angle bracket ('>'), end the loop.
        elif self.field[self.pos] == '>':
            self.pos += 1  # Move past the '>'.
            break
        
        # If the current character is an '@', set the expectroute flag to True.
        # This is part of the route address format.
        elif self.field[self.pos] == '@':
            self.pos += 1  # Move past the '@'.
            expectroute = True
        
        # If the current character is a colon (':'), just move past it.
        elif self.field[self.pos] == ':':
            self.pos += 1
        
        # Otherwise, get the address specification (adrspec) and break the loop.
        else:
            adlist = self.getaddrspec()  # Call the getaddrspec method to retrieve the address.
            self.pos += 1  # Move past the address specification.
            break
        
        # Skip over any white space or comments.
        self.gotonext()

    # Return the parsed address specification.
    return adlist
","The getrouteaddr() function is designed to parse a route address, typically found in the ""Return-Path"" of email headers. It first checks for the presence of an opening angle bracket (<), which denotes the start of the route address. It then proceeds to loop through the field, skipping over various route components such as domains, separators (like @ and :), and any comments or whitespace. The function handles the parsing by toggling an expectroute flag when it encounters certain characters (like @), indicating the structure of the route address. The function finally returns the parsed address specification (addrspec) once the entire route address is processed, or an empty result if the input does not conform to the expected format."
"def getaddrspec(self):
    """"""Parse an RFC 2822 addr-spec.""""""
    # Initialize an empty list to store the address specification components.
    aslist = []

    # Skip over leading whitespace or comments.
    self.gotonext()

    # Start processing the address specification.
    while self.pos < len(self.field):
        # Preserve whitespace unless specifically instructed otherwise.
        preserve_ws = True

        # If the current character is a period ('.'), handle it as a special case.
        if self.field[self.pos] == '.':
            # If the last item in the list is empty or only whitespace, remove it.
            if aslist and not aslist[-1].strip():
                aslist.pop()
            # Append a period ('.') to the address specification.
            aslist.append('.')
            self.pos += 1  # Move past the period.
            preserve_ws = False  # Do not preserve whitespace after a period.

        # If the current character is a double quote ('""'), handle quoted strings.
        elif self.field[self.pos] == '""':
            # Append the quoted string to the address specification, quoting it properly.
            aslist.append('""%s""' % quote(self.getquote()))

        # If the current character is part of a valid atom (a sequence of characters),
        # break out of the loop.
        elif self.field[self.pos] in self.atomends:
            # If the last item in the list is empty or only whitespace, remove it.
            if aslist and not aslist[-1].strip():
                aslist.pop()
            break

        # For any other characters, treat them as atoms (basic components of an address).
        else:
            aslist.append(self.getatom())

        # Process any whitespace that follows.
        ws = self.gotonext()

        # If whitespace is to be preserved and there's whitespace to append, add it.
        if preserve_ws and ws:
            aslist.append(ws)

    # If the position has reached the end of the input or the current character is not '@',
    # return the constructed address part (local part).
    if self.pos >= len(self.field) or self.field[self.pos] != '@':
        return EMPTYSTRING.join(aslist)

    # If we encounter an '@', append it to the address specification and move past it.
    aslist.append('@')
    self.pos += 1
    self.gotonext()

    # Parse the domain part of the address.
    domain = self.getdomain()

    # If the domain is invalid, return an empty address string.
    if not domain:
        return EMPTYSTRING

    # Return the full address specification by joining the local part and the domain.
    return EMPTYSTRING.join(aslist) + domain
","The getaddrspec() function is responsible for parsing an address specification (addr-spec) according to RFC 2822, which defines the format for email addresses. The function processes the local part of the email address, handling components like periods (.), quoted strings (""), and atoms (basic sequences of characters). The function also ensures that certain edge cases are handled, such as preventing consecutive periods or handling whitespace properly. Once the local part is parsed, the function checks for the @ symbol, which separates the local part from the domain. If the @ symbol is found, it proceeds to parse the domain using the getdomain() method. If any parsing errors occur, such as an invalid domain, the function returns an empty string, indicating a failed parse. Finally, the function returns the full address specification by concatenating the local part and domain or just the local part if the domain is absent."
"def getdomain(self):
    """"""Get the complete domain name from an address.""""""
    # Initialize an empty list to store the domain name components.
    sdlist = []

    # Start processing the field starting from the current position.
    while self.pos < len(self.field):
        # If the current character is whitespace, skip it and move the position forward.
        if self.field[self.pos] in self.LWS:
            self.pos += 1

        # If the current character is a comment start '(' (used for comments in email addresses),
        # parse the comment and append it to the comment list.
        elif self.field[self.pos] == '(':
            self.commentlist.append(self.getcomment())

        # If the current character is an opening square bracket '[' (indicating a domain literal),
        # call the getdomainliteral method to handle domain literals and append the result.
        elif self.field[self.pos] == '[':
            sdlist.append(self.getdomainliteral())

        # If the current character is a period ('.'), append it to the domain name list and move the position forward.
        elif self.field[self.pos] == '.':
            self.pos += 1
            sdlist.append('.')

        # If another '@' symbol is encountered, return an empty string to prevent parsing errors,
        # e.g., handling cases like ""a@malicious.org@important.com"" which should not be valid.
        elif self.field[self.pos] == '@':
            # bpo-34155: Avoid parsing domains with multiple '@' symbols.
            return EMPTYSTRING

        # If the current character is part of a valid atom (a valid part of a domain name),
        # break out of the loop as we have reached the end of the domain.
        elif self.field[self.pos] in self.atomends:
            break

        # For any other character, treat it as part of the domain name and get the atom.
        else:
            sdlist.append(self.getatom())

    # Return the complete domain name as a concatenated string from the list of domain components.
    return EMPTYSTRING.join(sdlist)","The getdomain() function parses the domain part of an email address (as defined in RFC 2822). The function processes the input string starting from the current position, collecting domain components while handling special cases like whitespace, comments, domain literals (enclosed in square brackets), periods ('.'), and atoms (basic sequences of characters). It checks for the presence of multiple @ symbols, which is an invalid case, and returns an empty string in such scenarios to prevent incorrect parsing. As it processes the domain, the function builds the domain name by appending valid components to a list. Once the domain is fully parsed, the function returns the complete domain name as a concatenated string. The function ensures that only valid domain structures are captured while skipping any unwanted characters or invalid formats."
"def getdelimited(self, beginchar, endchars, allowcomments=True):
    # If the current character is not the beginning delimiter, return an empty string.
    if self.field[self.pos] != beginchar:
        return ''

    # Initialize a list to store the parsed fragment (including the start delimiter).
    slist = ['']

    # Flag to handle quoted characters.
    quote = False

    # Move past the start delimiter.
    self.pos += 1

    # Continue parsing until the end of the field.
    while self.pos < len(self.field):
        # If a quoted character is encountered (preceded by a backslash), append it and reset the flag.
        if quote:
            slist.append(self.field[self.pos])
            quote = False
        
        # If an end delimiter character is encountered, stop parsing and break out of the loop.
        elif self.field[self.pos] in endchars:
            self.pos += 1
            break
        
        # If embedded RFC 2822 comments are allowed, and a comment start is encountered, parse the comment.
        elif allowcomments and self.field[self.pos] == '(':
            slist.append(self.getcomment())
            continue  # Already advanced position from getcomment, so skip the increment here.

        # If a backslash (escape character) is encountered, set the quote flag to handle the next character.
        elif self.field[self.pos] == '\\':
            quote = True

        # Otherwise, append the current character to the fragment.
        else:
            slist.append(self.field[self.pos])

        # Move to the next character.
        self.pos += 1

    # Return the concatenated string of the parsed fragment.
    return EMPTYSTRING.join(slist)
","The getdelimited() function parses a fragment of a header in an email message, enclosed by specific delimiters. The parsing begins with a character provided by the beginchar argument, and continues until one of the characters from the endchars sequence is encountered. If allowcomments is set to True, embedded comments (as defined in RFC 2822) are also allowed within the fragment and parsed separately. The function handles quoted characters (preceded by a backslash) and skips over comments within parentheses. The parsing stops when an end delimiter is encountered, and the function returns the parsed fragment as a single string. If the initial character does not match beginchar, the function immediately returns an empty string, indicating that no valid fragment was found. This function is useful for extracting header fragments that may contain optional embedded comments or quoted characters."
"def getatom(self, atomends=None):
    """"""Parse an RFC 2822 atom.

    Optional atomends specifies a different set of end token delimiters
    (the default is to use self.atomends).  This is used e.g. in
    getphraselist() since phrase endings must not include the '.' (which
    is legal in phrases).
    """"""
    # Initialize a list to store the parsed atom (starting with an empty string).
    atomlist = ['']

    # If atomends is not provided, use the default delimiters stored in self.atomends.
    if atomends is None:
        atomends = self.atomends

    # Continue parsing until the end of the field is reached.
    while self.pos < len(self.field):
        # If the current character is an atom end delimiter, stop parsing.
        if self.field[self.pos] in atomends:
            break
        else:
            # Otherwise, append the current character to the atom.
            atomlist.append(self.field[self.pos])
        
        # Move to the next character.
        self.pos += 1

    # Return the concatenated string of the parsed atom.
    return EMPTYSTRING.join(atomlist)
","The getatom() function parses an RFC 2822 atom, which is a basic unit in an email address or header field. An atom consists of a sequence of characters that are not special characters like whitespace, punctuation, or parentheses. The function iterates through the input field and collects characters until it encounters an end delimiter specified by the atomends argument (or the default set of delimiters in self.atomends). The function stops parsing when one of these delimiters is found, and the atom is returned as a string. If no specific end delimiters are provided, the function defaults to using the atomends attribute of the object. This is particularly useful when parsing RFC 2822 headers like email addresses, where atoms represent individual components (e.g., user names or domain parts) separated by delimiters."
"def getphraselist(self):
    """"""Parse a sequence of RFC 2822 phrases.

    A phrase is a sequence of words, which are in turn either RFC 2822
    atoms or quoted-strings.  Phrases are canonicalized by squeezing all
    runs of continuous whitespace into one space.
    """"""
    # Initialize an empty list to store the parsed phrase components.
    plist = []

    # Continue parsing until the end of the field is reached.
    while self.pos < len(self.field):
        # Skip over any white space (FWS: folding white space).
        if self.field[self.pos] in self.FWS:
            self.pos += 1
        # If the current character is a double quote, parse a quoted string.
        elif self.field[self.pos] == '""':
            plist.append(self.getquote())
        # If the current character is an opening parenthesis, parse a comment.
        elif self.field[self.pos] == '(':
            self.commentlist.append(self.getcomment())
        # If the current character is one of the phrase-ending characters, stop parsing.
        elif self.field[self.pos] in self.phraseends:
            break
        else:
            # Otherwise, parse an atom and add it to the phrase list.
            plist.append(self.getatom(self.phraseends))

    # Return the list of parsed phrases.
    return plist","The getphraselist() function parses a sequence of RFC 2822 phrases. A phrase consists of a sequence of words, which are either atoms (basic string units) or quoted-strings. The function processes each word within a phrase, handling special cases such as whitespace and comments. It skips over any folding white space (FWS), which is a sequence of spaces or tabs that are typically folded across multiple lines in an RFC 2822 header. When a quoted string is encountered, the getquote() function is used to parse it. Similarly, if a comment is found (denoted by parentheses), it is processed using the getcomment() function. The function stops parsing when it encounters a phrase-ending character or reaches the end of the input. All runs of continuous whitespace within the phrase are squeezed into a single space to canonicalize the phrase, as required by RFC 2822. Finally, the function returns a list of parsed components of the phrase."
"def clone(self, **kw):
    """"""Return a new instance with specified attributes changed.

    The new instance has the same attribute values as the current object,
    except for the changes passed in as keyword arguments.

    """"""
    # Create a new instance of the class (same as the current object) without calling the __init__ method.
    newpolicy = self.__class__.__new__(self.__class__)

    # Copy all the attributes from the current object to the new instance.
    for attr, value in self.__dict__.items():
        # Set each attribute on the new instance with the same value.
        object.__setattr__(newpolicy, attr, value)

    # Iterate over the keyword arguments and update the corresponding attributes in the new instance.
    for attr, value in kw.items():
        # If the attribute is not present on the current object, raise an error.
        if not hasattr(self, attr):
            raise TypeError(
                ""{!r} is an invalid keyword argument for {}"".format(
                    attr, self.__class__.__name__))  # Provide an error message with the invalid attribute.

        # Set the updated attribute value on the new instance.
        object.__setattr__(newpolicy, attr, value)

    # Return the new instance with the updated attributes.
    return newpolicy",
"def _extend_docstrings(cls):
    # Check if the class has a docstring and if it starts with a '+'
    if cls.__doc__ and cls.__doc__.startswith('+'):
        # If the class docstring starts with '+', append it to the base class's docstring
        cls.__doc__ = _append_doc(cls.__bases__[0].__doc__, cls.__doc__)

    # Iterate over all attributes and methods in the class
    for name, attr in cls.__dict__.items():
        # If the attribute has a docstring that starts with '+'
        if attr.__doc__ and attr.__doc__.startswith('+'):
            # Search for the method in the class's method resolution order (MRO)
            for c in (c for base in cls.__bases__ for c in base.mro()):
                # Get the docstring of the same method or attribute in the base class
                doc = getattr(getattr(c, name), '__doc__')
                if doc:
                    # If a docstring is found, append the current docstring to it
                    attr.__doc__ = _append_doc(doc, attr.__doc__)
                    break  # Stop once the base class docstring is found
    # Return the class with extended docstrings
    return cls","The _extend_docstrings() function is designed to enhance the documentation of classes and their methods by combining the docstrings of the current class with those of its base classes. If the class has a docstring starting with the + symbol, it appends it to the docstring of its first base class. Similarly, if a method or attribute in the class has a docstring that starts with +, the function searches for the corresponding method in the method resolution order (MRO) of the class's base classes. Once the relevant base class's docstring is found, it appends the current method's docstring to it, effectively extending the documentation. This ensures that class and method docstrings are built upon and enriched by the documentation of their base classes. The function returns the updated class with the extended docstrings."
"def getDOMImplementation(name=None, features=()):
    """"""getDOMImplementation(name = None, features = ()) -> DOM implementation.

    Return a suitable DOM implementation. The name is either
    well-known, the module name of a DOM implementation, or None. If
    it is not None, imports the corresponding module and returns
    DOMImplementation object if the import succeeds.

    If name is not given, consider the available implementations to
    find one with the required feature set. If no implementation can
    be found, raise an ImportError. The features list must be a sequence
    of (feature, version) pairs which are passed to hasFeature.""""""

    import os  # Importing os to handle environment variables
    creator = None  # Variable to store the DOM implementation creator function
    
    # Check if the name matches a well-known DOM implementation
    mod = well_known_implementations.get(name)
    if mod:
        # Dynamically import the corresponding module
        mod = __import__(mod, {}, {}, ['getDOMImplementation'])
        return mod.getDOMImplementation()  # Return the implementation from the module
    
    # If a name is provided but not in well-known implementations, use the registered ones
    elif name:
        return registered[name]()  # Get the DOM implementation using the registered creator
    
    # Check environment variable ""PYTHON_DOM"" if no name is provided and environment is allowed
    elif not sys.flags.ignore_environment and ""PYTHON_DOM"" in os.environ:
        return getDOMImplementation(name=os.environ[""PYTHON_DOM""])  # Recursive call with the environment variable value
    
    # If no specific implementation name is given, search for one with the required features
    if isinstance(features, str):
        # Parse features from a string format to a structured format if needed
        features = _parse_feature_string(features)
    for creator in registered.values():
        dom = creator()  # Create a DOM implementation using the registered creator
        if _good_enough(dom, features):  # Check if the implementation meets the required features
            return dom  # Return the matching DOM implementation
    
    # Fallback to well-known implementations if registered ones do not satisfy the requirements
    for creator in well_known_implementations.keys():
        try:
            dom = getDOMImplementation(name=creator)  # Recursive call for each well-known implementation
        except Exception:  # Handle potential errors like ImportError or AttributeError
            continue  # Skip to the next implementation if an error occurs
        if _good_enough(dom, features):  # Check if the implementation meets the required features
            return dom  # Return the matching DOM implementation
    
    # Raise an error if no suitable implementation is found
    raise ImportError(""no suitable DOM implementation found"")","The getDOMImplementation function provides a mechanism to locate and return a suitable Document Object Model (DOM) implementation based on the given name or features. If a name is specified, it attempts to import the corresponding module and retrieve its getDOMImplementation function. If no name is provided, it evaluates registered or well-known implementations in sequence, checking if they meet the required features using the helper function _good_enough. The function also considers an environment variable PYTHON_DOM to determine the implementation if applicable. If no matching implementation is found, an ImportError is raised. The flexibility to dynamically load and validate implementations makes it a robust utility for XML and HTML processing in Python.





"
"def _parse_feature_string(s):
    """"""
    Parse a space-separated feature string into a tuple of (feature, version) pairs.

    Args:
        s (str): A string containing space-separated feature names and optional version numbers.

    Returns:
        tuple: A tuple of (feature, version) pairs. The version is None if not provided.

    Raises:
        ValueError: If a feature name starts with a numeric character, indicating invalid syntax.
    """"""
    features = []  # Initialize a list to store parsed (feature, version) pairs
    parts = s.split()  # Split the input string into a list of space-separated components
    i = 0  # Initialize the index for iterating over parts
    length = len(parts)  # Get the total number of components

    while i < length:
        feature = parts[i]  # Get the current feature name
        # Check if the feature name starts with a numeric character, which is invalid
        if feature[0] in ""0123456789"":
            raise ValueError(""bad feature name: %r"" % (feature,))
        i = i + 1  # Move to the next component

        version = None  # Default the version to None
        if i < length:  # Check if there are more components
            v = parts[i]  # Get the next component
            # If the next component starts with a numeric character, treat it as a version number
            if v[0] in ""0123456789"":
                i = i + 1  # Move the index past the version
                version = v  # Assign the version

        features.append((feature, version))  # Append the (feature, version) pair to the list

    return tuple(features)  # Convert the list to a tuple and return it","The _parse_feature_string function parses a string of space-separated feature names and optional version numbers into a structured format as a tuple of (feature, version) pairs. Each feature name is checked to ensure it does not start with a numeric character, as this would indicate invalid syntax. If a version number follows a feature, it is associated with that feature; otherwise, the version is set to None. This utility is designed for interpreting feature requirements in a standardized way, ensuring input validation and compatibility with subsequent processing logic. If the input string contains invalid feature names, the function raises a ValueError. This ensures robust handling of improperly formatted feature strings."
"def _parse_ns_name(builder, name):
    """"""
    Parse a namespace-qualified name into its components.

    Args:
        builder: An object with an `_intern_setdefault` method for interning strings.
        name (str): A string representing a namespace-qualified name in the format 'URI localname [prefix]'.

    Returns:
        tuple: A tuple containing (namespace URI, local name, prefix, qualified name).

    Raises:
        AssertionError: If the input string does not contain at least one space.
        ValueError: If the input string format is unsupported (e.g., more than two spaces or spaces in URIs).
    """"""
    # Assert the input name contains at least one space separating URI and localname
    assert ' ' in name

    # Split the input string into parts separated by spaces
    parts = name.split(' ')
    # Retrieve the builder's method for interning strings
    intern = builder._intern_setdefault

    if len(parts) == 3:
        # Parse the URI, local name, and prefix if all three are provided
        uri, localname, prefix = parts
        # Intern the prefix
        prefix = intern(prefix, prefix)
        # Create and intern the qualified name (prefix:localname)
        qname = ""%s:%s"" % (prefix, localname)
        qname = intern(qname, qname)
        # Intern the local name
        localname = intern(localname, localname)
    elif len(parts) == 2:
        # Parse the URI and local name when no prefix is provided
        uri, localname = parts
        prefix = EMPTY_PREFIX  # Assign a default empty prefix
        # Use the local name as the qualified name and intern it
        qname = localname = intern(localname, localname)
    else:
        # Raise an error for unsupported input formats
        raise ValueError(""Unsupported syntax: spaces in URIs not supported: %r"" % name)

    # Intern the URI and return the components as a tuple
    return intern(uri, uri), localname, prefix, qname
","The _parse_ns_name function processes a namespace-qualified name string, parsing it into its fundamental components: namespace URI, local name, prefix, and qualified name. The input must include at least one space to separate the namespace URI and local name. If a prefix is provided, it is included in the qualified name (e.g., prefix:localname); otherwise, a default empty prefix is used. The function utilizes the builder._intern_setdefault method to intern strings, ensuring efficient reuse of immutable string objects. Unsupported formats, such as strings with more than two spaces or spaces within the URI, result in a ValueError. This function is useful in XML processing or similar domains where namespace-qualified names are common and require structured handling."
"def _handle_white_text_nodes(self, node, info):
    """"""
    Remove ignorable whitespace text nodes from a given node's childNodes.

    Args:
        self: The instance of the object calling this method, which includes configuration options.
        node: The DOM node whose child text nodes will be inspected.
        info: An object containing type information about the node, with a method `isElementContent()`.

    Returns:
        None. Modifies the DOM node tree in place.
    """"""
    # Check if whitespace in element content should be preserved or if the node is not element content
    if (self._options.whitespace_in_element_content
            or not info.isElementContent()):
        return  # Do nothing if whitespace should be preserved or the content type does not require trimming.

    # Prepare to collect text nodes that contain only whitespace
    L = []  # List to store ignorable whitespace text nodes

    # Iterate over the children of the given node
    for child in node.childNodes:
        # Check if the child is a text node and contains only whitespace
        if child.nodeType == TEXT_NODE and not child.data.strip():
            L.append(child)  # Add the ignorable text node to the list

    # Remove all collected ignorable whitespace text nodes from the DOM tree
    for child in L:
        node.removeChild(child)  # Remove the text node from its parent","The _handle_white_text_nodes function is designed to process and clean up a DOM node's child nodes by removing ignorable whitespace text nodes. If the configuration (self._options.whitespace_in_element_content) allows preserving whitespace or if the node's type information (info.isElementContent()) indicates that the content should not be trimmed, the function exits early. Otherwise, it identifies text nodes that contain only whitespace by iterating through the node's childNodes. These ignorable nodes are collected in a list and subsequently removed from the DOM tree. This function is particularly useful in XML or HTML processing where ignorable whitespace might need to be trimmed to adhere to content model rules."
"def xml_decl_handler(self, version, encoding, standalone):
    """"""
    Handle the XML declaration in a document and update the document's metadata.

    Args:
        self: The instance of the object calling this method, which includes a `document` attribute.
        version (str): The XML version string, e.g., ""1.0"" or ""1.1"".
        encoding (str): The character encoding of the XML document, e.g., ""UTF-8"".
        standalone (int): Indicates if the XML document is standalone. 
                          Values: 1 (True), 0 (False), or -1 (not specified).

    Returns:
        None. Modifies the `document` attribute of the object in place.
    """"""
    # Set the XML version for the document
    self.document.version = version
    # Set the character encoding for the document
    self.document.encoding = encoding

    # Handle the standalone declaration. The pyexpat API uses an integer representation:
    # - standalone >= 0 indicates that the standalone attribute is explicitly set
    # - 1 means standalone=""yes"", 0 means standalone=""no""
    if standalone >= 0:
        # If standalone is 1, set document.standalone to True
        if standalone:
            self.document.standalone = True
        # If standalone is 0, set document.standalone to False
        else:
            self.document.standalone = False","The xml_decl_handler function processes the XML declaration of a document, specifically handling the version, encoding, and standalone attributes, and updates the associated document's metadata accordingly. The function sets the document.version and document.encoding attributes directly from the provided arguments. It also interprets the standalone parameter, which follows the pyexpat API's integer representation: 1 for standalone=""yes"", 0 for standalone=""no"", and -1 if the standalone attribute is not specified. When standalone is explicitly set, the function assigns a boolean value (True or False) to the document.standalone attribute. This function integrates the XML declaration into the document model, ensuring consistency with the document's metadata and its processing logic."
"def startContainer(self, node):
    """"""
    Evaluate whether to process a container node based on its type and a user-defined filter.

    Args:
        self: The instance of the object calling this method, containing `filter` and `_nodetype_mask`.
        node: The container node to evaluate, with a `nodeType` attribute.

    Returns:
        int: A value indicating whether the node should be accepted, skipped, or rejected.

    Raises:
        ParseEscape: If the filter returns a `FILTER_INTERRUPT` value.
        ValueError: If the filter returns a value not in the allowed set of filter return values.
    """"""
    # Retrieve the mask corresponding to the node's type
    mask = self._nodetype_mask[node.nodeType]

    # Check if the node type is included in what the filter is configured to process
    if self.filter.whatToShow & mask:
        # Call the filter's startContainer method to determine processing behavior
        val = self.filter.startContainer(node)

        # Raise an exception if the filter signals to interrupt processing
        if val == FILTER_INTERRUPT:
            raise ParseEscape

        # Validate the return value of the filter against allowed responses
        if val not in _ALLOWED_FILTER_RETURNS:
            raise ValueError(
                ""startContainer() returned illegal value: "" + repr(val))

        # Return the filter's response if valid
        return val
    else:
        # Default to accepting the node if the type is not part of the filter's scope
        return FILTER_ACCEPT","The startContainer function determines how a container node in a document tree should be processed based on its type and a user-defined filter. It first retrieves a mask corresponding to the node's type and checks if this type is included in the filter's whatToShow configuration. If included, it invokes the filter's startContainer method, which returns a directive (FILTER_ACCEPT, FILTER_SKIP, or FILTER_REJECT) or signals interruption (FILTER_INTERRUPT). The function raises a ParseEscape exception for interruptions and validates the filter's return value against a predefined set of allowed values, raising a ValueError for invalid responses. If the node type is outside the filter's scope, the function defaults to accepting the node. This ensures robust and customizable node filtering for document processing."
"def end_element_handler(self, *args):
    """"""
    Handle the end of an XML element during parsing, managing parser state and nested element levels.

    Args:
        self: The instance of the object calling this method, which maintains parsing state.
        *args: Additional arguments passed by the parser (not used in this method).

    Returns:
        None. Modifies the parser's state and event handlers in place.
    """"""
    # Check if we are at the root level of the element hierarchy
    if self._level == 0:
        # Restore the old element handlers when exiting the root level
        parser = self._builder._parser  # Get the current parser instance from the builder
        self._builder.install(parser)  # Reinstall the builder's default setup
        # Restore the previous StartElementHandler and EndElementHandler
        parser.StartElementHandler = self._old_start
        parser.EndElementHandler = self._old_end
    else:
        # If still within nested elements, decrement the nesting level counter
        self._level = self._level - 1","The end_element_handler function manages parser state when an XML element's end tag is encountered. It uses a _level attribute to track the nesting depth of elements during parsing. If _level reaches 0, indicating that the parser is exiting the root level, the function restores the previous StartElementHandler and EndElementHandler to their original configurations, ensuring proper parser behavior after the current handler's scope ends. Otherwise, it decrements the _level counter, maintaining accurate tracking of nested elements. This function is critical for controlling event handler transitions and ensuring seamless parsing of XML documents with nested structures."
"def parseString(self, string):
    """"""
    Parse a document fragment from a string and return the fragment node.

    Args:
        self: The instance of the object calling this method, containing the document and parsing state.
        string (str): The input string containing the document fragment to be parsed.

    Returns:
        The fragment node created from the parsed string.

    Raises:
        Re-raises any exceptions encountered during parsing after resetting the parser state.
    """"""
    # Store the input string as the current source
    self._source = string

    # Get a parser instance specific to this document
    parser = self.getParser()

    # Retrieve the doctype (if any) from the original document
    doctype = self.originalDocument.doctype
    ident = """"

    # If a doctype is present, construct an identifier and subset
    if doctype:
        # Get the internal subset or generate declarations as fallback
        subset = doctype.internalSubset or self._getDeclarations()

        # Build the identifier based on publicId or systemId if available
        if doctype.publicId:
            ident = 'PUBLIC ""%s"" ""%s""' % (doctype.publicId, doctype.systemId)
        elif doctype.systemId:
            ident = 'SYSTEM ""%s""' % doctype.systemId
    else:
        # No doctype present; the subset is empty
        subset = """"

    # Fetch namespace declarations from the node's ancestors
    nsattrs = self._getNSattrs()

    # Construct the document using a template with doctype, subset, and namespaces
    document = _FRAGMENT_BUILDER_TEMPLATE % (ident, subset, nsattrs)

    try:
        # Parse the constructed document using the parser
        parser.Parse(document, True)
    except:
        # Reset the parser state if an error occurs
        self.reset()
        # Re-raise the exception for handling by the caller
        raise

    # Retrieve the parsed fragment node
    fragment = self.fragment

    # Reset the parser state after successful parsing
    self.reset()

    # Return the parsed fragment node
    return fragment","The parseString function parses an XML document fragment provided as a string and returns the corresponding fragment node. It initializes a parser and constructs an intermediate document by incorporating the document type declaration (if available), namespace declarations from ancestor nodes, and the provided string. The constructed document is then processed using the parser. If a parsing error occurs, the parser state is reset, and the exception is propagated. Upon successful parsing, the function retrieves and returns the fragment node, resetting the parser state afterward. This method handles document fragments with optional doctype and namespace support, enabling dynamic XML parsing and construction."
"def external_entity_ref_handler(self, context, base, systemId, publicId):
    """"""
    Handle external entity references during parsing, switching between 
    the current document and a temporary fragment for subtree parsing.

    Args:
        self: The instance of the object calling this method, containing parsing state and document references.
        context: The parsing context provided by the Expat parser.
        base: The base URI for resolving the systemId.
        systemId (str): The system identifier of the external entity.
        publicId (str): The public identifier of the external entity.

    Returns:
        int: Returns -1 if the entity is a special internal fragment,
        otherwise delegates handling to the parent class.
    """"""
    # Check if the system ID matches the internal fragment system ID
    if systemId == _FRAGMENT_BUILDER_INTERNAL_SYSTEM_ID:
        # Save the current document and node to restore later
        old_document = self.document
        old_cur_node = self.curNode

        # Create a parser for the external entity
        parser = self._parser.ExternalEntityParserCreate(context)

        # Switch to the original document and create a new fragment for parsing
        self.document = self.originalDocument
        self.fragment = self.document.createDocumentFragment()
        self.curNode = self.fragment

        try:
            # Parse the source content into the new fragment
            parser.Parse(self._source, True)
        finally:
            # Restore the original document and node, and clear the source
            self.curNode = old_cur_node
            self.document = old_document
            self._source = None

        # Indicate that the special internal fragment was processed
        return -1
    else:
        # Delegate handling of other external entities to the parent class
        return ExpatBuilder.external_entity_ref_handler(
            self, context, base, systemId, publicId)","The external_entity_ref_handler function processes external entity references encountered during XML parsing. If the systemId corresponds to a special internal fragment identifier, it temporarily switches the parser's context to parse the subtree into a document fragment. The function uses a separate parser instance created for the external entity to process the input and assigns the parsed content to a fragment node, ensuring the original document's state is restored afterward. For other systemId values, it delegates the handling to the parent class. This function ensures that specific internal entity references are handled locally while maintaining compatibility with the broader XML structure."
"def start_element_handler(self, name, attributes):
    """"""
    Handle the start of an element during XML parsing, creating a new element node 
    and appending it to the current node.

    Args:
        self: The instance of the object calling this method, containing the document and node state.
        name (str): The qualified name of the element (potentially including a namespace).
        attributes (dict): A dictionary of attributes for the element.

    Returns:
        None: This function modifies the document structure but does not return any value.
    """"""
    # If the element name contains a space, it indicates a namespaced element
    if ' ' in name:
        # Parse the namespace, local name, prefix, and qualified name from the element name
        uri, localname, prefix, qname = _parse_ns_name(self, name)
    else:
        # If no namespace, use defaults for uri and prefix
        uri = EMPTY_NAMESPACE
        qname = name
        localname = None
        prefix = EMPTY_PREFIX

    # Create a new Element node using the parsed information
    node = minidom.Element(qname, uri, prefix, localname)
    
    # Set the owner document of the node to the current document
    node.ownerDocument = self.document

    # Append the new node as a child of the current node
    _append_child(self.curNode, node)

    # Update the current node to be the newly created element
    self.curNode = node","The start_element_handler function is responsible for handling the start of an XML element during the parsing process. It first checks whether the element's name includes a namespace (indicated by a space in the name). If so, it uses the _parse_ns_name function to extract the namespace, local name, prefix, and qualified name. For elements without a namespace, default values are used for the namespace and prefix. It then creates a new Element node using the qualified name and namespace details, assigns the current document as the node's owner, and appends the node as a child of the current node in the document structure. Finally, it updates the curNode to point to the newly created element, allowing further child nodes to be appended to it."
"def _getNSattrs(self):
    """"""
    Return a string of namespace attributes from this element and its ancestors.

    This function traverses up the element's ancestry, collecting namespace 
    declarations from each ancestor element and returns them as a string 
    that can be used in an XML context.

    Returns:
        str: A string containing namespace declarations (e.g., 'xmlns:prefix=""uri""').
    """"""
    # Placeholder for the actual namespace attributes string
    attrs = """"
    
    # Start from the current element's context
    context = self.context
    # List to track prefixes we've already added to avoid duplicates
    L = []

    # Traverse up the element's ancestors
    while context:
        # If the context has namespace prefix/URI mappings
        if hasattr(context, '_ns_prefix_uri'):
            for prefix, uri in context._ns_prefix_uri.items():
                # Add a new namespace declaration for every unique prefix found
                if prefix in L:
                    continue
                L.append(prefix)
                
                # Determine the correct namespace declaration name
                if prefix:
                    declname = ""xmlns:"" + prefix
                else:
                    declname = ""xmlns""
                
                # Format the declaration and append to the attrs string
                if attrs:
                    attrs = ""%s\n    %s='%s'"" % (attrs, declname, uri)
                else:
                    attrs = "" %s='%s'"" % (declname, uri)

        # Move to the parent context to gather namespaces from ancestors
        context = context.parentNode
    
    return attrs","The _getNSattrs function is responsible for constructing a string containing namespace declarations for the current element and all its ancestor elements. The function starts from the current context and traverses upwards through the ancestry of the element. For each ancestor that has namespace declarations (stored in the _ns_prefix_uri attribute), it adds the corresponding xmlns attributes to a string, ensuring that each namespace prefix is only included once. The resulting string of namespace declarations is returned in a format suitable for XML documents (e.g., xmlns:prefix=""uri""). This string can be used to include the necessary namespace information when serializing the element into XML."
"def parseFragment(file, context, namespaces=True):
    """"""
    Parse a fragment of a document, given the context from which it was originally extracted.
    The context should be the parent node of the node(s) that are in the fragment.

    Args:
        file (str or file-like object): The input file, either a file name or an open file object to parse.
        context (Node): The parent node that holds the context for the fragment.
        namespaces (bool): Whether to include namespace processing during parsing. Defaults to True.

    Returns:
        Node: The resulting parsed document fragment.
    """"""
    # Choose the appropriate builder based on whether namespaces are required
    if namespaces:
        builder = FragmentBuilderNS(context)  # Namespace-aware builder
    else:
        builder = FragmentBuilder(context)  # Standard builder without namespace support

    # If 'file' is a string (file path), open it and parse
    if isinstance(file, str):
        with open(file, 'rb') as fp:
            result = builder.parseFile(fp)
    else:
        # If 'file' is already a file-like object, parse directly
        result = builder.parseFile(file)
    
    # Return the resulting document fragment
    return result","The parseFragment function is used to parse a fragment of an XML document, given a specific context (the parent node of the fragment). The file argument can either be a file path (string) or an open file object. The context is the parent node that provides the surrounding context for the fragment being parsed. The function also includes an optional namespaces argument, which determines whether the parsing should consider XML namespaces. If namespaces is True, it uses a namespace-aware builder (FragmentBuilderNS), otherwise, it uses a standard builder (FragmentBuilder). The function opens the file (if it is a string) or uses the provided file object, and then parses the file using the appropriate builder. It returns the parsed document fragment as a Node.





"
"def toprettyxml(self, indent=""\t"", newl=""\n"", encoding=None, standalone=None):
    # Check if encoding is provided. If not, use StringIO for in-memory string handling.
    if encoding is None:
        writer = io.StringIO()
    else:
        # If encoding is provided, use TextIOWrapper to wrap BytesIO and handle the encoding.
        writer = io.TextIOWrapper(io.BytesIO(),
                                  encoding=encoding,
                                  errors=""xmlcharrefreplace"",  # Replace invalid characters with XML char references
                                  newline='\n')  # Ensure line breaks are consistent.

    # Check if the node is a DOCUMENT_NODE
    if self.nodeType == Node.DOCUMENT_NODE:
        # If it is a document node, pass the encoding to include it in the XML header
        self.writexml(writer, """", indent, newl, encoding, standalone)
    else:
        # If it is not a document node, write the XML without encoding and standalone options
        self.writexml(writer, """", indent, newl)

    # Return the output as a string, depending on whether encoding was provided
    if encoding is None:
        # If no encoding, return the content from StringIO.
        return writer.getvalue()
    else:
        # If encoding is provided, detach the writer and return the content with the specified encoding.
        return writer.detach().getvalue()","The toprettyxml function is designed to produce a well-formatted (pretty-printed) XML string representation of an XML node, typically used for output or serialization. It supports various formatting options, including indentation style, newline characters, encoding, and the inclusion of an XML declaration for document nodes."
"def appendChild(self, node):
    # Check if the node to append is a DOCUMENT_FRAGMENT_NODE
    if node.nodeType == self.DOCUMENT_FRAGMENT_NODE:
        # If it's a document fragment, recursively append each child node
        for c in tuple(node.childNodes):
            self.appendChild(c)
        # Return the document fragment itself, as per the DOM's unspecified behavior
        return node

    # Ensure the node is of a valid type to be appended as a child
    if node.nodeType not in self._child_node_types:
        raise xml.dom.HierarchyRequestErr(
            ""%s cannot be child of %s"" % (repr(node), repr(self)))

    # Clear the ID cache if the node type requires it (i.e., has children)
    elif node.nodeType in _nodeTypes_with_children:
        _clear_id_cache(self)

    # If the node already has a parent, remove it from the old parent
    if node.parentNode is not None:
        node.parentNode.removeChild(node)

    # Append the new child node to the current node
    _append_child(self, node)

    # Set the new child's nextSibling to None (since it's the last child)
    node.nextSibling = None

    # Return the newly appended node
    return node",The appendChild function is used to append a new child node to the list of children of the current node. It modifies the DOM structure by adding a node to the end of the list of children. The method ensures that the node to be appended is of a valid type and handles various cases related to document fragments and existing parent-child relationships.
"def replaceChild(self, newChild, oldChild):
    # Handle the case where the new child is a DOCUMENT_FRAGMENT_NODE
    if newChild.nodeType == self.DOCUMENT_FRAGMENT_NODE:
        # Save the reference to the sibling of the old child
        refChild = oldChild.nextSibling
        # Remove the old child from the current node
        self.removeChild(oldChild)
        # Insert the new child (fragment) before the reference child
        return self.insertBefore(newChild, refChild)

    # Ensure the new child is of a valid type to be appended to the current node
    if newChild.nodeType not in self._child_node_types:
        raise xml.dom.HierarchyRequestErr(
            ""%s cannot be child of %s"" % (repr(newChild), repr(self)))

    # If the new child is the same as the old child, do nothing
    if newChild is oldChild:
        return

    # If the new child already has a parent, remove it from the old parent
    if newChild.parentNode is not None:
        newChild.parentNode.removeChild(newChild)

    try:
        # Find the index of the old child in the current node's child nodes
        index = self.childNodes.index(oldChild)
    except ValueError:
        # Raise an error if the old child is not found
        raise xml.dom.NotFoundErr()

    # Replace the old child with the new child at the same position
    self.childNodes[index] = newChild
    # Set the parent node of the new child to be the current node
    newChild.parentNode = self
    # Set the parent node of the old child to None, as it is removed
    oldChild.parentNode = None

    # Clear the ID cache if necessary (if either node has children)
    if (newChild.nodeType in _nodeTypes_with_children
        or oldChild.nodeType in _nodeTypes_with_children):
        _clear_id_cache(self)

    # Update the sibling references for both the new and old child
    newChild.nextSibling = oldChild.nextSibling
    newChild.previousSibling = oldChild.previousSibling
    oldChild.nextSibling = None
    oldChild.previousSibling = None

    # Update the previous sibling's nextSibling and the next sibling's previousSibling
    if newChild.previousSibling:
        newChild.previousSibling.nextSibling = newChild
    if newChild.nextSibling:
        newChild.nextSibling.previousSibling = newChild

    # Return the old child, now replaced
    return oldChild","The replaceChild function replaces an existing child node (oldChild) with a new child node (newChild) in the DOM structure. This method ensures the integrity of the DOM hierarchy by correctly handling the insertion of the new child, updating sibling references, and removing the old child from the parent node."
"def normalize(self):
    # Initialize an empty list to collect the normalized child nodes
    L = []
    
    # Iterate through the child nodes of the current node
    for child in self.childNodes:
        if child.nodeType == Node.TEXT_NODE:
            # If the text node is empty, discard it
            if not child.data:
                if L:
                    # If there are previous nodes, update the sibling reference
                    L[-1].nextSibling = child.nextSibling
                if child.nextSibling:
                    # Update the previousSibling of the next sibling
                    child.nextSibling.previousSibling = child.previousSibling
                # Unlink the empty child node from the parent
                child.unlink()
            elif L and L[-1].nodeType == child.nodeType:
                # If the last node in the list is also a text node, collapse them
                node = L[-1]
                node.data = node.data + child.data
                node.nextSibling = child.nextSibling
                if child.nextSibling:
                    # Update the previousSibling of the next sibling
                    child.nextSibling.previousSibling = node
                # Unlink the collapsed text node
                child.unlink()
            else:
                # Otherwise, add the text node to the list
                L.append(child)
        else:
            # For non-text nodes, just append them to the list
            L.append(child)
            if child.nodeType == Node.ELEMENT_NODE:
                # Recursively normalize child elements
                child.normalize()
    
    # Update the child nodes of the current node to the normalized list
    self.childNodes[:] = L","The normalize function is used to clean up and consolidate the child nodes of an XML element. It ensures that consecutive text nodes are merged into one and removes any empty text nodes, resulting in a more efficient and well-structured DOM. This method also recursively normalizes child elements to ensure the entire subtree is processed."
"def setUserData(self, key, data, handler):
    # Initialize a variable to hold any existing data for the given key
    old = None
    
    try:
        # Attempt to access the _user_data attribute (a dictionary storing user data)
        d = self._user_data
    except AttributeError:
        # If _user_data doesn't exist, create an empty dictionary for it
        d = {}
        self._user_data = d
    
    # Check if the key already exists in the user data dictionary
    if key in d:
        # If the key exists, store the current value of data before modification
        old = d[key][0]
    
    if data is None:
        # If data is None, remove the entry for the given key from the dictionary
        handler = None  # Ignore any handler if data is None
        if old is not None:
            del d[key]  # Delete the key-value pair
    else:
        # Otherwise, store the new data and the handler in the dictionary
        d[key] = (data, handler)
    
    # Return the old data value (if it existed)
    return old","The setUserData function is used to associate custom user data with a key in the current object. It allows adding, updating, or removing user data based on the provided key. Additionally, it can associate a handler function with the data, which can be used to process the data later."
"def _write_data(writer, text, attr):
    """"""Writes datachars to writer, handling special characters.""""""
    
    # If the text is empty, there's nothing to write
    if not text:
        return
    
    # Check and replace special characters for safe XML output
    if ""&"" in text:
        text = text.replace(""&"", ""&amp;"")  # Replace '&' with '&amp;'
    if ""<"" in text:
        text = text.replace(""<"", ""&lt;"")  # Replace '<' with '&lt;'
    if "">"" in text:
        text = text.replace("">"", ""&gt;"")  # Replace '>' with '&gt;'
    
    # If the text is for an attribute, escape additional characters
    if attr:
        if '""' in text:
            text = text.replace('""', ""&quot;"")  # Replace '""' with '&quot;'
        if ""\r"" in text:
            text = text.replace(""\r"", ""&#13;"")  # Replace carriage return with '&#13;'
        if ""\n"" in text:
            text = text.replace(""\n"", ""&#10;"")  # Replace newline with '&#10;'
        if ""\t"" in text:
            text = text.replace(""\t"", ""&#9;"")  # Replace tab with '&#9;'
    
    # Write the processed text to the writer
    writer.write(text)","The _write_data function is responsible for writing a string (text) to a writer object, ensuring that any special XML characters are correctly escaped to maintain proper XML syntax."
"def _get_elements_by_tagName_helper(parent, name, rc):
    """"""
    Recursively traverses the DOM tree starting from the 'parent' node
    to find all elements with the specified 'name'. The results are 
    appended to the 'rc' list.

    Args:
        parent: The parent node from which to begin the search.
        name: The tag name to search for. If ""*"" is passed, all elements are returned.
        rc: A list to accumulate the matching elements.

    Returns:
        A list of matching elements (rc).
    """"""
    
    # Traverse each child node of the parent node
    for node in parent.childNodes:
        # Check if the node is an element node
        if node.nodeType == Node.ELEMENT_NODE:
            # If 'name' is ""*"" or the node's tagName matches the specified name, append it
            if name == ""*"" or node.tagName == name:
                rc.append(node)
        
        # Recursively call the function for the node's children
        _get_elements_by_tagName_helper(node, name, rc)
    
    # Return the list of matching elements
    return rc","The _get_elements_by_tagName_helper function recursively traverses the DOM tree, starting from a specified parent node, to find all elements that match a given tag name (name). It accumulates the matching elements in the list rc, which is passed by reference."
"def _get_elements_by_tagName_ns_helper(parent, nsURI, localName, rc):
    """"""
    Recursively traverses the DOM tree starting from the 'parent' node
    to find all elements with a matching namespace URI and local name.
    The results are appended to the 'rc' list.

    Args:
        parent: The parent node from which to begin the search.
        nsURI: The namespace URI to search for. If ""*"" is passed, all namespaces are considered.
        localName: The local name of the element to search for. If ""*"" is passed, all local names are considered.
        rc: A list to accumulate the matching elements.

    Returns:
        A list of matching elements (rc).
    """"""
    
    # Traverse each child node of the parent node
    for node in parent.childNodes:
        # Check if the node is an element node
        if node.nodeType == Node.ELEMENT_NODE:
            # If localName is ""*"" or the node's localName matches the specified localName,
            # and if nsURI is ""*"" or the node's namespaceURI matches the specified nsURI, append it
            if (localName == ""*"" or node.localName == localName) and \
               (nsURI == ""*"" or node.namespaceURI == nsURI):
                rc.append(node)
        
        # Recursively call the function for the node's children
        _get_elements_by_tagName_ns_helper(node, nsURI, localName, rc)
    
    # Return the list of matching elements
    return rc","The _get_elements_by_tagName_ns_helper function is designed to recursively traverse a DOM tree starting from a given parent node, searching for elements that match both a specified namespace URI (nsURI) and local name (localName). The matching elements are accumulated in the rc list, which is passed by reference."
"def _get_isId(self):
    """"""
    Checks whether the current element is considered an ID based on the document's element info.

    Returns:
        bool: True if the element is an ID, False otherwise.
    """"""
    # If the '_is_id' attribute is set to True, return True immediately
    if self._is_id:
        return True

    # Get the owner document and the owner element of the current node
    doc = self.ownerDocument
    elem = self.ownerElement

    # If either the document or the element is None, the ID cannot be determined
    if doc is None or elem is None:
        return False

    # Get the element information from the document
    info = doc._get_elem_info(elem)

    # If the element information is None, the ID cannot be determined
    if info is None:
        return False

    # If the element has a namespace URI, check if it's an ID in the specific namespace
    if self.namespaceURI:
        return info.isIdNS(self.namespaceURI, self.localName)
    else:
        # Otherwise, check if the element is an ID based on its nodeName
        return info.isId(self.nodeName)","The _get_isId function is designed to determine whether the current node (element) is considered an ""ID"" according to the DOM structure and document-specific rules. The ID of an element is typically used to uniquely identify it within an XML document, often with the constraint that it must be unique across the document or a specific XML namespace."
"def _get_schemaType(self):
    """"""
    Retrieves the schema type for the current element's attribute based on the document's element info.

    Returns:
        str: The schema type of the element, or _no_type if it cannot be determined.
    """"""
    # Get the owner document and owner element of the current node
    doc = self.ownerDocument
    elem = self.ownerElement

    # If either the document or the element is None, return the default _no_type
    if doc is None or elem is None:
        return _no_type

    # Get the element information from the document
    info = doc._get_elem_info(elem)

    # If the element information is None, return _no_type
    if info is None:
        return _no_type

    # If the element has a namespace URI, get the attribute type for the element in that namespace
    if self.namespaceURI:
        return info.getAttributeTypeNS(self.namespaceURI, self.localName)
    else:
        # Otherwise, get the attribute type based on the node name
        return info.getAttributeType(self.nodeName)","The _get_schemaType function is designed to retrieve the schema type associated with an attribute of the current element, based on the element's information stored in the document. This schema type is important for validating attributes against a specified XML schema, ensuring that the data conforms to expected types (e.g., string, integer, date)."
"def writexml(self, writer, indent="""", addindent="""", newl=""""):
    """"""
    Writes an XML element to a file-like object.

    This function takes an XML element and writes it as a well-formed XML
    element to the given writer object (such as a file or StringIO).
    
    Args:
        writer: A file-like object that supports the `write()` method, such as 
                a file or StringIO object.
        indent (str): The current indentation level for the element.
        addindent (str): The additional indentation to add for child elements.
        newl (str): The newline character(s) to use when writing elements.
    """"""
    # Write the opening tag with current indentation and the element's tag name
    writer.write(indent + ""<"" + self.tagName)

    # Get attributes of the element
    attrs = self._get_attributes()

    # Write each attribute as name=""value""
    for a_name in attrs.keys():
        writer.write("" %s=\"""" % a_name)
        # Write the attribute value, handling special characters
        _write_data(writer, attrs[a_name].value, True)
        writer.write(""\"""")

    # If the element has child nodes, write them recursively
    if self.childNodes:
        writer.write("">"")
        if len(self.childNodes) == 1 and self.childNodes[0].nodeType in (
                Node.TEXT_NODE, Node.CDATA_SECTION_NODE):
            # If there is only one text or CDATA node, write it directly
            self.childNodes[0].writexml(writer, '', '', '')
        else:
            writer.write(newl)
            # For other child nodes, write them recursively with indentation
            for node in self.childNodes:
                node.writexml(writer, indent + addindent, addindent, newl)
            writer.write(indent)
        # Write the closing tag for the element
        writer.write(""</%s>%s"" % (self.tagName, newl))
    else:
        # If the element has no children, write the self-closing tag
        writer.write(""/>%s"" % (newl))","The writexml function writes an XML element to a file-like object in a well-formed XML format. It serializes the element's tag, its attributes, and any child nodes recursively to an output stream (such as a file or StringIO object). This function is commonly used in XML manipulation libraries to output serialized XML data."
"def setIdAttributeNode(self, idAttr):
    """"""
    Sets the specified attribute as an ID attribute for the element.

    This method assigns an attribute node as the ID attribute for the current 
    element. It performs validation checks to ensure that the operation is allowed, 
    and modifies internal data structures accordingly.

    Args:
        idAttr (Attr): The attribute node to be set as the ID attribute for this element.

    Raises:
        xml.dom.NotFoundErr: If the attribute node is not found or does not belong to the element.
        xml.dom.NoModificationAllowedErr: If the modification is not allowed (e.g., due to 
                                            containment restrictions).
    """"""
    # Check if the attribute is valid and belongs to the current element
    if idAttr is None or not self.isSameNode(idAttr.ownerElement):
        raise xml.dom.NotFoundErr()

    # Check if the element is contained within an entity reference, which does not allow modifications
    if _get_containing_entref(self) is not None:
        raise xml.dom.NoModificationAllowedErr()

    # Set the attribute as an ID if it is not already marked as such
    if not idAttr._is_id:
        idAttr._is_id = True  # Mark the attribute as an ID
        self._magic_id_nodes += 1  # Increment the count of ID attributes for the element
        self.ownerDocument._magic_id_count += 1  # Increment the total count of ID attributes in the document
        _clear_id_cache(self)  # Clear the cache to ensure the ID-related data is consistent
","The setIdAttributeNode function is used to designate an attribute as the ID attribute for an XML element. This function includes a series of checks to ensure that the attribute can be safely set as an ID. If successful, the function updates internal counters and flags, ensuring that the attribute is correctly recognized as the element's ID."
"def _set_attribute_node(element, attr):
    """"""
    Sets the specified attribute node for the given element.

    This function associates an attribute node with an element and updates
    the element's internal attribute storage, handling any necessary cache
    clearing to maintain consistency. It also creates a circular reference 
    between the element and the attribute, which is handled later to avoid memory leaks.

    Args:
        element (Element): The element to which the attribute will be added.
        attr (Attr): The attribute node to be added to the element.
    """"""
    # Clear any cached ID information associated with the element to ensure consistency
    _clear_id_cache(element)

    # Ensure the element has an _attrs dictionary to store attributes
    element._ensure_attributes()

    # Add the attribute to the element's attribute dictionaries
    element._attrs[attr.name] = attr  # Add to the regular attribute dictionary (by name)
    element._attrsNS[(attr.namespaceURI, attr.localName)] = attr  # Add to the namespace-based attribute dictionary

    # This creates a circular reference between the element and the attribute
    # (element -> attribute -> element). This circular reference is resolved later
    # by breaking the cycle with Element.unlink().
    attr.ownerElement = element  # Set the element as the owner of the attribute","The _set_attribute_node function is responsible for associating an attribute node (attr) with an element (element). It performs the following actions:

Cache Clearing: First, it clears any cached ID-related data for the element by calling _clear_id_cache(element). This ensures that the element's internal state remains consistent when adding or removing attributes.

Ensure Attributes: It calls element._ensure_attributes() to ensure that the element has an attribute dictionary where attributes can be stored. This method ensures that the element is ready to store new attributes.

Adding the Attribute: The function then adds the attribute to the element's internal storage:

It stores the attribute in the _attrs dictionary using the attribute's name as the key.
It also stores the attribute in the _attrsNS dictionary using a tuple of the attribute's namespaceURI and localName as the key.
Circular Reference: The function creates a circular reference by setting the ownerElement of the attribute to the current element. This means that the attribute now points back to the element as its owner. This circular reference could potentially cause memory leaks, but it is resolved later using the Element.unlink() method to break the cycle."
"def substringData(self, offset, count):
    """"""
    Returns a substring of the data starting at the specified offset with the given length.

    This method retrieves a portion of the data starting at the specified offset and continuing
    for the specified count of characters. If the offset or count is out of bounds or negative, 
    an error is raised.

    Args:
        offset (int): The starting index of the substring.
        count (int): The number of characters to extract from the data.

    Returns:
        str: A substring of the data starting at the offset and of the specified length.

    Raises:
        xml.dom.IndexSizeErr: If the offset is negative, beyond the end of the data, or if count is negative.
    """"""
    # Check if the offset is negative and raise an error if so
    if offset < 0:
        raise xml.dom.IndexSizeErr(""offset cannot be negative"")

    # Check if the offset is beyond the end of the data
    if offset >= len(self.data):
        raise xml.dom.IndexSizeErr(""offset cannot be beyond end of data"")

    # Check if the count is negative and raise an error if so
    if count < 0:
        raise xml.dom.IndexSizeErr(""count cannot be negative"")

    # Return the substring of data from the specified offset and count
    return self.data[offset:offset+count]","The substringData method is designed to extract a substring from an internal data string, starting at a specific offset and continuing for a specified length (count). It performs several validation checks to ensure that both the offset and count are valid.

Offset Check: The method checks if the offset is less than 0, raising an IndexSizeErr if this is the case. It also verifies that the offset does not exceed the length of the data string.

Count Check: It checks if the count is less than 0, and raises an IndexSizeErr if the count is negative.

Substring Extraction: Once the checks pass, the method extracts and returns a substring from the data, starting at the specified offset and continuing for the specified count of characters."
"def splitText(self, offset):
    """"""
    Splits the text node at the specified offset, creating a new text node
    containing the remainder of the data.

    This method splits the text of the current text node into two parts. The original
    text node will retain the portion of the text before the specified offset, and a new
    text node will be created containing the text starting at the offset. If the split
    occurs within a parent node, the new text node is inserted immediately after the
    current text node.

    Args:
        offset (int): The index at which to split the text.

    Returns:
        newText: A new text node containing the substring from the offset to the end.

    Raises:
        xml.dom.IndexSizeErr: If the offset is negative or beyond the length of the text data.
    """"""
    # Check if the offset is out of range
    if offset < 0 or offset > len(self.data):
        raise xml.dom.IndexSizeErr(""illegal offset value"")

    # Create a new text node and initialize its data
    newText = self.__class__()
    newText.data = self.data[offset:]
    newText.ownerDocument = self.ownerDocument

    # Get the next sibling of the current node
    next = self.nextSibling

    # If the current node has a parent and is part of the parent's child nodes
    if self.parentNode and self in self.parentNode.childNodes:
        # Insert the new text node after the current node or at the end of the parent node's children
        if next is None:
            self.parentNode.appendChild(newText)
        else:
            self.parentNode.insertBefore(newText, next)

    # Update the original text node to keep the data before the offset
    self.data = self.data[:offset]

    # Return the new text node containing the remaining data
    return newText",
"def load_stubs(self, log_mem=False):
        """"""Load all events in their `stub` (name, alias, etc only) form.

        Used in `update` mode.
        """"""
        # Initialize parameter related to diagnostic output of memory usage
        if log_mem:
            import psutil
            process = psutil.Process(os.getpid())
            rss = process.memory_info().rss
            LOG_MEMORY_INT = 1000
            MEMORY_LIMIT = 1000.0

        def _add_stub_manually(_fname):
            """"""Create and add a 'stub' by manually loading parameters from
            JSON files.

            Previously this was done by creating a full `Entry` instance, then
            using the `Entry.get_stub()` method to trim it down.  This was very
            slow and memory intensive, hence this improved approach.
            """"""
            # FIX: should this be ``fi.endswith(``.gz')`` ?
            fname = uncompress_gz(_fname) if '.gz' in _fname else _fname

            stub = None
            stub_name = None
            with codecs.open(fname, 'r') as jfil:
                # Load the full JSON file
                data = json.load(jfil, object_pairs_hook=OrderedDict)
                # Extract the top-level keys (should just be the name of the
                # entry)
                stub_name = list(data.keys())
                # Make sure there is only a single top-level entry
                if len(stub_name) != 1:
                    err = ""json file '{}' has multiple keys: {}"".format(
                        fname, list(stub_name))
                    self._log.error(err)
                    raise ValueError(err)
                stub_name = stub_name[0]

                # Make sure a non-stub entry doesnt already exist with this
                # name
                if stub_name in self.entries and not self.entries[
                        stub_name]._stub:
                    err_str = (
                        ""ERROR: non-stub entry already exists with name '{}'""
                        .format(stub_name))
                    self.log.error(err_str)
                    raise RuntimeError(err_str)

                # Remove the outmost dict level
                data = data[stub_name]
                # Create a new `Entry` (subclass) instance
                proto = self.proto
                stub = proto(catalog=self, name=stub_name, stub=True)
                # Add stub parameters if they are available
                if proto._KEYS.ALIAS in data:
                    stub[proto._KEYS.ALIAS] = data[proto._KEYS.ALIAS]
                if proto._KEYS.DISTINCT_FROM in data:
                    stub[proto._KEYS.DISTINCT_FROM] = data[
                        proto._KEYS.DISTINCT_FROM]
                if proto._KEYS.RA in data:
                    stub[proto._KEYS.RA] = data[proto._KEYS.RA]
                if proto._KEYS.DEC in data:
                    stub[proto._KEYS.DEC] = data[proto._KEYS.DEC]
                if proto._KEYS.DISCOVER_DATE in data:
                    stub[proto._KEYS.DISCOVER_DATE] = data[
                        proto._KEYS.DISCOVER_DATE]
                if proto._KEYS.SOURCES in data:
                    stub[proto._KEYS.SOURCES] = data[
                        proto._KEYS.SOURCES]

            # Store the stub
            self.entries[stub_name] = stub
            self.log.debug(""Added stub for '{}'"".format(stub_name))

        currenttask = 'Loading entry stubs'
        files = self.PATHS.get_repo_output_file_list()
        for ii, _fname in enumerate(pbar(files, currenttask)):
            # Run normally
            # _add_stub(_fname)

            # Run 'manually' (extract stub parameters directly from JSON)
            _add_stub_manually(_fname)

            if log_mem:
                rss = process.memory_info().rss / 1024 / 1024
                if ii % LOG_MEMORY_INT == 0 or rss > MEMORY_LIMIT:
                    log_memory(self.log, ""\nLoaded stub {}"".format(ii),
                               logging.INFO)
                    if rss > MEMORY_LIMIT:
                        err = (
                            ""Memory usage {}, has exceeded {} on file {} '{}'"".
                            format(rss, MEMORY_LIMIT, ii, _fname))
                        self.log.error(err)
                        raise RuntimeError(err)

        return self.entries","Load all events in their `stub` (name, alias, etc only) form.

        Used in `update` mode."
"async def get_my_did_with_meta(wallet_handle: int, did: str) -> str:
    """"""
    Get DID metadata and verkey stored in the wallet.

    :param wallet_handle: wallet handler (created by open_wallet).
    :param did: The DID to retrieve metadata.
    :return: DID with verkey and metadata.
    """"""

    logger = logging.getLogger(__name__)
    logger.debug(""get_my_did_with_meta: >>> wallet_handle: %r, did: %r"",
                 wallet_handle,
                 did)

    if not hasattr(get_my_did_with_meta, ""cb""):
        logger.debug(""get_my_did_with_meta: Creating callback"")
        get_my_did_with_meta.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))

    c_wallet_handle = c_int32(wallet_handle)
    c_did = c_char_p(did.encode('utf-8'))

    did_with_meta = await do_call('indy_get_my_did_with_meta',
                                  c_wallet_handle,
                                  c_did,
                                  get_my_did_with_meta.cb)

    res = did_with_meta.decode()

    logger.debug(""get_my_did_with_meta: <<< res: %r"", res)
    return res","Get DID metadata and verkey stored in the wallet.

    :param wallet_handle: wallet handler (created by open_wallet).
    :param did: The DID to retrieve metadata.
    :return: DID with verkey and metadata."
"def streaming_bulk(client, actions, chunk_size=500, max_chunk_bytes=100 * 1014 * 1024,
        raise_on_error=True, expand_action_callback=expand_action,
        raise_on_exception=True, **kwargs):
    """"""
    Streaming bulk consumes actions from the iterable passed in and yields
    results per action. For non-streaming usecases use
    :func:`~elasticsearch.helpers.bulk` which is a wrapper around streaming
    bulk that returns summary information about the bulk operation once the
    entire input is consumed and sent.
    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use
    :arg actions: iterable containing the actions to be executed
    :arg chunk_size: number of docs in one chunk sent to es (default: 500)
    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)
    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)
        from the execution of the last chunk when some occur. By default we raise.
    :arg raise_on_exception: if ``False`` then don't propagate exceptions from
        call to ``bulk`` and just report the items that failed as failed.
    :arg expand_action_callback: callback executed on each action passed in,
        should return a tuple containing the action line and the data line
        (`None` if data line should be omitted).
    """"""
    actions = map(expand_action_callback, actions)

    for bulk_actions in _chunk_actions(actions, chunk_size, max_chunk_bytes, client.transport.serializer):
        for result in _process_bulk_chunk(client, bulk_actions, raise_on_exception, raise_on_error, **kwargs):
            yield result","Streaming bulk consumes actions from the iterable passed in and yields
    results per action. For non-streaming usecases use
    :func:`~elasticsearch.helpers.bulk` which is a wrapper around streaming
    bulk that returns summary information about the bulk operation once the
    entire input is consumed and sent.
    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use
    :arg actions: iterable containing the actions to be executed
    :arg chunk_size: number of docs in one chunk sent to es (default: 500)
    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)
    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)
        from the execution of the last chunk when some occur. By default we raise.
    :arg raise_on_exception: if ``False`` then don't propagate exceptions from
        call to ``bulk`` and just report the items that failed as failed.
    :arg expand_action_callback: callback executed on each action passed in,
        should return a tuple containing the action line and the data line
        (`None` if data line should be omitted)."
"def build_header(self, title):
        """"""Generate the header for the Markdown file.""""""
        header = ['---',
                  'title: ' + title,
                  'author(s): ' + self.user,
                  'tags: ',
                  'created_at: ' + str(self.date_created),
                  'updated_at: ' + str(self.date_updated),
                  'tldr: ',
                  'thumbnail: ',
                  '---']

        self.out = header + self.out",Generate the header for the Markdown file.
"def classification_tikhonov_simplex(G, y, M, tau=0.1, **kwargs):
    r""""""Solve a classification problem on graph via Tikhonov minimization
    with simple constraints.

    The function first transforms :math:`y` in logits :math:`Y`, then solves

    .. math:: \operatorname*{arg min}_X \| M X - Y \|_2^2 + \tau \ tr(X^T L X)
              \text{ s.t. } sum(X) = 1 \text{ and } X >= 0,

    where :math:`X` and :math:`Y` are logits.

    Parameters
    ----------
    G : :class:`pygsp.graphs.Graph`
    y : array, length G.n_vertices
        Measurements.
    M : array of boolean, length G.n_vertices
        Masking vector.
    tau : float
        Regularization parameter.
    kwargs : dict
        Parameters for :func:`pyunlocbox.solvers.solve`.

    Returns
    -------
    logits : array, length G.n_vertices
        The logits :math:`X`.

    Examples
    --------
    >>> from pygsp import graphs, learning
    >>> import matplotlib.pyplot as plt
    >>>
    >>> G = graphs.Logo()
    >>> G.estimate_lmax()

    Create a ground truth signal:

    >>> signal = np.zeros(G.n_vertices)
    >>> signal[G.info['idx_s']] = 1
    >>> signal[G.info['idx_p']] = 2

    Construct a measurement signal from a binary mask:

    >>> rs = np.random.RandomState(42)
    >>> mask = rs.uniform(0, 1, G.n_vertices) > 0.5
    >>> measures = signal.copy()
    >>> measures[~mask] = np.nan

    Solve the classification problem by reconstructing the signal:

    >>> recovery = learning.classification_tikhonov_simplex(
    ...     G, measures, mask, tau=0.1, verbosity='NONE')

    Plot the results.
    Note that we recover the class with ``np.argmax(recovery, axis=1)``.

    >>> prediction = np.argmax(recovery, axis=1)
    >>> fig, ax = plt.subplots(2, 3, sharey=True, figsize=(10, 6))
    >>> _ = G.plot_signal(signal, ax=ax[0, 0], title='Ground truth')
    >>> _ = G.plot_signal(measures, ax=ax[0, 1], title='Measurements')
    >>> _ = G.plot_signal(prediction, ax=ax[0, 2], title='Recovered class')
    >>> _ = G.plot_signal(recovery[:, 0], ax=ax[1, 0], title='Logit 0')
    >>> _ = G.plot_signal(recovery[:, 1], ax=ax[1, 1], title='Logit 1')
    >>> _ = G.plot_signal(recovery[:, 2], ax=ax[1, 2], title='Logit 2')
    >>> _ = fig.tight_layout()

    """"""

    functions, solvers = _import_pyunlocbox()

    if tau <= 0:
        raise ValueError('Tau should be greater than 0.')

    y[M == False] = 0
    Y = _to_logits(y.astype(np.int))
    Y[M == False, :] = 0

    def proj_simplex(y):
        d = y.shape[1]
        a = np.ones(d)
        idx = np.argsort(y)

        def evalpL(y, k, idx):
            return np.sum(y[idx[k:]] - y[idx[k]]) - 1

        def bisectsearch(idx, y):
            idxL, idxH = 0, d-1
            L = evalpL(y, idxL, idx)
            H = evalpL(y, idxH, idx)

            if L < 0:
                return idxL

            while (idxH-idxL) > 1:
                iMid = int((idxL + idxH) / 2)
                M = evalpL(y, iMid, idx)

                if M > 0:
                    idxL, L = iMid, M
                else:
                    idxH, H = iMid, M

            return idxH

        def proj(idx, y):
            k = bisectsearch(idx, y)
            lam = (np.sum(y[idx[k:]]) - 1) / (d - k)
            return np.maximum(0, y - lam)

        x = np.empty_like(y)
        for i in range(len(y)):
            x[i] = proj(idx[i], y[i])
        # x = np.stack(map(proj, idx, y))

        return x

    def smooth_eval(x):
        xTLx = np.sum(x * (G.L.dot(x)))
        e = M * ((M * x.T) - Y.T)
        l2 = np.sum(e * e)
        return tau * xTLx + l2

    def smooth_grad(x):
        return 2 * ((M * (M * x.T - Y.T)).T + tau * G.L * x)

    f1 = functions.func()
    f1._eval = smooth_eval
    f1._grad = smooth_grad

    f2 = functions.func()
    f2._eval = lambda x: 0  # Indicator functions evaluate to zero.
    f2._prox = lambda x, step: proj_simplex(x)

    step = 0.5 / (1 + tau * G.lmax)
    solver = solvers.forward_backward(step=step)
    ret = solvers.solve([f1, f2], Y.copy(), solver, **kwargs)
    return ret['sol']","r""""""Solve a classification problem on graph via Tikhonov minimization
    with simple constraints.

    The function first transforms :math:`y` in logits :math:`Y`, then solves

    .. math:: \operatorname*{arg min}_X \| M X - Y \|_2^2 + \tau \ tr(X^T L X)
              \text{ s.t. } sum(X) = 1 \text{ and } X >= 0,

    where :math:`X` and :math:`Y` are logits.

    Parameters
    ----------
    G : :class:`pygsp.graphs.Graph`
    y : array, length G.n_vertices
        Measurements.
    M : array of boolean, length G.n_vertices
        Masking vector.
    tau : float
        Regularization parameter.
    kwargs : dict
        Parameters for :func:`pyunlocbox.solvers.solve`.

    Returns
    -------
    logits : array, length G.n_vertices
        The logits :math:`X`.

    Examples
    --------
    >>> from pygsp import graphs, learning
    >>> import matplotlib.pyplot as plt
    >>>
    >>> G = graphs.Logo()
    >>> G.estimate_lmax()

    Create a ground truth signal:

    >>> signal = np.zeros(G.n_vertices)
    >>> signal[G.info['idx_s']] = 1
    >>> signal[G.info['idx_p']] = 2

    Construct a measurement signal from a binary mask:

    >>> rs = np.random.RandomState(42)
    >>> mask = rs.uniform(0, 1, G.n_vertices) > 0.5
    >>> measures = signal.copy()
    >>> measures[~mask] = np.nan

    Solve the classification problem by reconstructing the signal:

    >>> recovery = learning.classification_tikhonov_simplex(
    ...     G, measures, mask, tau=0.1, verbosity='NONE')

    Plot the results.
    Note that we recover the class with ``np.argmax(recovery, axis=1)``.

    >>> prediction = np.argmax(recovery, axis=1)
    >>> fig, ax = plt.subplots(2, 3, sharey=True, figsize=(10, 6))
    >>> _ = G.plot_signal(signal, ax=ax[0, 0], title='Ground truth')
    >>> _ = G.plot_signal(measures, ax=ax[0, 1], title='Measurements')
    >>> _ = G.plot_signal(prediction, ax=ax[0, 2], title='Recovered class')
    >>> _ = G.plot_signal(recovery[:, 0], ax=ax[1, 0], title='Logit 0')
    >>> _ = G.plot_signal(recovery[:, 1], ax=ax[1, 1], title='Logit 1')
    >>> _ = G.plot_signal(recovery[:, 2], ax=ax[1, 2], title='Logit 2')
    >>> _ = fig.tight_layout()"
"def iload(cls, entry_point='numina.pipeline.1'):
        """"""Load all available DRPs in 'entry_point'.""""""

        for entry in pkg_resources.iter_entry_points(group=entry_point):
            try:
                drp_loader = entry.load()
                drpins = drp_loader()
                if cls.instrumentdrp_check(drpins, entry.name):
                    yield drpins
            except Exception as error:
                print('Problem loading', entry, file=sys.stderr)
                print(""Error is: "", error, file=sys.stderr)",Load all available DRPs in 'entry_point'.
"def whichchain(atom):
    """"""Returns the residue number of an PyBel or OpenBabel atom.""""""
    atom = atom if not isinstance(atom, Atom) else atom.OBAtom  # Convert to OpenBabel Atom
    return atom.GetResidue().GetChain() if atom.GetResidue() is not None else None",Returns the residue number of an PyBel or OpenBabel atom.
"def get_task(task_id, completed=True):
    """""" Get a task by task id where a task_id is required.

        :param task_id: task ID
        :type task_id: str
        :param completed: include completed tasks?
        :type completed: bool

        :return: a task
        :rtype: obj
    """"""
    tasks = get_tasks(task_id=task_id, completed=completed)

    if len(tasks) == 0:
        return None

    assert len(tasks) == 1, 'get_task should return at max 1 task for a task id'
    return tasks[0]","Get a task by task id where a task_id is required.

        :param task_id: task ID
        :type task_id: str
        :param completed: include completed tasks?
        :type completed: bool

        :return: a task
        :rtype: obj"
"def getPortType(self):
        """"""Return the PortType object that is referenced by this port.""""""
        wsdl = self.getService().getWSDL()
        binding = wsdl.bindings[self.binding]
        return wsdl.portTypes[binding.type]",Return the PortType object that is referenced by this port.
"def angle_between_vectors(x, y):
    """""" Compute the angle between vector x and y """"""
    dp = dot_product(x, y)
    if dp == 0:
        return 0
    xm = magnitude(x)
    ym = magnitude(y)
    return math.acos(dp / (xm*ym)) * (180. / math.pi)",Compute the angle between vector x and y
"def get_volume_connector(self, assigner_id):
        """"""Get connector information of the instance for attaching to volumes.

        Connector information is a dictionary representing the ip of the
        machine that will be making the connection, the name of the iscsi
        initiator and the hostname of the machine as follows::

            {
                'zvm_fcp': fcp
                'wwpns': [wwpn]
                'host': host
            }
        """"""

        empty_connector = {'zvm_fcp': [], 'wwpns': [], 'host': ''}

        # init fcp pool
        self.fcp_mgr.init_fcp(assigner_id)
        fcp_list = self.fcp_mgr.get_available_fcp()
        if not fcp_list:
            errmsg = ""No available FCP device found.""
            LOG.warning(errmsg)
            return empty_connector
        wwpns = []
        for fcp_no in fcp_list:
            wwpn = self.fcp_mgr.get_wwpn(fcp_no)
            if not wwpn:
                errmsg = ""FCP device %s has no available WWPN."" % fcp_no
                LOG.warning(errmsg)
            else:
                wwpns.append(wwpn)

        if not wwpns:
            errmsg = ""No available WWPN found.""
            LOG.warning(errmsg)
            return empty_connector

        inv_info = self._smtclient.get_host_info()
        zvm_host = inv_info['zvm_host']
        if zvm_host == '':
            errmsg = ""zvm host not specified.""
            LOG.warning(errmsg)
            return empty_connector

        connector = {'zvm_fcp': fcp_list,
                     'wwpns': wwpns,
                     'host': zvm_host}
        LOG.debug('get_volume_connector returns %s for %s' %
                  (connector, assigner_id))
        return connector","Get connector information of the instance for attaching to volumes.

        Connector information is a dictionary representing the ip of the
        machine that will be making the connection, the name of the iscsi
        initiator and the hostname of the machine as follows::

            {
                'zvm_fcp': fcp
                'wwpns': [wwpn]
                'host': host
            }"
"def define_header_values(self, http_method, route, values, update=False):
        """"""Define header values for a given request.

        By default, header values are determined from the class attribute
        `headers`. But if you want to change the headers used in the
        documentation for a specific route, this method lets you do that.

        :param str http_method: An HTTP method, like ""get"".
        :param str route: The route to match.
        :param dict values: A dictionary of headers for the example request.
        :param bool update: If True, the values will be merged into the default
            headers for the request. If False, the values will replace
            the default headers.
        """"""
        self.defined_header_values[(http_method.lower(), route)] = {
            'update': update,
            'values': values
        }","Define header values for a given request.

        By default, header values are determined from the class attribute
        `headers`. But if you want to change the headers used in the
        documentation for a specific route, this method lets you do that.

        :param str http_method: An HTTP method, like ""get"".
        :param str route: The route to match.
        :param dict values: A dictionary of headers for the example request.
        :param bool update: If True, the values will be merged into the default
            headers for the request. If False, the values will replace
            the default headers."
"def _trigger_event(self, event, *args, **kwargs):
        """"""Invoke an event handler.""""""
        run_async = kwargs.pop('run_async', False)
        if event in self.handlers:
            if run_async:
                return self.start_background_task(self.handlers[event], *args)
            else:
                try:
                    return self.handlers[event](*args)
                except:
                    self.logger.exception(event + ' handler error')",Invoke an event handler.
"def latex(source: str):
    """"""
    Add a mathematical equation in latex math-mode syntax to the display.
    Instead of the traditional backslash escape character, the @ character is
    used instead to prevent backslash conflicts with Python strings. For
    example, \\delta would be @delta.

    :param source:
        The string representing the latex equation to be rendered.
    """"""
    r = _get_report()
    if 'katex' not in r.library_includes:
        r.library_includes.append('katex')

    r.append_body(render_texts.latex(source.replace('@', '\\')))
    r.stdout_interceptor.write_source('[ADDED] Latex equation\n')","Add a mathematical equation in latex math-mode syntax to the display.
    Instead of the traditional backslash escape character, the @ character is
    used instead to prevent backslash conflicts with Python strings. For
    example, \\delta would be @delta.

    :param source:
        The string representing the latex equation to be rendered."
"def GetNetworkAddressWithTime(self):
        """"""
        Get a network address object.

        Returns:
            NetworkAddressWithTime: if we have a connection to a node.
            None: otherwise.
        """"""
        if self.port is not None and self.host is not None and self.Version is not None:
            return NetworkAddressWithTime(self.host, self.port, self.Version.Services)
        return None","Get a network address object.

        Returns:
            NetworkAddressWithTime: if we have a connection to a node.
            None: otherwise."
"def active_vectors_info(self):
        """"""Return the active scalar's field and name: [field, name]""""""
        if not hasattr(self, '_active_vectors_info'):
            self._active_vectors_info = [POINT_DATA_FIELD, None] # field and name
        _, name = self._active_vectors_info

        # rare error where scalar name isn't a valid scalar
        if name not in self.point_arrays:
            if name not in self.cell_arrays:
                name = None

        return self._active_vectors_info","Return the active scalar's field and name: [field, name]"
"def _genBgTerm_fromSNPs(self,vTot=0.5,vCommon=0.1,pCausal=0.5,plot=False):
        """""" generate  """"""

        if self.X is None:
            print('Reading in all SNPs. This is slow.')
            rv = plink_reader.readBED(self.bfile,useMAFencoding=True)
            X  = rv['snps']
        else:
            X  = self.X

        S  = X.shape[1]
        vSpecific = vTot-vCommon

        # select causal SNPs
        nCausal = int(SP.floor(pCausal*S))
        Ic = selectRnd(nCausal,S)
        X = X[:,Ic]

        # common effect
        Bc  = SP.dot(self.genWeights(nCausal,self.P),self.genTraitEffect())
        Yc  = SP.dot(X,Bc)
        Yc *= SP.sqrt(vCommon/Yc.var(0).mean())

        # indipendent effect
        Bi  = SP.randn(nCausal,self.P)
        Yi  = SP.dot(X,Bi)
        Yi *= SP.sqrt(vSpecific/Yi.var(0).mean())

        if plot:
            import pylab as PL
            PL.ion()
            for p in range(self.P):
                PL.subplot(self.P,1,p+1)
                PL.plot(SP.arange(self.X.shape[1])[Ic],Bc[:,p],'o',color='y',alpha=0.05)
                PL.plot(SP.arange(self.X.shape[1])[Ic],Bi[:,p],'o',color='r',alpha=0.05)
                #PL.ylim(-2,2)
                PL.plot([0,Ic.shape[0]],[0,0],'k')

        return Yc, Yi",generate
"def bin(self, *columns, **vargs):
        """"""Group values by bin and compute counts per bin by column.

        By default, bins are chosen to contain all values in all columns. The
        following named arguments from numpy.histogram can be applied to
        specialize bin widths:

        If the original table has n columns, the resulting binned table has
        n+1 columns, where column 0 contains the lower bound of each bin.

        Args:
            ``columns`` (str or int): Labels or indices of columns to be
                binned. If empty, all columns are binned.

            ``bins`` (int or sequence of scalars): If bins is an int,
                it defines the number of equal-width bins in the given range
                (10, by default). If bins is a sequence, it defines the bin
                edges, including the rightmost edge, allowing for non-uniform
                bin widths.

            ``range`` ((float, float)): The lower and upper range of
                the bins. If not provided, range contains all values in the
                table. Values outside the range are ignored.

            ``density`` (bool): If False, the result will contain the number of
                samples in each bin. If True, the result is the value of the
                probability density function at the bin, normalized such that
                the integral over the range is 1. Note that the sum of the
                histogram values will not be equal to 1 unless bins of unity
                width are chosen; it is not a probability mass function.
        """"""
        if columns:
            self = self.select(*columns)
        if 'normed' in vargs:
            vargs.setdefault('density', vargs.pop('normed'))
        density = vargs.get('density', False)
        tag = 'density' if density else 'count'

        cols = list(self._columns.values())
        _, bins = np.histogram(cols, **vargs)

        binned = type(self)().with_column('bin', bins)
        for label in self.labels:
            counts, _ = np.histogram(self[label], bins=bins, density=density)
            binned[label + ' ' + tag] = np.append(counts, 0)
        return binned","Group values by bin and compute counts per bin by column.

        By default, bins are chosen to contain all values in all columns. The
        following named arguments from numpy.histogram can be applied to
        specialize bin widths:

        If the original table has n columns, the resulting binned table has
        n+1 columns, where column 0 contains the lower bound of each bin.

        Args:
            ``columns`` (str or int): Labels or indices of columns to be
                binned. If empty, all columns are binned.

            ``bins`` (int or sequence of scalars): If bins is an int,
                it defines the number of equal-width bins in the given range
                (10, by default). If bins is a sequence, it defines the bin
                edges, including the rightmost edge, allowing for non-uniform
                bin widths.

            ``range`` ((float, float)): The lower and upper range of
                the bins. If not provided, range contains all values in the
                table. Values outside the range are ignored.

            ``density`` (bool): If False, the result will contain the number of
                samples in each bin. If True, the result is the value of the
                probability density function at the bin, normalized such that
                the integral over the range is 1. Note that the sum of the
                histogram values will not be equal to 1 unless bins of unity
                width are chosen; it is not a probability mass function."
"def load(self, *args, **kwargs):
        """"""
        Load instrument data into instrument object.data

        (Wraps pysat.Instrument.load; documentation of that function is
        reproduced here.)

        Parameters
        ---------
        yr : integer
            Year for desired data
        doy : integer
            day of year
        data : datetime object
            date to load
        fname : 'string'
            filename to be loaded
        verifyPad : boolean
            if true, padding data not removed (debug purposes)
        """"""

        for instrument in self.instruments:
            instrument.load(*args, **kwargs)","Load instrument data into instrument object.data

        (Wraps pysat.Instrument.load; documentation of that function is
        reproduced here.)

        Parameters
        ---------
        yr : integer
            Year for desired data
        doy : integer
            day of year
        data : datetime object
            date to load
        fname : 'string'
            filename to be loaded
        verifyPad : boolean
            if true, padding data not removed (debug purposes)"
"def _iterdump(self, file_name, headers=None):
        """"""
        Function for dumping values from a file.

        Should only be used by developers.

        Args:
            file_name: name of the file
            headers: list of headers to pick
                default:
                [""Discharge_Capacity"", ""Charge_Capacity""]

        Returns: pandas.DataFrame

        """"""
        if headers is None:
            headers = [""Discharge_Capacity"", ""Charge_Capacity""]

        step_txt = self.headers_normal['step_index_txt']
        point_txt = self.headers_normal['data_point_txt']
        cycle_txt = self.headers_normal['cycle_index_txt']

        self.logger.debug(""iterating through file: %s"" % file_name)
        if not os.path.isfile(file_name):
            print(""Missing file_\n   %s"" % file_name)

        filesize = os.path.getsize(file_name)
        hfilesize = humanize_bytes(filesize)
        txt = ""Filesize: %i (%s)"" % (filesize, hfilesize)
        self.logger.info(txt)

        table_name_global = TABLE_NAMES[""global""]
        table_name_stats = TABLE_NAMES[""statistic""]
        table_name_normal = TABLE_NAMES[""normal""]

        # creating temporary file and connection

        temp_dir = tempfile.gettempdir()
        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
        shutil.copy2(file_name, temp_dir)
        constr = self.__get_res_connector(temp_filename)

        if use_ado:
            conn = dbloader.connect(constr)
        else:
            conn = dbloader.connect(constr, autocommit=True)

        self.logger.debug(""tmp file: %s"" % temp_filename)
        self.logger.debug(""constr str: %s"" % constr)

        # --------- read global-data ------------------------------------
        self.logger.debug(""reading global data table"")
        sql = ""select * from %s"" % table_name_global
        global_data_df = pd.read_sql_query(sql, conn)
        # col_names = list(global_data_df.columns.values)
        self.logger.debug(""sql statement: %s"" % sql)

        tests = global_data_df[self.headers_normal['test_id_txt']]
        number_of_sets = len(tests)
        self.logger.debug(""number of datasets: %i"" % number_of_sets)
        self.logger.debug(""only selecting first test"")
        test_no = 0
        self.logger.debug(""setting data for test number %i"" % test_no)
        loaded_from = file_name
        # fid = FileID(file_name)
        start_datetime = global_data_df[self.headers_global['start_datetime_txt']][test_no]
        test_ID = int(global_data_df[self.headers_normal['test_id_txt']][test_no])  # OBS
        test_name = global_data_df[self.headers_global['test_name_txt']][test_no]

        # --------- read raw-data (normal-data) -------------------------
        self.logger.debug(""reading raw-data"")

        columns = [""Data_Point"", ""Step_Index"", ""Cycle_Index""]
        columns.extend(headers)
        columns_txt = "", "".join([""%s""] * len(columns)) % tuple(columns)

        sql_1 = ""select %s "" % columns_txt
        sql_2 = ""from %s "" % table_name_normal
        sql_3 = ""where %s=%s "" % (self.headers_normal['test_id_txt'], test_ID)
        sql_5 = ""order by %s"" % self.headers_normal['data_point_txt']
        import time
        info_list = []
        info_header = [""cycle"", ""row_count"", ""start_point"", ""end_point""]
        info_header.extend(headers)
        self.logger.info("" "".join(info_header))
        self.logger.info(""-------------------------------------------------"")

        for cycle_number in range(1, 2000):
            t1 = time.time()
            self.logger.debug(""picking cycle %i"" % cycle_number)
            sql_4 = ""AND %s=%i "" % (cycle_txt, cycle_number)
            sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
            self.logger.debug(""sql statement: %s"" % sql)
            normal_df = pd.read_sql_query(sql, conn)
            t2 = time.time()
            dt = t2 - t1
            self.logger.debug(""time: %f"" % dt)
            if normal_df.empty:
                self.logger.debug(""reached the end"")
                break
            row_count, _ = normal_df.shape
            start_point = normal_df[point_txt].min()
            end_point = normal_df[point_txt].max()
            last = normal_df.iloc[-1, :]

            step_list = [cycle_number, row_count, start_point, end_point]
            step_list.extend([last[x] for x in headers])
            info_list.append(step_list)

        self._clean_up_loadres(None, conn, temp_filename)
        info_dict = pd.DataFrame(info_list, columns=info_header)
        return info_dict","Function for dumping values from a file.

        Should only be used by developers.

        Args:
            file_name: name of the file
            headers: list of headers to pick
                default:
                [""Discharge_Capacity"", ""Charge_Capacity""]

        Returns: pandas.DataFrame"
"def get_env(env_file='.env'):
    """"""
    Set default environment variables from .env file
    """"""
    try:
        with open(env_file) as f:
            for line in f.readlines():
                try:
                    key, val = line.split('=', maxsplit=1)
                    os.environ.setdefault(key.strip(), val.strip())
                except ValueError:
                    pass
    except FileNotFoundError:
        pass",Set default environment variables from .env file
"def UpdateHuntOutputPluginState(self,
                                  hunt_id,
                                  state_index,
                                  update_fn,
                                  cursor=None):
    """"""Updates hunt output plugin state for a given output plugin.""""""

    hunt_id_int = db_utils.HuntIDToInt(hunt_id)

    query = ""SELECT hunt_id FROM hunts WHERE hunt_id = %s""
    rows_returned = cursor.execute(query, [hunt_id_int])
    if rows_returned == 0:
      raise db.UnknownHuntError(hunt_id)

    columns = "", "".join(_HUNT_OUTPUT_PLUGINS_STATES_COLUMNS)
    query = (""SELECT {columns} FROM hunt_output_plugins_states ""
             ""WHERE hunt_id = %s AND plugin_id = %s"".format(columns=columns))
    rows_returned = cursor.execute(query, [hunt_id_int, state_index])
    if rows_returned == 0:
      raise db.UnknownHuntOutputPluginStateError(hunt_id, state_index)

    state = self._HuntOutputPluginStateFromRow(cursor.fetchone())
    modified_plugin_state = update_fn(state.plugin_state)

    query = (""UPDATE hunt_output_plugins_states ""
             ""SET plugin_state = %s ""
             ""WHERE hunt_id = %s AND plugin_id = %s"")
    args = [modified_plugin_state.SerializeToString(), hunt_id_int, state_index]
    cursor.execute(query, args)
    return state",Updates hunt output plugin state for a given output plugin.
"def add_field(self, name, ftype, docfield=None):
        """""" Add a field to the document (and to the underlying schema)
        
        :param name: name of the new field
        :type name: str
        :param ftype: type of the new field
        :type ftype: subclass of :class:`.GenericType`
        """"""
        self.schema.add_field(name, ftype)
        self[name] = docfield or DocField.FromType(ftype)","Add a field to the document (and to the underlying schema)
        
        :param name: name of the new field
        :type name: str
        :param ftype: type of the new field
        :type ftype: subclass of :class:`.GenericType`"
"def dump(self, C_out, scale_out=None, stream=None, fmt='lha', skip_redundant=True):
        """"""Return a string representation of the parameters and Wilson
        coefficients `C_out` in DSixTools output format. If `stream` is
        specified, export it to a file. `fmt` defaults to `lha` (the SLHA-like
        DSixTools format), but can also be `json` or `yaml` (see the
        pylha documentation).""""""
        C = OrderedDict()
        if scale_out is not None:
            C['SCALES'] = {'values': [[1, self.scale_high], [2, scale_out]]}
        else:
            C['SCALES'] = {'values': [[1, self.scale_high]]}
        sm = io.sm_dict2lha(C_out)['BLOCK']
        C.update(sm)
        wc = io.wc_dict2lha(C_out, skip_redundant=skip_redundant)['BLOCK']
        C.update(wc)
        return pylha.dump({'BLOCK': C}, fmt=fmt, stream=stream)","Return a string representation of the parameters and Wilson
        coefficients `C_out` in DSixTools output format. If `stream` is
        specified, export it to a file. `fmt` defaults to `lha` (the SLHA-like
        DSixTools format), but can also be `json` or `yaml` (see the
        pylha documentation)."
"def calculte_tensor_to_label_output_shapes(operator):
    '''
    Allowed input/output patterns are
        1. [N, C] ---> [N, 1]

    Note that N must be 1 currently because TensorToProbability doesn't support batch size larger than 1.
    '''
    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)
    check_input_and_output_types(operator, good_input_types=[FloatTensorType])

    N = operator.inputs[0].type.shape[0]
    if operator.target_opset < 7:
        output_shape = [1, 1]
    else:
        output_shape = [N, 1]

    if type(operator.outputs[0].type) in [Int64Type, Int64TensorType]:
        operator.outputs[0].type = Int64TensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)
    elif type(operator.outputs[0].type) in [StringType, StringTensorType]:
        operator.outputs[0].type = StringTensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)
    else:
        raise ValueError('Unsupported label type')","Allowed input/output patterns are
        1. [N, C] ---> [N, 1]

    Note that N must be 1 currently because TensorToProbability doesn't support batch size larger than 1."
"def auth_data(self):
        '''
        Gather and create the authorization data sets

        We're looking at several constructs here.

        Standard eauth: allow jsmith to auth via pam, and execute any command
        on server web1
        external_auth:
          pam:
            jsmith:
              - web1:
                - .*

        Django eauth: Import the django library, dynamically load the Django
        model called 'model'.  That model returns a data structure that
        matches the above for standard eauth.  This is what determines
        who can do what to which machines

        django:
          ^model:
            <stuff returned from django>

        Active Directory Extended:

        Users in the AD group 'webadmins' can run any command on server1
        Users in the AD group 'webadmins' can run test.ping and service.restart
        on machines that have a computer object in the AD 'webservers' OU
        Users in the AD group 'webadmins' can run commands defined in the
        custom attribute (custom attribute not implemented yet, this is for
        future use)
          ldap:
             webadmins%:  <all users in the AD 'webadmins' group>
               - server1:
                   - .*
               - ldap(OU=webservers,dc=int,dc=bigcompany,dc=com):
                  - test.ping
                  - service.restart
               - ldap(OU=Domain Controllers,dc=int,dc=bigcompany,dc=com):
                 - allowed_fn_list_attribute^
        '''
        auth_data = self.opts['external_auth']
        merge_lists = self.opts['pillar_merge_lists']

        if 'django' in auth_data and '^model' in auth_data['django']:
            auth_from_django = salt.auth.django.retrieve_auth_entries()
            auth_data = salt.utils.dictupdate.merge(auth_data,
                                                    auth_from_django,
                                                    strategy='list',
                                                    merge_lists=merge_lists)

        if 'ldap' in auth_data and __opts__.get('auth.ldap.activedirectory', False):
            auth_data['ldap'] = salt.auth.ldap.__expand_ldap_entries(auth_data['ldap'])
            log.debug(auth_data['ldap'])

        #for auth_back in self.opts.get('external_auth_sources', []):
        #    fstr = '{0}.perms'.format(auth_back)
        #    if fstr in self.loadauth.auth:
        #        auth_data.append(getattr(self.loadauth.auth)())
        return auth_data","Gather and create the authorization data sets

        We're looking at several constructs here.

        Standard eauth: allow jsmith to auth via pam, and execute any command
        on server web1
        external_auth:
          pam:
            jsmith:
              - web1:
                - .*

        Django eauth: Import the django library, dynamically load the Django
        model called 'model'.  That model returns a data structure that
        matches the above for standard eauth.  This is what determines
        who can do what to which machines

        django:
          ^model:
            <stuff returned from django>

        Active Directory Extended:

        Users in the AD group 'webadmins' can run any command on server1
        Users in the AD group 'webadmins' can run test.ping and service.restart
        on machines that have a computer object in the AD 'webservers' OU
        Users in the AD group 'webadmins' can run commands defined in the
        custom attribute (custom attribute not implemented yet, this is for
        future use)
          ldap:
             webadmins%:  <all users in the AD 'webadmins' group>
               - server1:
                   - .*
               - ldap(OU=webservers,dc=int,dc=bigcompany,dc=com):
                  - test.ping
                  - service.restart
               - ldap(OU=Domain Controllers,dc=int,dc=bigcompany,dc=com):
                 - allowed_fn_list_attribute^"
"def _sanitize_entity(self, entity):
        """"""
        Make given entity 'sane' for further use.
        """"""
        aliases = {
            ""current_state"": ""state"",
            ""is_flapping"": ""flapping"",
            ""scheduled_downtime_depth"": ""in_downtime"",
            ""has_been_checked"": ""checked"",
            ""should_be_scheduled"": ""scheduled"",
            ""active_checks_enabled"": ""active_checks"",
            ""passive_checks_enabled"": ""passive_checks"",
        }
        sane = {}
        for akey in aliases.keys():
            sane[aliases[akey]] = None

        aliases_keys = aliases.keys()
        for key in entity.keys():
            if key not in aliases_keys:
                continue

            alias = aliases[key]
            try:
                sane[alias] = int(entity[key])
            except Exception:
                sane[alias] = None

        if sane[""active_checks""] not in [0, 1]:
            sane[""active_checks""] = 0
        elif sane[""active_checks""] == 1:
            sane[""passive_checks""] = 0

        if sane[""passive_checks""] not in [0, 1]:
            sane[""passive_checks""] = 0

        return sane",Make given entity 'sane' for further use.
"def _parse_status_code(response):
    """"""
    Return error string code if the response is an error, otherwise ``""OK""``
    """"""

    # This happens when a status response is expected
    if isinstance(response, string_types):
        return response

    # This happens when a list of structs are expected
    is_single_list = isinstance(response, list) and len(response) == 1
    if is_single_list and isinstance(response[0], string_types):
        return response[0]

    # This happens when a struct of any kind is returned
    return ""OK""","Return error string code if the response is an error, otherwise ``""OK""``"
"def add_message(request, level, message, extra_tags='', fail_silently=False, *args, **kwargs):
    """"""
    Attempts to add a message to the request using the 'messages' app.
    """"""
    if hasattr(request, '_messages'):
        return request._messages.add(level, message, extra_tags, *args, **kwargs)
    if not fail_silently:
        raise MessageFailure('You cannot add messages without installing '
                             'django.contrib.messages.middleware.MessageMiddleware')",Attempts to add a message to the request using the 'messages' app.
"def read(self, size=None):
    """"""Reads a byte string from the file-like object at the current offset.

    The function will read a byte string of the specified size or
    all of the remaining data if no size was specified.

    Args:
      size (Optional[int]): number of bytes to read, where None is all
          remaining data.

    Returns:
      bytes: data read.

    Raises:
      IOError: if the read failed.
      OSError: if the read failed.
    """"""
    if not self._is_open:
      raise IOError('Not opened.')

    if self._current_offset < 0:
      raise IOError('Invalid current offset value less than zero.')

    if self._current_offset > self._size:
      return b''

    if size is None or self._current_offset + size > self._size:
      size = self._size - self._current_offset

    self._tar_ext_file.seek(self._current_offset, os.SEEK_SET)

    data = self._tar_ext_file.read(size)

    # It is possible the that returned data size is not the same as the
    # requested data size. At this layer we don't care and this discrepancy
    # should be dealt with on a higher layer if necessary.
    self._current_offset += len(data)

    return data","Reads a byte string from the file-like object at the current offset.

    The function will read a byte string of the specified size or
    all of the remaining data if no size was specified.

    Args:
      size (Optional[int]): number of bytes to read, where None is all
          remaining data.

    Returns:
      bytes: data read.

    Raises:
      IOError: if the read failed.
      OSError: if the read failed."
"def _sysv_enabled(name, root):
    '''
    A System-V style service is assumed disabled if the ""startup"" symlink
    (starts with ""S"") to its script is found in /etc/init.d in the current
    runlevel.
    '''
    # Find exact match (disambiguate matches like ""S01anacron"" for cron)
    rc = _root('/etc/rc{}.d/S*{}'.format(_runlevel(), name), root)
    for match in glob.glob(rc):
        if re.match(r'S\d{,2}%s' % name, os.path.basename(match)):
            return True
    return False","A System-V style service is assumed disabled if the ""startup"" symlink
    (starts with ""S"") to its script is found in /etc/init.d in the current
    runlevel."
"def _get_reg_software(include_components=True,
                      include_updates=True):
    '''
    This searches the uninstall keys in the registry to find a match in the sub
    keys, it will return a dict with the display name as the key and the
    version as the value

    Args:

        include_components (bool):
            Include sub components of installed software. Default is ``True``

        include_updates (bool):
            Include software updates and Windows updates. Default is ``True``

    Returns:
        dict: A dictionary of installed software with versions installed

    .. code-block:: cfg

        {'<package_name>': '<version>'}
    '''
    # Logic for this can be found in this question:
    # https://social.technet.microsoft.com/Forums/windows/en-US/d913471a-d7fb-448d-869b-da9025dcc943/where-does-addremove-programs-get-its-information-from-in-the-registry
    # and also in the collectPlatformDependentApplicationData function in
    # https://github.com/aws/amazon-ssm-agent/blob/master/agent/plugins/inventory/gatherers/application/dataProvider_windows.go
    reg_software = {}

    def skip_component(hive, key, sub_key, use_32bit):
        '''
        'SystemComponent' must be either absent or present with a value of 0,
        because this value is usually set on programs that have been installed
        via a Windows Installer Package (MSI).

        Returns:
            bool: True if the package needs to be skipped, otherwise False
        '''
        if include_components:
            return False
        if __utils__['reg.value_exists'](
                hive=hive,
                key='{0}\\{1}'.format(key, sub_key),
                vname='SystemComponent',
                use_32bit_registry=use_32bit):
            if __utils__['reg.read_value'](
                    hive=hive,
                    key='{0}\\{1}'.format(key, sub_key),
                    vname='SystemComponent',
                    use_32bit_registry=use_32bit)['vdata'] > 0:
                return True
        return False

    def skip_win_installer(hive, key, sub_key, use_32bit):
        '''
        'WindowsInstaller' must be either absent or present with a value of 0.
        If the value is set to 1, then the application is included in the list
        if and only if the corresponding compressed guid is also present in
        HKLM:\\Software\\Classes\\Installer\\Products

        Returns:
            bool: True if the package needs to be skipped, otherwise False
        '''
        products_key = 'Software\\Classes\\Installer\\Products\\{0}'
        if __utils__['reg.value_exists'](
                hive=hive,
                key='{0}\\{1}'.format(key, sub_key),
                vname='WindowsInstaller',
                use_32bit_registry=use_32bit):
            if __utils__['reg.read_value'](
                    hive=hive,
                    key='{0}\\{1}'.format(key, sub_key),
                    vname='WindowsInstaller',
                    use_32bit_registry=use_32bit)['vdata'] > 0:
                squid = salt.utils.win_functions.guid_to_squid(sub_key)
                if not __utils__['reg.key_exists'](
                        hive='HKLM',
                        key=products_key.format(squid),
                        use_32bit_registry=use_32bit):
                    return True
        return False

    def skip_uninstall_string(hive, key, sub_key, use_32bit):
        '''
        'UninstallString' must be present, because it stores the command line
        that gets executed by Add/Remove programs, when the user tries to
        uninstall a program.

        Returns:
            bool: True if the package needs to be skipped, otherwise False
        '''
        if not __utils__['reg.value_exists'](
                hive=hive,
                key='{0}\\{1}'.format(key, sub_key),
                vname='UninstallString',
                use_32bit_registry=use_32bit):
            return True
        return False

    def skip_release_type(hive, key, sub_key, use_32bit):
        '''
        'ReleaseType' must either be absent or if present must not have a
        value set to 'Security Update', 'Update Rollup', or 'Hotfix', because
        that indicates it's an update to an existing program.

        Returns:
            bool: True if the package needs to be skipped, otherwise False
        '''
        if include_updates:
            return False
        skip_types = ['Hotfix',
                      'Security Update',
                      'Update Rollup']
        if __utils__['reg.value_exists'](
                hive=hive,
                key='{0}\\{1}'.format(key, sub_key),
                vname='ReleaseType',
                use_32bit_registry=use_32bit):
            if __utils__['reg.read_value'](
                    hive=hive,
                    key='{0}\\{1}'.format(key, sub_key),
                    vname='ReleaseType',
                    use_32bit_registry=use_32bit)['vdata'] in skip_types:
                return True
        return False

    def skip_parent_key(hive, key, sub_key, use_32bit):
        '''
        'ParentKeyName' must NOT be present, because that indicates it's an
        update to the parent program.

        Returns:
            bool: True if the package needs to be skipped, otherwise False
        '''
        if __utils__['reg.value_exists'](
                hive=hive,
                key='{0}\\{1}'.format(key, sub_key),
                vname='ParentKeyName',
                use_32bit_registry=use_32bit):
            return True

        return False

    def add_software(hive, key, sub_key, use_32bit):
        '''
        'DisplayName' must be present with a valid value, as this is reflected
        as the software name returned by pkg.list_pkgs. Also, its value must
        not start with 'KB' followed by 6 numbers - as that indicates a
        Windows update.
        '''
        d_name_regdata = __utils__['reg.read_value'](
            hive=hive,
            key='{0}\\{1}'.format(key, sub_key),
            vname='DisplayName',
            use_32bit_registry=use_32bit)

        if (not d_name_regdata['success'] or
                d_name_regdata['vtype'] not in ['REG_SZ', 'REG_EXPAND_SZ'] or
                d_name_regdata['vdata'] in ['(value not set)', None, False]):
            return
        d_name = d_name_regdata['vdata']

        if not include_updates:
            if re.match(r'^KB[0-9]{6}', d_name):
                return

        d_vers_regdata = __utils__['reg.read_value'](
            hive=hive,
            key='{0}\\{1}'.format(key, sub_key),
            vname='DisplayVersion',
            use_32bit_registry=use_32bit)

        d_vers = 'Not Found'
        if (d_vers_regdata['success'] and
                d_vers_regdata['vtype'] in ['REG_SZ', 'REG_EXPAND_SZ', 'REG_DWORD']):
            if isinstance(d_vers_regdata['vdata'], int):
                d_vers = six.text_type(d_vers_regdata['vdata'])
            elif d_vers_regdata['vdata'] and d_vers_regdata['vdata'] != '(value not set)':  # Check for blank values
                d_vers = d_vers_regdata['vdata']

        reg_software.setdefault(d_name, []).append(d_vers)

    # Start gathering information from the registry
    # HKLM Uninstall 64 bit
    kwargs = {'hive': 'HKLM',
              'key': 'Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall',
              'use_32bit': False}
    for sub_key in __utils__['reg.list_keys'](hive=kwargs['hive'],
                                              key=kwargs['key']):
        kwargs['sub_key'] = sub_key
        if skip_component(**kwargs):
            continue
        if skip_win_installer(**kwargs):
            continue
        if skip_uninstall_string(**kwargs):
            continue
        if skip_release_type(**kwargs):
            continue
        if skip_parent_key(**kwargs):
            continue
        add_software(**kwargs)

    # HKLM Uninstall 32 bit
    kwargs['use_32bit'] = True
    for sub_key in __utils__['reg.list_keys'](hive=kwargs['hive'],
                                              key=kwargs['key'],
                                              use_32bit_registry=kwargs['use_32bit']):
        kwargs['sub_key'] = sub_key
        if skip_component(**kwargs):
            continue
        if skip_win_installer(**kwargs):
            continue
        if skip_uninstall_string(**kwargs):
            continue
        if skip_release_type(**kwargs):
            continue
        if skip_parent_key(**kwargs):
            continue
        add_software(**kwargs)

    # HKLM Uninstall 64 bit
    kwargs = {'hive': 'HKLM',
              'key': 'Software\\Classes\\Installer\\Products',
              'use_32bit': False}
    userdata_key = 'Software\\Microsoft\\Windows\\CurrentVersion\\Installer\\' \
                   'UserData\\S-1-5-18\\Products'
    for sub_key in __utils__['reg.list_keys'](hive=kwargs['hive'], key=kwargs['key']):
        # If the key does not exist in userdata, skip it
        if not __utils__['reg.key_exists'](
                hive=kwargs['hive'],
                key='{0}\\{1}'.format(userdata_key, sub_key)):
            continue
        kwargs['sub_key'] = sub_key
        if skip_component(**kwargs):
            continue
        if skip_win_installer(**kwargs):
            continue
        add_software(**kwargs)

    # Uninstall for each user on the system (HKU), 64 bit
    # This has a propensity to take a while on a machine where many users have
    # logged in. Untested in such a scenario
    hive_hku = 'HKU'
    uninstall_key = '{0}\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall'
    product_key = '{0}\\Software\\Microsoft\\Installer\\Products'
    user_data_key = 'Software\\Microsoft\\Windows\\CurrentVersion\\Installer\\' \
                    'UserData\\{0}\\Products\\{1}'
    for user_guid in __utils__['reg.list_keys'](hive=hive_hku):
        kwargs = {'hive': hive_hku,
                  'key': uninstall_key.format(user_guid),
                  'use_32bit': False}
        for sub_key in __utils__['reg.list_keys'](hive=kwargs['hive'],
                                                  key=kwargs['key']):
            kwargs['sub_key'] = sub_key
            if skip_component(**kwargs):
                continue
            if skip_win_installer(**kwargs):
                continue
            if skip_uninstall_string(**kwargs):
                continue
            if skip_release_type(**kwargs):
                continue
            if skip_parent_key(**kwargs):
                continue
            add_software(**kwargs)

        # While we have the user guid, we're gong to check userdata in HKLM
        for sub_key in __utils__['reg.list_keys'](hive=hive_hku,
                                                  key=product_key.format(user_guid)):
            kwargs = {'hive': 'HKLM',
                      'key': user_data_key.format(user_guid, sub_key),
                      'sub_key': 'InstallProperties',
                      'use_32bit': False}
            if __utils__['reg.key_exists'](hive=kwargs['hive'],
                                           key=kwargs['key']):
                if skip_component(**kwargs):
                    continue
                add_software(**kwargs)

    # Uninstall for each user on the system (HKU), 32 bit
    for user_guid in __utils__['reg.list_keys'](hive=hive_hku,
                                                use_32bit_registry=True):
        kwargs = {'hive': hive_hku,
                  'key': uninstall_key.format(user_guid),
                  'use_32bit': True}
        for sub_key in __utils__['reg.list_keys'](hive=kwargs['hive'],
                                                  key=kwargs['key'],
                                                  use_32bit_registry=kwargs['use_32bit']):
            kwargs['sub_key'] = sub_key
            if skip_component(**kwargs):
                continue
            if skip_win_installer(**kwargs):
                continue
            if skip_uninstall_string(**kwargs):
                continue
            if skip_release_type(**kwargs):
                continue
            if skip_parent_key(**kwargs):
                continue
            add_software(**kwargs)

        # While we have the user guid, we're gong to check userdata in HKLM
        for sub_key_2 in __utils__['reg.list_keys'](hive=hive_hku,
                                                    key=product_key.format(user_guid),
                                                    use_32bit_registry=True):
            kwargs = {'hive': 'HKLM',
                      'key': user_data_key.format(user_guid, sub_key_2),
                      'sub_key': 'InstallProperties',
                      'use_32bit': True}
            if __utils__['reg.key_exists'](hive=kwargs['hive'],
                                           key=kwargs['key'],
                                           use_32bit_registry=kwargs['use_32bit']):
                if skip_component(**kwargs):
                    continue
                add_software(**kwargs)

    return reg_software","This searches the uninstall keys in the registry to find a match in the sub
    keys, it will return a dict with the display name as the key and the
    version as the value

    Args:

        include_components (bool):
            Include sub components of installed software. Default is ``True``

        include_updates (bool):
            Include software updates and Windows updates. Default is ``True``

    Returns:
        dict: A dictionary of installed software with versions installed

    .. code-block:: cfg

        {'<package_name>': '<version>'}"
"def errorRecorder(self, lineNumber, offset, text, check):
        """"""
        A function to override report_error in pycodestyle.
        And record output warnings.

        @param lineNumber: line number
        @param offset: column offset
        @param text: warning message
        @param check: check object in pycodestyle
        """"""
        code = text.split("" "")[0]
        lineOffset = self.report.line_offset

        self.warnings.append((lineOffset + lineNumber,
                              offset + 1, code, text))","A function to override report_error in pycodestyle.
        And record output warnings.

        @param lineNumber: line number
        @param offset: column offset
        @param text: warning message
        @param check: check object in pycodestyle"
"def partitions(collection):
    """"""Generate all set partitions of a collection.

    Example:
        >>> list(partitions(range(3)))  # doctest: +NORMALIZE_WHITESPACE
        [[[0, 1, 2]],
         [[0], [1, 2]],
         [[0, 1], [2]],
         [[1], [0, 2]],
         [[0], [1], [2]]]
    """"""
    collection = list(collection)

    # Special cases
    if not collection:
        return

    if len(collection) == 1:
        yield [collection]
        return

    first = collection[0]
    for smaller in partitions(collection[1:]):
        for n, subset in enumerate(smaller):
            yield smaller[:n] + [[first] + subset] + smaller[n+1:]
        yield [[first]] + smaller","Generate all set partitions of a collection.

    Example:
        >>> list(partitions(range(3)))  # doctest: +NORMALIZE_WHITESPACE
        [[[0, 1, 2]],
         [[0], [1, 2]],
         [[0, 1], [2]],
         [[1], [0, 2]],
         [[0], [1], [2]]]"
"def load_progress(self, resume_step):
        """""" load_progress: loads progress from restoration file
            Args: resume_step (str): step at which to resume session
            Returns: manager with progress from step
        """"""
        resume_step = Status[resume_step]
        progress_path = self.get_restore_path(resume_step)

        # If progress is corrupted, revert to step before
        while not self.check_for_session(resume_step):
            config.LOGGER.error(""Ricecooker has not reached {0} status. Reverting to earlier step..."".format(resume_step.name))
            # All files are corrupted or absent, restart process
            if resume_step.value - 1 < 0:
                self.init_session()
                return self
            resume_step = Status(resume_step.value - 1)
            progress_path = self.get_restore_path(resume_step)
        config.LOGGER.error(""Starting from status {0}"".format(resume_step.name))

        # Load manager
        with open(progress_path, 'rb') as handle:
            manager = pickle.load(handle)
            if isinstance(manager, RestoreManager):
                return manager
            else:
                return self","load_progress: loads progress from restoration file
            Args: resume_step (str): step at which to resume session
            Returns: manager with progress from step"
"def publish_topology_opened(self, topology_id):
        """"""Publish a TopologyOpenedEvent to all topology listeners.

        :Parameters:
         - `topology_id`: A unique identifier for the topology this server
           is a part of.
        """"""
        event = TopologyOpenedEvent(topology_id)
        for subscriber in self.__topology_listeners:
            try:
                subscriber.opened(event)
            except Exception:
                _handle_exception()","Publish a TopologyOpenedEvent to all topology listeners.

        :Parameters:
         - `topology_id`: A unique identifier for the topology this server
           is a part of."
"def create_image_menu_item(self, text, image_name):
        """"""
        Function creates a menu item with an image
        """"""
        menu_item = Gtk.ImageMenuItem(text)
        img = self.create_image(image_name)
        menu_item.set_image(img)
        return menu_item",Function creates a menu item with an image
"def syntheticMemberDecorator(self,
                                 memberName,
                                 defaultValue,
                                 contract,
                                 readOnly,
                                 privateMemberName,
                                 memberDelegate):
        """"""
    :type memberName: str
    :type readOnly: bool
    :type privateMemberName: str|None
    :type memberDelegate: IMemberDelegate
""""""
        def decoratorFunction(cls):
            syntheticMember = SyntheticMember(memberName,
                                              defaultValue,
                                              contract,
                                              readOnly,
                                              privateMemberName,
                                              memberDelegate = memberDelegate)

            SyntheticClassController(cls).addSyntheticMember(syntheticMember)

            return cls
        return decoratorFunction",":type memberName: str
    :type readOnly: bool
    :type privateMemberName: str|None
    :type memberDelegate: IMemberDelegate"
"def update_existing_collection(self,
                                   owner_id,
                                   collection_id=None,
                                   json_repr=None,
                                   auth_info=None,
                                   parent_sha=None,
                                   merged_sha=None,
                                   commit_msg=''):
        """"""Validate and save this JSON. Ensure (and return) a unique collection id""""""
        collection = self._coerce_json_to_collection(json_repr)
        if collection is None:
            msg = ""File failed to parse as JSON:\n{j}"".format(j=json_repr)
            raise ValueError(msg)
        if not self._is_valid_collection_json(collection):
            msg = ""JSON is not a valid collection:\n{j}"".format(j=json_repr)
            raise ValueError(msg)
        if not collection_id:
            raise ValueError(""Collection id not provided (or invalid)"")
        if not self.has_doc(collection_id):
            msg = ""Unexpected collection id '{}' (expected an existing id!)"".format(collection_id)
            raise ValueError(msg)
        # pass the id and collection JSON to a proper git action
        r = None
        try:
            # remove any 'url' field before saving; it will be restored when the doc is fetched (via API)
            if 'url' in collection:
                del collection['url']
            # keep it simple (collection is already validated! no annotations needed!)
            r = self.commit_and_try_merge2master(file_content=collection,
                                                 doc_id=collection_id,
                                                 auth_info=auth_info,
                                                 parent_sha=parent_sha,
                                                 commit_msg=commit_msg,
                                                 merged_sha=merged_sha)
            # identify shard for this id!?
        except:
            raise
        return r",Validate and save this JSON. Ensure (and return) a unique collection id
"def _tower_loss(images, labels, num_classes, scope, reuse_variables=None):
  """"""Calculate the total loss on a single tower running the ImageNet model.

  We perform 'batch splitting'. This means that we cut up a batch across
  multiple GPU's. For instance, if the batch size = 32 and num_gpus = 2,
  then each tower will operate on an batch of 16 images.

  Args:
    images: Images. 4D tensor of size [batch_size, FLAGS.image_size,
                                       FLAGS.image_size, 3].
    labels: 1-D integer Tensor of [batch_size].
    num_classes: number of classes
    scope: unique prefix string identifying the ImageNet tower, e.g.
      'tower_0'.

  Returns:
     Tensor of shape [] containing the total loss for a batch of data
  """"""
  # When fine-tuning a model, we do not restore the logits but instead we
  # randomly initialize the logits. The number of classes in the output of the
  # logit is the number of classes in specified Dataset.
  restore_logits = not FLAGS.fine_tune

  # Build inference Graph.
  with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):
    logits = inception.inference(images, num_classes, for_training=True,
                                 restore_logits=restore_logits,
                                 scope=scope)

  # Build the portion of the Graph calculating the losses. Note that we will
  # assemble the total_loss using a custom function below.
  split_batch_size = images.get_shape().as_list()[0]
  inception.loss(logits, labels, batch_size=split_batch_size)

  # Assemble all of the losses for the current tower only.
  losses = tf.get_collection(slim.losses.LOSSES_COLLECTION, scope)

  # Calculate the total loss for the current tower.
  regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  total_loss = tf.add_n(losses + regularization_losses, name='total_loss')

  # Compute the moving average of all individual losses and the total loss.
  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')
  loss_averages_op = loss_averages.apply(losses + [total_loss])

  # Attach a scalar summmary to all individual losses and the total loss; do the
  # same for the averaged version of the losses.
  for l in losses + [total_loss]:
    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training
    # session. This helps the clarity of presentation on TensorBoard.
    loss_name = re.sub('%s_[0-9]*/' % inception.TOWER_NAME, '', l.op.name)
    # Name each loss as '(raw)' and name the moving average version of the loss
    # as the original loss name.
    tf.summary.scalar(loss_name +' (raw)', l)
    tf.summary.scalar(loss_name, loss_averages.average(l))

  with tf.control_dependencies([loss_averages_op]):
    total_loss = tf.identity(total_loss)
  return total_loss","Calculate the total loss on a single tower running the ImageNet model.

  We perform 'batch splitting'. This means that we cut up a batch across
  multiple GPU's. For instance, if the batch size = 32 and num_gpus = 2,
  then each tower will operate on an batch of 16 images.

  Args:
    images: Images. 4D tensor of size [batch_size, FLAGS.image_size,
                                       FLAGS.image_size, 3].
    labels: 1-D integer Tensor of [batch_size].
    num_classes: number of classes
    scope: unique prefix string identifying the ImageNet tower, e.g.
      'tower_0'.

  Returns:
     Tensor of shape [] containing the total loss for a batch of data"
"def schedule_exception(exception, target):
    """"""schedule a greenlet to have an exception raised in it immediately

    :param exception: the exception to raise in the greenlet
    :type exception: Exception
    :param target: the greenlet that should receive the exception
    :type target: greenlet
    """"""
    if not isinstance(target, compat.greenlet):
        raise TypeError(""can only schedule exceptions for greenlets"")
    if target.dead:
        raise ValueError(""can't send exceptions to a dead greenlet"")
    schedule(target)
    state.to_raise[target] = exception","schedule a greenlet to have an exception raised in it immediately

    :param exception: the exception to raise in the greenlet
    :type exception: Exception
    :param target: the greenlet that should receive the exception
    :type target: greenlet"
"def dict_map(function, dictionary):
	""""""
	dict_map is much like the built-in function map.  It takes a dictionary
	and applys a function to the values of that dictionary, returning a
	new dictionary with the mapped values in the original keys.

	>>> d = dict_map(lambda x:x+1, dict(a=1, b=2))
	>>> d == dict(a=2,b=3)
	True
	""""""
	return dict((key, function(value)) for key, value in dictionary.items())","dict_map is much like the built-in function map.  It takes a dictionary
	and applys a function to the values of that dictionary, returning a
	new dictionary with the mapped values in the original keys.

	>>> d = dict_map(lambda x:x+1, dict(a=1, b=2))
	>>> d == dict(a=2,b=3)
	True"
"def get_metadata_or_fail(metadata_key):
  """"""
  Call get_metadata; halt with fail() if it raises an exception
  """"""
  try:
    return http_get_metadata(metadata_key)
  except IOError as error:
    fail(""Exception in http_get_metadata {} {}"".format(metadata_key, repr(error)))",Call get_metadata; halt with fail() if it raises an exception
"def PluRunOff_single_winner(self, profile):
        """"""
        Returns a number that associates the winner of a profile under Plurality with Runoff rule.

        :ivar Profile profile: A Profile object that represents an election profile.
        """"""

        # Currently, we expect the profile to contain complete ordering over candidates. Ties are
        # allowed however.
        elecType = profile.getElecType()
        if elecType != ""soc"" and elecType != ""toc"" and elecType != ""csv"":
            print(""ERROR: unsupported election type"")
            exit()

        # Initialization
        prefcounts = profile.getPreferenceCounts()
        len_prefcounts = len(prefcounts)
        rankmaps = profile.getRankMaps()
        ranking = MechanismPlurality().getRanking(profile)

        # 1st round: find the top 2 candidates in plurality scores
        # Compute the 1st-place candidate in plurality scores
        # print(ranking)
        max_cand = ranking[0][0][0]

        # Compute the 2nd-place candidate in plurality scores
        # Automatically using tie-breaking rule--numerically increasing order
        if len(ranking[0][0]) > 1:
            second_max_cand = ranking[0][0][1]
        else:
            second_max_cand = ranking[0][1][0]

        top_2 = [max_cand, second_max_cand]
        # 2nd round: find the candidate with maximum plurality score
        dict_top2 = {max_cand: 0, second_max_cand: 0}
        for i in range(len_prefcounts):
            vote_top2 = {key: value for key, value in rankmaps[i].items() if key in top_2}
            top_position = min(vote_top2.values())
            keys = [x for x in vote_top2.keys() if vote_top2[x] == top_position]
            for key in keys:
                dict_top2[key] += prefcounts[i]

        # print(dict_top2)
        winner = max(dict_top2.items(), key=lambda x: x[1])[0]

        return winner","Returns a number that associates the winner of a profile under Plurality with Runoff rule.

        :ivar Profile profile: A Profile object that represents an election profile."
"def get_group_members(self, group_name):
        """"""Get group members via provisioning API.
        If you get back an error 999, then the provisioning API is not enabled.

        :param group_name:  name of group to list members
        :returns: list of group members
        :raises: HTTPResponseError in case an HTTP error status was returned

        """"""
        res = self._make_ocs_request(
            'GET',
            self.OCS_SERVICE_CLOUD,
            'groups/' + group_name
        )

        if res.status_code == 200:
            tree = ET.fromstring(res.content)
            self._check_ocs_status(tree, [100])
            return [group.text for group in tree.find('data/users')]

        raise HTTPResponseError(res)","Get group members via provisioning API.
        If you get back an error 999, then the provisioning API is not enabled.

        :param group_name:  name of group to list members
        :returns: list of group members
        :raises: HTTPResponseError in case an HTTP error status was returned"
"def addcommenttomergerequest(self, project_id, mergerequest_id, note):
        """"""
        Add a comment to a merge request.

        :param project_id: ID of the project originating the merge request
        :param mergerequest_id: ID of the merge request to comment on
        :param note: Text of comment
        :return: True if success
        """"""
        request = requests.post(
            '{0}/{1}/merge_request/{2}/comments'.format(self.projects_url, project_id, mergerequest_id),
            data={'note': note}, headers=self.headers, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)

        return request.status_code == 201","Add a comment to a merge request.

        :param project_id: ID of the project originating the merge request
        :param mergerequest_id: ID of the merge request to comment on
        :param note: Text of comment
        :return: True if success"
"def deregister_image(self, ami_id, region='us-east-1'):
        """"""
        Deregister an AMI by id
        :param ami_id:
        :param region: region to deregister from
        :return:
        """"""
        deregister_cmd = ""aws ec2 --profile {} --region {} deregister-image --image-id {}""\
            .format(self.aws_project, region, ami_id)
        print ""De-registering old image, now that the new one exists.""
        print ""De-registering cmd: {}"".format(deregister_cmd)
        res = subprocess.check_output(shlex.split(deregister_cmd))
        print ""Response: {}"".format(res)
        print ""Not monitoring de-register command""","Deregister an AMI by id
        :param ami_id:
        :param region: region to deregister from
        :return:"
"def progress(self,
                 enumerable,
                 task_progress_object=None):
        """"""
        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.
        :param enumerable:              Collection to iterate over.
        :param task_progress_object:    [Optional] TaskProgress object holding the progress bar information.
        :return:                        The logger instance.
        """"""
        self.list = enumerable
        self.list_length = len(enumerable)
        self.task_id = uuid.uuid4()
        self.index = 0

        if task_progress_object:
            # Force total attribute
            task_progress_object.total = self.list_length
        else:
            task_progress_object = TaskProgress(total=self.list_length,
                                                display_time=True,
                                                prefix='Progress')

        # Create a task progress
        self.set_task_object(task_id=self.task_id,
                             task_progress_object=task_progress_object)

        return self","Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.
        :param enumerable:              Collection to iterate over.
        :param task_progress_object:    [Optional] TaskProgress object holding the progress bar information.
        :return:                        The logger instance."
"def src_file(self):
        """"""
        Get the latest src_uri for a stage 3 tarball.

        Returns (str):
            Latest src_uri from gentoo's distfiles mirror.
        """"""
        try:
            src_uri = (curl[Gentoo._LATEST_TXT] | tail[""-n"", ""+3""]
                       | cut[""-f1"", ""-d ""])().strip()
        except ProcessExecutionError as proc_ex:
            src_uri = ""NOT-FOUND""
            LOG.error(""Could not determine latest stage3 src uri: %s"",
                      str(proc_ex))
        return src_uri","Get the latest src_uri for a stage 3 tarball.

        Returns (str):
            Latest src_uri from gentoo's distfiles mirror."
"def get_template_names(self):
        '''
        Build the list of templates related to this user
        '''

        # Get user template
        template_model = getattr(self, 'template_model', ""{0}/{1}_{2}"".format(self._appname.lower(), self._modelname.lower(), self.get_template_names_key))
        template_model_ext = getattr(self, 'template_model_ext', 'html')
        templates = get_template(template_model, self.user, self.language, template_model_ext, raise_error=False)
        if type(templates) == list:
            templates.append(""codenerix/{0}.html"".format(self.get_template_names_key))

        # Return thet of templates
        return templates",Build the list of templates related to this user
"def split_name(obj):
    """"""
    If the supplied legislator/person object is missing 'first_name'
    or 'last_name' then use name_tools to split.
    """"""
    if obj['_type'] in ('person', 'legislator'):
        for key in ('first_name', 'last_name'):
            if key not in obj or not obj[key]:
                # Need to split
                (obj['first_name'], obj['last_name'],
                 obj['suffixes']) = name_tools.split(obj['full_name'])[1:]
                break

    return obj","If the supplied legislator/person object is missing 'first_name'
    or 'last_name' then use name_tools to split."
"def plot_return_on_dollar(rets, title='Return on $1', show_maxdd=0, figsize=None, ax=None, append=0, label=None, **plot_args):
    """""" Show the cumulative return of specified rets and max drawdowns if selected.""""""
    crets = (1. + returns_cumulative(rets, expanding=1))
    if isinstance(crets, pd.DataFrame):
        tmp = crets.copy()
        for c in tmp.columns:
            s = tmp[c]
            fv = s.first_valid_index()
            fi = s.index.get_loc(fv)
            if fi != 0:
                tmp.ix[fi - 1, c] = 1.
            else:
                if not s.index.freq:
                    # no frequency set
                    freq = guess_freq(s.index)
                    s = s.asfreq(freq)
                first = s.index.shift(-1)[0]
                tmp = pd.concat([pd.DataFrame({c: [1.]}, index=[first]), tmp])
        crets = tmp
        if append:
            toadd = crets.index.shift(1)[-1]
            crets = pd.concat([crets, pd.DataFrame(np.nan, columns=crets.columns, index=[toadd])])
    else:
        fv = crets.first_valid_index()
        fi = crets.index.get_loc(fv)
        if fi != 0:
            crets = crets.copy()
            crets.iloc[fi - 1] = 1.
        else:
            if not crets.index.freq:
                first = crets.asfreq(guess_freq(crets.index)).index.shift(-1)[0]
            else:
                first = crets.index.shift(-1)[0]
            tmp = pd.Series([1.], index=[first])
            tmp = tmp.append(crets)
            crets = tmp

        if append:
            toadd = pd.Series(np.nan, index=[crets.index.shift(1)[-1]])
            crets = crets.append(toadd)

    ax = crets.plot(figsize=figsize, title=title, ax=ax, label=label, **plot_args)
    AxesFormat().Y.apply_format(new_float_formatter()).X.label("""").apply(ax)
    #ax.tick_params(labelsize=14)
    if show_maxdd:
        # find the max drawdown available by using original rets
        if isinstance(rets, pd.DataFrame):
            iterator = rets.iteritems()
        else:
            iterator = iter([('', rets)])

        for c, col in iterator:
            dd, dt = max_drawdown(col, inc_date=1)
            lbl = c and c + ' maxdd' or 'maxdd'
            # get cret to place annotation correctly
            if isinstance(crets, pd.DataFrame):
                amt = crets.ix[dt, c]
            else:
                amt = crets[dt]

            bbox_props = dict(boxstyle=""round"", fc=""w"", ec=""0.5"", alpha=0.7)
            # sub = lambda c: c and len(c) > 2 and c[:2] or c
            try:
                dtstr = '{0}'.format(dt.to_period())
            except:
                dtstr = '{0}'.format(dt)

            ax.text(dt, amt, ""mdd {0}"".format(dtstr).strip(), ha=""center"",
                    va=""center"", size=10, bbox=bbox_props)
    plt.tight_layout()",Show the cumulative return of specified rets and max drawdowns if selected.
"def remove_subproducts(self):
        """"""Removes all archived files subproducts associated with this DP""""""
        if not self.fullpath or not self.archived:
            raise RuntimeError(""""""Can't remove a non-archived data product"""""")
        for root, dirs, files in os.walk(self.subproduct_dir(), topdown=False):
            for name in files:
                try:
                    os.remove(os.path.join(root, name))
                except:
                    pass
            for name in dirs:
                try:
                    os.remove(os.path.join(root, name))
                except:
                    pass",Removes all archived files subproducts associated with this DP
"def crl(self):
        """"""
        Returns up to date CRL of this CA
        """"""
        revoked_certs = self.get_revoked_certs()
        crl = crypto.CRL()
        now_str = timezone.now().strftime(generalized_time)
        for cert in revoked_certs:
            revoked = crypto.Revoked()
            revoked.set_serial(bytes_compat(cert.serial_number))
            revoked.set_reason(b'unspecified')
            revoked.set_rev_date(bytes_compat(now_str))
            crl.add_revoked(revoked)
        return crl.export(self.x509, self.pkey, days=1, digest=b'sha256')",Returns up to date CRL of this CA
"def list_container_subdirs(self, container, limit=None, marker=None,
            prefix=None, delimiter=None, full_listing=False):
        """"""
        Although you cannot nest directories, you can simulate a hierarchical
        structure within a single container by adding forward slash characters
        (/) in the object name. This method returns a list of all of these
        pseudo-subdirectories in the specified container.
        """"""
        return self._manager.list_subdirs(container, limit=limit,
                marker=marker, prefix=prefix, delimiter=delimiter,
                full_listing=full_listing)","Although you cannot nest directories, you can simulate a hierarchical
        structure within a single container by adding forward slash characters
        (/) in the object name. This method returns a list of all of these
        pseudo-subdirectories in the specified container."
"def hit_delete(self, all_hits, hit_ids=None):
        ''' Delete HIT. '''
        if all_hits:
            hits_data = self.amt_services.get_all_hits()
            hit_ids = [hit.options['hitid'] for hit in hits_data if \
                       hit.options['status'] == ""Reviewable""]
        for hit in hit_ids:
            # Check that the HIT is reviewable
            status = self.amt_services.get_hit_status(hit)
            if not status:
                print ""*** Error getting hit status""
                return
            if self.amt_services.get_hit_status(hit) != ""Reviewable"":
                print(""*** This hit is not 'Reviewable' and so can not be ""
                      ""deleted"")
                return
            else:
                success = self.amt_services.delete_hit(hit)
                # self.web_services.delete_ad(hit)  # also delete the ad
                if success:
                    if self.sandbox:
                        print ""deleting sandbox HIT"", hit
                    else:
                        print ""deleting live HIT"", hit",Delete HIT.
"def get_input_stream(environ, safe_fallback=True):
    """"""Returns the input stream from the WSGI environment and wraps it
    in the most sensible way possible.  The stream returned is not the
    raw WSGI stream in most cases but one that is safe to read from
    without taking into account the content length.

    .. versionadded:: 0.9

    :param environ: the WSGI environ to fetch the stream from.
    :param safe: indicates weather the function should use an empty
                 stream as safe fallback or just return the original
                 WSGI input stream if it can't wrap it safely.  The
                 default is to return an empty string in those cases.
    """"""
    stream = environ['wsgi.input']
    content_length = get_content_length(environ)

    # A wsgi extension that tells us if the input is terminated.  In
    # that case we return the stream unchanged as we know we can savely
    # read it until the end.
    if environ.get('wsgi.input_terminated'):
        return stream

    # If we don't have a content length we fall back to an empty stream
    # in case of a safe fallback, otherwise we return the stream unchanged.
    # The non-safe fallback is not recommended but might be useful in
    # some situations.
    if content_length is None:
        return safe_fallback and _empty_stream or stream

    # Otherwise limit the stream to the content length
    return LimitedStream(stream, content_length)","Returns the input stream from the WSGI environment and wraps it
    in the most sensible way possible.  The stream returned is not the
    raw WSGI stream in most cases but one that is safe to read from
    without taking into account the content length.

    .. versionadded:: 0.9

    :param environ: the WSGI environ to fetch the stream from.
    :param safe: indicates weather the function should use an empty
                 stream as safe fallback or just return the original
                 WSGI input stream if it can't wrap it safely.  The
                 default is to return an empty string in those cases."
"def save_csv(data,  # type: Iterable[Tuple[Union[str, int], ...]]
             headers,  # type: Iterable[str]
             file  # type: TextIO
             ):
    # type: (...) -> None
    """"""
    Output generated data to file as CSV with header.

    :param data: An iterable of tuples containing raw data.
    :param headers: Iterable of feature names
    :param file: A writeable stream in which to write the CSV
    """"""

    print(','.join(headers), file=file)
    writer = csv.writer(file)
    writer.writerows(data)","Output generated data to file as CSV with header.

    :param data: An iterable of tuples containing raw data.
    :param headers: Iterable of feature names
    :param file: A writeable stream in which to write the CSV"
"def _ParseFieldsMetadata(self, structure):
    """"""Parses the fields metadata and updates the log line definition to match.

    Args:
      structure (pyparsing.ParseResults): structure parsed from the log file.
    """"""
    fields = structure.fields.split(' ')

    log_line_structure = pyparsing.Empty()
    if fields[0] == 'date' and fields[1] == 'time':
      log_line_structure += self.DATE_TIME.setResultsName('date_time')
      fields = fields[2:]

    for member in fields:
      log_line_structure += self._LOG_LINE_STRUCTURES.get(member, self.URI)

    updated_structures = []
    for line_structure in self._line_structures:
      if line_structure[0] != 'logline':
        updated_structures.append(line_structure)
    updated_structures.append(('logline', log_line_structure))
    # TODO: self._line_structures is a work-around and this needs
    # a structural fix.
    self._line_structures = updated_structures","Parses the fields metadata and updates the log line definition to match.

    Args:
      structure (pyparsing.ParseResults): structure parsed from the log file."
"def _websocket_mask_python(mask: bytes, data: bytes) -> bytes:
    """"""Websocket masking function.

    `mask` is a `bytes` object of length 4; `data` is a `bytes` object of any length.
    Returns a `bytes` object of the same length as `data` with the mask applied
    as specified in section 5.3 of RFC 6455.

    This pure-python implementation may be replaced by an optimized version when available.
    """"""
    mask_arr = array.array(""B"", mask)
    unmasked_arr = array.array(""B"", data)
    for i in range(len(data)):
        unmasked_arr[i] = unmasked_arr[i] ^ mask_arr[i % 4]
    return unmasked_arr.tobytes()","Websocket masking function.

    `mask` is a `bytes` object of length 4; `data` is a `bytes` object of any length.
    Returns a `bytes` object of the same length as `data` with the mask applied
    as specified in section 5.3 of RFC 6455.

    This pure-python implementation may be replaced by an optimized version when available."
"def get_info(self, location):
        """"""Returns (url, revision), where both are strings""""""
        assert not location.rstrip('/').endswith(self.dirname), 'Bad directory: %s' % location
        output = call_subprocess(
            [self.cmd, 'info', location], show_stdout=False, extra_environ={'LANG': 'C'})
        match = _svn_url_re.search(output)
        if not match:
            logger.warn('Cannot determine URL of svn checkout %s' % display_path(location))
            logger.info('Output that cannot be parsed: \n%s' % output)
            return None, None
        url = match.group(1).strip()
        match = _svn_revision_re.search(output)
        if not match:
            logger.warn('Cannot determine revision of svn checkout %s' % display_path(location))
            logger.info('Output that cannot be parsed: \n%s' % output)
            return url, None
        return url, match.group(1)","Returns (url, revision), where both are strings"
"def canonicalize_url(url, keep_params=False, keep_fragments=False):
    """"""Canonicalize the given url by applying the following procedures:

    # a sort query arguments, first by key, then by value
    # b percent encode paths and query arguments. non-ASCII characters are
    # c percent-encoded using UTF-8 (RFC-3986)
    # d normalize all spaces (in query arguments) '+' (plus symbol)
    # e normalize percent encodings case (%2f -> %2F)
    # f remove query arguments with blank values (unless site in NONCANONIC_SITES)
    # g remove fragments (unless #!)
    # h remove username/password at front of domain
    # i remove port if 80, keep if not
    # k remove query arguments (unless site in USEFUL_QUERY_KEYS)

    The url passed can be a str or unicode, while the url returned is always a
    str.
    """"""
    if keep_params:
        # Preserve all query params
        parsed = extract(norm(url))
    else:
        # Remove unwanted params
        parsed = extract(url_query_cleaner(normalize(url), parameterlist=config.USEFUL_QUERY_KEYS))

    # Sort params, remove blank if not wanted
    query = urllib.urlencode(sorted(urlparse.parse_qsl(parsed.query, keep_blank_values=keep_params)))
    fragment = getFragment(url, keep_fragments)

    # The following is to remove orphaned '=' from query string params with no values
    query = re.sub(r""=$"", """", query.replace(""=&"", ""&""))

    # Reconstruct URL, escaping apart from safe chars
    # See http://stackoverflow.com/questions/2849756/list-of-valid-characters-for-the-fragment-identifier-in-an-url
    # http://stackoverflow.com/questions/4669692/valid-characters-for-directory-part-of-a-url-for-short-links
    safe = ""/.-_~!$&'()*+,;=:@""
    newurl = construct(URL(parsed.scheme, '', '', parsed.subdomain, parsed.domain, parsed.tld, parsed.port, quote(parsed.path, safe=safe), query, quote(fragment, safe=safe), ''))
    return newurl.rstrip('/')","Canonicalize the given url by applying the following procedures:

    # a sort query arguments, first by key, then by value
    # b percent encode paths and query arguments. non-ASCII characters are
    # c percent-encoded using UTF-8 (RFC-3986)
    # d normalize all spaces (in query arguments) '+' (plus symbol)
    # e normalize percent encodings case (%2f -> %2F)
    # f remove query arguments with blank values (unless site in NONCANONIC_SITES)
    # g remove fragments (unless #!)
    # h remove username/password at front of domain
    # i remove port if 80, keep if not
    # k remove query arguments (unless site in USEFUL_QUERY_KEYS)

    The url passed can be a str or unicode, while the url returned is always a
    str."
"def record(self, value=1.0, time_ms=None):
        """"""
        Record a value at a known time.
        Arguments:
            value (double): The value we are recording
            time_ms (int): A POSIX timestamp in milliseconds.
                Default: The time when record() is evaluated (now)

        Raises:
            QuotaViolationException: if recording this value moves a
                metric beyond its configured maximum or minimum bound
        """"""
        if time_ms is None:
            time_ms = time.time() * 1000
        self._last_record_time = time_ms
        with self._lock:  # XXX high volume, might be performance issue
            # increment all the stats
            for stat in self._stats:
                stat.record(self._config, value, time_ms)
            self._check_quotas(time_ms)
        for parent in self._parents:
            parent.record(value, time_ms)","Record a value at a known time.
        Arguments:
            value (double): The value we are recording
            time_ms (int): A POSIX timestamp in milliseconds.
                Default: The time when record() is evaluated (now)

        Raises:
            QuotaViolationException: if recording this value moves a
                metric beyond its configured maximum or minimum bound"
"def random_pairs_with_replacement(n, shape, random_state=None):
    """"""make random record pairs""""""

    if not isinstance(random_state, np.random.RandomState):
        random_state = np.random.RandomState(random_state)

    n_max = max_pairs(shape)

    if n_max <= 0:
        raise ValueError('n_max must be larger than 0')

    # make random pairs
    indices = random_state.randint(0, n_max, n)

    if len(shape) == 1:
        return _map_tril_1d_on_2d(indices, shape[0])
    else:
        return np.unravel_index(indices, shape)",make random record pairs
"def command(self):
        """"""Returns a string representing the command you have to type to obtain the same packet""""""
        f = []
        for fn,fv in self.fields.items():
            fld = self.get_field(fn)
            if isinstance(fv, Packet):
                fv = fv.command()
            elif fld.islist and fld.holds_packets and type(fv) is list:
                #fv = ""[%s]"" % "","".join( map(Packet.command, fv))
                fv = ""[%s]"" % "","".join([ Packet.command(i) for i in fv ])
            else:
                fv = repr(fv)
            f.append(""%s=%s"" % (fn, fv))
        c = ""%s(%s)"" % (self.__class__.__name__, "", "".join(f))
        pc = self.payload.command()
        if pc:
            c += ""/""+pc
        return c",Returns a string representing the command you have to type to obtain the same packet
"def set_position_fast(self, val):
        """"""Set the devive OPEN LEVEL.""""""
        if val == 0:
            self.close_fast()
        else:
            setlevel = 255
            if val < 1:
                setlevel = val * 100
            elif val <= 0xff:
                setlevel = val
            set_command = StandardSend(
                self._address, COMMAND_LIGHT_ON_FAST_0X12_NONE, cmd2=setlevel)
            self._send_method(set_command, self._open_message_received)",Set the devive OPEN LEVEL.
"def population_chart_header_element(feature, parent):
    """"""Retrieve population chart header string from definitions.""""""
    _ = feature, parent  # NOQA
    header = population_chart_header['string_format']
    return header.capitalize()",Retrieve population chart header string from definitions.
"def parse_dict(parameters):
    """"""Decorator to parse parameters as a dict according to a set of criteria.

    This outer method is called to set up the decorator.

    Arguments:
        parameters: An array of parameter declarations tuples in the format:
        ('<param_name>', {'validate': [<ValidatorClass>,...], <options...>})

    Usage:

    @chassis.util.parameters.parse_dict([
        ('email', {'validators': [validators.Email], 'required': True}),
        ('password', {'validators': [validators.Password], 'required': True})
        ])
    def post(self, data):
        # Render JSON for the provided parameters
        self.render_json({'email': data['email'],
                          'password': data['password']})
    """"""
    # pylint: disable=protected-access
    @decorators.include_original
    def decorate(method):
        """"""Setup returns this decorator, which is called on the method.""""""

        def call(self, *args):
            """"""This is called whenever the decorated method is invoked.""""""

            arg_dict = _parse_arguments(self, method, parameters)
            return method(self, *args, data=arg_dict)

        # TODO: Autogenerate documentation data for parameters.

        return call
    return decorate","Decorator to parse parameters as a dict according to a set of criteria.

    This outer method is called to set up the decorator.

    Arguments:
        parameters: An array of parameter declarations tuples in the format:
        ('<param_name>', {'validate': [<ValidatorClass>,...], <options...>})

    Usage:

    @chassis.util.parameters.parse_dict([
        ('email', {'validators': [validators.Email], 'required': True}),
        ('password', {'validators': [validators.Password], 'required': True})
        ])
    def post(self, data):
        # Render JSON for the provided parameters
        self.render_json({'email': data['email'],
                          'password': data['password']})"
"def isexec(path):
    '''
    Check if given path points to an executable file.

    :param path: file path
    :type path: str
    :return: True if executable, False otherwise
    :rtype: bool
    '''
    return os.path.isfile(path) and os.access(path, os.X_OK)","Check if given path points to an executable file.

    :param path: file path
    :type path: str
    :return: True if executable, False otherwise
    :rtype: bool"
"def validate(self, uri):
		"""""" Check that an query part of an URI is compatible with this descriptor. Return True if the URI is
		compatible.

		:param uri: an URI to check

		:return: bool
		""""""
		if WURIComponentVerifier.validate(self, uri) is False:
			return False
		try:
			WStrictURIQuery(
				WURIQuery.parse(uri.component(self.component())),
				*self.__specs,
				extra_parameters=self.__extra_parameters
			)
		except ValueError:
			return False
		return True","Check that an query part of an URI is compatible with this descriptor. Return True if the URI is
		compatible.

		:param uri: an URI to check

		:return: bool"
"def dump_age(age=None):
    """"""Formats the duration as a base-10 integer.

    :param age: should be an integer number of seconds,
                a :class:`datetime.timedelta` object, or,
                if the age is unknown, `None` (default).
    """"""
    if age is None:
        return
    if isinstance(age, timedelta):
        # do the equivalent of Python 2.7's timedelta.total_seconds(),
        # but disregarding fractional seconds
        age = age.seconds + (age.days * 24 * 3600)

    age = int(age)
    if age < 0:
        raise ValueError(""age cannot be negative"")

    return str(age)","Formats the duration as a base-10 integer.

    :param age: should be an integer number of seconds,
                a :class:`datetime.timedelta` object, or,
                if the age is unknown, `None` (default)."
"def _merge_includes(self):
        """"""
        If ""include"" option exists in ""default.cfg"",
        read the file(glob-match) in the directory.
        """"""
        raw_include_path = self.get_global_include()
        if raw_include_path:
            abs_include_path = self._get_global_include_abs_path(
                raw_include_path
            )
            self._validate_global_include(abs_include_path)
            self.set_global_include(abs_include_path)

            for infile in glob.glob(abs_include_path):
                self.config.merge(
                    self._configobj_factory(infile=infile)
                )","If ""include"" option exists in ""default.cfg"",
        read the file(glob-match) in the directory."
"def parse_url_path(self, url_path: str) -> str:
        """"""Converts a static URL path into a filesystem path.

        ``url_path`` is the path component of the URL with
        ``static_url_prefix`` removed.  The return value should be
        filesystem path relative to ``static_path``.

        This is the inverse of `make_static_url`.
        """"""
        if os.path.sep != ""/"":
            url_path = url_path.replace(""/"", os.path.sep)
        return url_path","Converts a static URL path into a filesystem path.

        ``url_path`` is the path component of the URL with
        ``static_url_prefix`` removed.  The return value should be
        filesystem path relative to ``static_path``.

        This is the inverse of `make_static_url`."
"def _check_realign(data):
    """"""Check for realignment, which is not supported in GATK4
    """"""
    if ""gatk4"" not in data[""algorithm""].get(""tools_off"", []) and not ""gatk4"" == data[""algorithm""].get(""tools_off""):
        if data[""algorithm""].get(""realign""):
            raise ValueError(""In sample %s, realign specified but it is not supported for GATK4. ""
                             ""Realignment is generally not necessary for most variant callers."" %
                             (dd.get_sample_name(data)))","Check for realignment, which is not supported in GATK4"
"def upsert(
    queryset, model_objs, unique_fields,
    update_fields=None, returning=False, sync=False,
    ignore_duplicate_updates=True,
    return_untouched=False
):
    """"""
    Perform a bulk upsert on a table, optionally syncing the results.

    Args:
        queryset (Model|QuerySet): A model or a queryset that defines the collection to sync
        model_objs (List[Model]): A list of Django models to sync. All models in this list
            will be bulk upserted and any models not in the table (or queryset) will be deleted
            if sync=True.
        unique_fields (List[str]): A list of fields that define the uniqueness of the model. The
            model must have a unique constraint on these fields
        update_fields (List[str], default=None): A list of fields to update whenever objects
            already exist. If an empty list is provided, it is equivalent to doing a bulk
            insert on the objects that don't exist. If `None`, all fields will be updated.
        returning (bool|List[str]): If True, returns all fields. If a list, only returns
            fields in the list
        sync (bool, default=False): Perform a sync operation on the queryset
        ignore_duplicate_updates (bool, default=False): Don't perform an update if the row is
            a duplicate.
        return_untouched (bool, default=False): Return untouched rows by the operation
    """"""
    queryset = queryset if isinstance(queryset, models.QuerySet) else queryset.objects.all()
    model = queryset.model

    # Populate automatically generated fields in the rows like date times
    _fill_auto_fields(model, model_objs)

    # Sort the rows to reduce the chances of deadlock during concurrent upserts
    model_objs = _sort_by_unique_fields(model, model_objs, unique_fields)
    update_fields = _get_update_fields(model, unique_fields, update_fields)

    return _fetch(queryset, model_objs, unique_fields, update_fields, returning, sync,
                  ignore_duplicate_updates=ignore_duplicate_updates,
                  return_untouched=return_untouched)","Perform a bulk upsert on a table, optionally syncing the results.

    Args:
        queryset (Model|QuerySet): A model or a queryset that defines the collection to sync
        model_objs (List[Model]): A list of Django models to sync. All models in this list
            will be bulk upserted and any models not in the table (or queryset) will be deleted
            if sync=True.
        unique_fields (List[str]): A list of fields that define the uniqueness of the model. The
            model must have a unique constraint on these fields
        update_fields (List[str], default=None): A list of fields to update whenever objects
            already exist. If an empty list is provided, it is equivalent to doing a bulk
            insert on the objects that don't exist. If `None`, all fields will be updated.
        returning (bool|List[str]): If True, returns all fields. If a list, only returns
            fields in the list
        sync (bool, default=False): Perform a sync operation on the queryset
        ignore_duplicate_updates (bool, default=False): Don't perform an update if the row is
            a duplicate.
        return_untouched (bool, default=False): Return untouched rows by the operation"
"def isSnappedToGrid( self ):
        """"""
        Returns if both the x and y directions are snapping to the grid.
        
        :return     <bool>
        """"""
        shift = QtCore.QCoreApplication.keyboardModifiers() == QtCore.Qt.ShiftModifier
        return (self._xSnapToGrid and self._ySnapToGrid) or shift","Returns if both the x and y directions are snapping to the grid.
        
        :return     <bool>"
"def parse_uci(self, uci: str) -> Move:
        """"""
        Parses the given move in UCI notation.

        Supports both Chess960 and standard UCI notation.

        The returned move is guaranteed to be either legal or a null move.

        :raises: :exc:`ValueError` if the move is invalid or illegal in the
            current position (but not a null move).
        """"""
        move = Move.from_uci(uci)

        if not move:
            return move

        move = self._to_chess960(move)
        move = self._from_chess960(self.chess960, move.from_square, move.to_square, move.promotion, move.drop)

        if not self.is_legal(move):
            raise ValueError(""illegal uci: {!r} in {}"".format(uci, self.fen()))

        return move","Parses the given move in UCI notation.

        Supports both Chess960 and standard UCI notation.

        The returned move is guaranteed to be either legal or a null move.

        :raises: :exc:`ValueError` if the move is invalid or illegal in the
            current position (but not a null move)."
"def process_lists(self):
        """"""Do any preprocessing of the lists.""""""
        for l1_idx, obj1 in enumerate(self.l1):
            for l2_idx, obj2 in enumerate(self.l2):
                if self.equal(obj1, obj2):
                    self.matches.add((l1_idx, l2_idx))",Do any preprocessing of the lists.
"def minimum_bracketing(fct, initial_value=0.0, natural_length=1.0 + DOUBLE_TOL):
    '''
    Given a function func, and given distinct inital points ax and bx, this routine searches
    in the downhill direction (defined by the function as evaluated at the initial points)
    and returns new points at ax, bx, cx that bracket a minimum of the function.
    Also returned are the function values at the three points.
    See Press, et al. (1992) ""Numerical recipes in C"", 2nd ed., p.400.
    '''

    def _minimum_bracketing(a, b, fct):
        v = mn_brak(a, b, fct)
        fa = v[3]
        fb = v[4]
        fc = v[5]
        if not (fa > fb and fb < fc):
            return False
        # Return the three bracketing points.
        return (v[0], v[1], v[2])

    ta = initial_value if initial_value != None else 0.0
    tb = ta + natural_length
    ret = _minimum_bracketing(ta, tb, fct)
    return ret","Given a function func, and given distinct inital points ax and bx, this routine searches
    in the downhill direction (defined by the function as evaluated at the initial points)
    and returns new points at ax, bx, cx that bracket a minimum of the function.
    Also returned are the function values at the three points.
    See Press, et al. (1992) ""Numerical recipes in C"", 2nd ed., p.400."
"def maximization_step(num_words, stanzas, schemes, probs):
    """"""
    Update latent variables t_table, rprobs
    """"""
    t_table = numpy.zeros((num_words, num_words + 1))
    rprobs = numpy.ones(schemes.num_schemes)
    for i, stanza in enumerate(stanzas):
        scheme_indices = schemes.get_schemes_for_len(len(stanza))
        for scheme_index in scheme_indices:
            myprob = probs[i, scheme_index]
            rprobs[scheme_index] += myprob
            scheme = schemes.scheme_list[scheme_index]
            rhymelists = get_rhymelists(stanza, scheme)
            for rhymelist in rhymelists:
                for j, word_index in enumerate(rhymelist):
                    t_table[word_index, -1] += myprob
                    for word_index2 in rhymelist[:j] + rhymelist[j + 1:]:
                        t_table[word_index, word_index2] += myprob

    # Normalize t_table
    t_table_sums = numpy.sum(t_table, axis=0)
    for i, t_table_sum in enumerate(t_table_sums.tolist()):
        if t_table_sum != 0:
            t_table[:, i] /= t_table_sum

    # Normalize rprobs
    totrprob = numpy.sum(rprobs)
    rprobs /= totrprob

    return t_table, rprobs","Update latent variables t_table, rprobs"
"def p_case_block(self, p):
        """"""
        case_block \
            : LBRACE case_clauses_opt RBRACE
            | LBRACE case_clauses_opt default_clause case_clauses_opt RBRACE
        """"""
        statements = []
        for s in p[2:-1]:
            if isinstance(s, list):
                for i in s:
                    statements.append(i)
            elif isinstance(s, self.asttypes.Default):
                statements.append(s)
        p[0] = self.asttypes.CaseBlock(statements)
        p[0].setpos(p)","case_block \
            : LBRACE case_clauses_opt RBRACE
            | LBRACE case_clauses_opt default_clause case_clauses_opt RBRACE"
"def spht_slow(ssphere, nmax, mmax):
    """"""(PURE PYTHON) Transforms ScalarPatternUniform object *ssphere* 
    into a set of scalar spherical harmonics stored in ScalarCoefs.

    Example::

        >>> p = spherepy.random_patt_uniform(6, 8)
        >>> c = spherepy.spht(p)
        >>> spherepy.pretty_coefs(c)

    Args:
      ssphere (ScalarPatternUniform): The pattern to be transformed.

      nmax (int, optional): The maximum number of *n* values required. If a 
      value isn't passed, *nmax* is the number of rows in ssphere minus one.

      mmax (int, optional): The maximum number of *m* values required. If a 
      value isn't passed, *mmax* is half the number of columns in ssphere
      minus one.

    Returns:
      ScalarCoefs: The object containing the coefficients of the scalar
      spherical harmonic transform.


    Raises:
      ValueError: If *nmax* and *mmax* are too large or *mmax* > *nmax*.

    """"""

    if mmax > nmax:
        raise ValueError(err_msg['nmax_g_mmax'])

    nrows = ssphere._dsphere.shape[0]
    ncols = ssphere._dsphere.shape[1]

    if np.mod(nrows, 2) == 1 or np.mod(ncols, 2) == 1:
        raise ValueError(err_msg['ncols_even'])

    fdata = np.fft.fft2(ssphere._dsphere) / (nrows * ncols)
    ops.fix_even_row_data_fc(fdata)
    
    fdata_extended = np.zeros([nrows + 2, ncols], dtype=np.complex128)

    ops.pad_rows_fdata(fdata, fdata_extended)

    ops.sin_fc(fdata_extended)
    
    sc = pysphi.fc_to_sc(fdata_extended, nmax, mmax)
                                
    return ScalarCoefs(sc, nmax, mmax)","(PURE PYTHON) Transforms ScalarPatternUniform object *ssphere* 
    into a set of scalar spherical harmonics stored in ScalarCoefs.

    Example::

        >>> p = spherepy.random_patt_uniform(6, 8)
        >>> c = spherepy.spht(p)
        >>> spherepy.pretty_coefs(c)

    Args:
      ssphere (ScalarPatternUniform): The pattern to be transformed.

      nmax (int, optional): The maximum number of *n* values required. If a 
      value isn't passed, *nmax* is the number of rows in ssphere minus one.

      mmax (int, optional): The maximum number of *m* values required. If a 
      value isn't passed, *mmax* is half the number of columns in ssphere
      minus one.

    Returns:
      ScalarCoefs: The object containing the coefficients of the scalar
      spherical harmonic transform.


    Raises:
      ValueError: If *nmax* and *mmax* are too large or *mmax* > *nmax*."
"def match_concept(self,string):
        '''Find all matches in this :class:`Bottle` for ``string`` and return the best match'''
        matches = self.match_all_concepts(string)
        if len(matches)>0:
            return matches[0]
        return None",Find all matches in this :class:`Bottle` for ``string`` and return the best match
"def scan_uow_candidates(self):
        """""" method performs two actions:
            - enlist stale or invalid units of work into reprocessing queue
            - cancel UOWs that are older than 2 days and have been submitted more than 1 hour ago """"""
        try:
            since = settings.settings['synergy_start_timeperiod']
            uow_list = self.uow_dao.get_reprocessing_candidates(since)
        except LookupError as e:
            self.logger.info('flow: no UOW candidates found for reprocessing: {0}'.format(e))
            return

        for uow in uow_list:
            try:
                if uow.process_name not in self.managed_handlers:
                    self.logger.debug('process {0} is not known to the Synergy Scheduler. Skipping its UOW.'
                                      .format(uow.process_name))
                    continue

                thread_handler = self.managed_handlers[uow.process_name]
                assert isinstance(thread_handler, ManagedThreadHandler)
                if not thread_handler.process_entry.is_on:
                    self.logger.debug('process {0} is inactive. Skipping its UOW.'.format(uow.process_name))
                    continue

                entry = PriorityEntry(uow)
                if entry in self.reprocess_uows[uow.process_name]:
                    # given UOW is already registered in the reprocessing queue
                    continue

                # ASSUMPTION: UOW is re-created by a state machine during reprocessing
                # thus - any UOW older 2 days could be marked as STATE_CANCELED
                if datetime.utcnow() - uow.created_at > timedelta(hours=settings.settings['gc_life_support_hours']):
                    self._cancel_uow(uow)
                    continue

                # if the UOW has been idle for more than 1 hour - resubmit it
                if datetime.utcnow() - uow.submitted_at > timedelta(hours=settings.settings['gc_resubmit_after_hours'])\
                        or uow.is_invalid:
                    # enlist the UOW into the reprocessing queue
                    self.reprocess_uows[uow.process_name].put(entry)

            except Exception as e:
                self.logger.error('flow exception: {0}'.format(e), exc_info=True)","method performs two actions:
            - enlist stale or invalid units of work into reprocessing queue
            - cancel UOWs that are older than 2 days and have been submitted more than 1 hour ago"
"def load(self, filename):
        """"""Load a frequency list from file (in the format produced by the save method)""""""
        f = io.open(filename,'r',encoding='utf-8')
        for line in f:
            data = line.strip().split(""\t"")
            type, count = data[:2]
            self.count(type,count)
        f.close()",Load a frequency list from file (in the format produced by the save method)
"def reversible_deroot(self):
        """""" Stores info required to restore rootedness to derooted Tree. Returns
        the edge that was originally rooted, the length of e1, and the length
        of e2.

        Dendropy Derooting Process:
        In a rooted tree the root node is bifurcating. Derooting makes it
        trifurcating.

        Call the two edges leading out of the root node e1 and e2.
        Derooting with Tree.deroot() deletes one of e1 and e2 (let's say e2),
        and stretches the other to the sum of their lengths. Call this e3.

        Rooted tree:                   Derooted tree:
                 A                         A   B
                 |_ B                       \ /
                /                            |
               /e1                           |e3 (length = e1+e2; e2 is deleted)
        Root--o               ===>           |
               \e2                     Root--o _ C
                \ _ C                        |
                 |                           D
                 D

        Reverse this with Tree.reroot_at_edge(edge, length1, length2, ...)
        """"""
        root_edge = self._tree.seed_node.edge
        lengths = dict([(edge, edge.length) for edge
                        in self._tree.seed_node.incident_edges() if edge is not root_edge])
        self._tree.deroot()
        reroot_edge = (set(self._tree.seed_node.incident_edges())
                       & set(lengths.keys())).pop()
        self._tree.encode_bipartitions()
        self._dirty = True
        return (reroot_edge, reroot_edge.length - lengths[reroot_edge],
                lengths[reroot_edge])","Stores info required to restore rootedness to derooted Tree. Returns
        the edge that was originally rooted, the length of e1, and the length
        of e2.

        Dendropy Derooting Process:
        In a rooted tree the root node is bifurcating. Derooting makes it
        trifurcating.

        Call the two edges leading out of the root node e1 and e2.
        Derooting with Tree.deroot() deletes one of e1 and e2 (let's say e2),
        and stretches the other to the sum of their lengths. Call this e3.

        Rooted tree:                   Derooted tree:
                 A                         A   B
                 |_ B                       \ /
                /                            |
               /e1                           |e3 (length = e1+e2; e2 is deleted)
        Root--o               ===>           |
               \e2                     Root--o _ C
                \ _ C                        |
                 |                           D
                 D

        Reverse this with Tree.reroot_at_edge(edge, length1, length2, ...)"
"def process_directory(directory_name, lazy=False):
    """"""Processes a directory filled with CSXML files, first normalizing the
    character encodings to utf-8, and then processing into a list of INDRA
    statements.

    Parameters
    ----------
    directory_name : str
        The name of a directory filled with csxml files to process
    lazy : bool
        If True, the statements will not be generated immediately, but rather
        a generator will be formulated, and statements can be retrieved by
        using `iter_statements`. If False, the `statements` attribute will be
        populated immediately. Default is False.

    Returns
    -------
    mp : indra.sources.medscan.processor.MedscanProcessor
        A MedscanProcessor populated with INDRA statements extracted from the
        csxml files
    """"""

    # Parent Medscan processor containing extractions from all files
    mp = MedscanProcessor()
    mp.process_directory(directory_name, lazy)
    return mp","Processes a directory filled with CSXML files, first normalizing the
    character encodings to utf-8, and then processing into a list of INDRA
    statements.

    Parameters
    ----------
    directory_name : str
        The name of a directory filled with csxml files to process
    lazy : bool
        If True, the statements will not be generated immediately, but rather
        a generator will be formulated, and statements can be retrieved by
        using `iter_statements`. If False, the `statements` attribute will be
        populated immediately. Default is False.

    Returns
    -------
    mp : indra.sources.medscan.processor.MedscanProcessor
        A MedscanProcessor populated with INDRA statements extracted from the
        csxml files"
"def previous_unwrittable_on_col(view, coords):
    """"""Return position of the previous (in column) letter that is unwrittable""""""
    x, y = coords
    miny = -1
    for offset in range(y - 1, miny, -1):
        letter = view[x, offset]
        if letter not in REWRITABLE_LETTERS:
            return offset
    return None",Return position of the previous (in column) letter that is unwrittable
"def convert_datetime_array(array):
    ''' Convert NumPy datetime arrays to arrays to milliseconds since epoch.

    Args:
        array : (obj)
            A NumPy array of datetime to convert

            If the value passed in is not a NumPy array, it will be returned as-is.

    Returns:
        array

    '''

    if not isinstance(array, np.ndarray):
        return array

    try:
        dt2001 = np.datetime64('2001')
        legacy_datetime64 = (dt2001.astype('int64') ==
                             dt2001.astype('datetime64[ms]').astype('int64'))
    except AttributeError as e:
        if e.args == (""'module' object has no attribute 'datetime64'"",):
            # for compatibility with PyPy that doesn't have datetime64
            if 'PyPy' in sys.version:
                legacy_datetime64 = False
                pass
            else:
                raise e
        else:
            raise e

    # not quite correct, truncates to ms..
    if array.dtype.kind == 'M':
        if legacy_datetime64:
            if array.dtype == np.dtype('datetime64[ns]'):
                array = array.astype('int64') / 10**6.0
        else:
            array =  array.astype('datetime64[us]').astype('int64') / 1000.

    elif array.dtype.kind == 'm':
        array = array.astype('timedelta64[us]').astype('int64') / 1000.

    return array","Convert NumPy datetime arrays to arrays to milliseconds since epoch.

    Args:
        array : (obj)
            A NumPy array of datetime to convert

            If the value passed in is not a NumPy array, it will be returned as-is.

    Returns:
        array"
"def _handle_text(self, text: str):
        """"""
        Check to see if text is a command. Otherwise, check to see if the
        second word is a command.

        Commands get handeled by the `_handle_command` method

        If not command, check to see if first word is a service or an author.
        Default program is to send message replies. However, we will also
        check to see if the second word is a command and handle approparitly

        This method does simple string parsing and high level program control
        """"""
        # If first word is command
        if self.is_command(text):
            self._logger.debug(' first word is a command')
            # get the command, args, and kwargs out using `shlex`
            command, args, kwargs = _get_cmd_args_kwargs(text)
            self._logger.info(' command: %s, %s %s', command, args, kwargs)
            # hand off to the `handle_command` method
            result = self._handle_command(command, args, kwargs)

            if result:
                if isinstance(result, str):
                    print(result)
                else:
                    _pprint.pprint(result)
            # Exit the method here if in this block
            return
        # Else first word is not a command
        else:
            self._logger.debug(' first word is not a command')
            # get the first word and then the rest of the text.
            try:
                first_word, second_word = text.split(' ', 1)
                self._logger.debug(' first word: %s', first_word)
            except ValueError:
                self._logger.debug('No second word in chain!')
                return self._handle_NLP(text)
            # check if second word/string is a command
            if self.is_command(second_word):
                self._logger.info(' second word is a command')
                # get the command, args, and kwargs out using `shlex`
                command, args, kwargs = _get_cmd_args_kwargs(second_word)
                self._logger.debug(' second word: %s', command)
                self._logger.debug(' command %s', command)
                self._logger.debug('args %s ', args)
                self._logger.debug('kwargs %s', kwargs)
                return self._first_word_not_cmd(first_word, command, args, kwargs)
            # if second word is not a command, default to NLP
            else:
                self._logger.info(' defaulting to message since second word '
                                  'isn\'t a command')

                return self._handle_NLP(text)","Check to see if text is a command. Otherwise, check to see if the
        second word is a command.

        Commands get handeled by the `_handle_command` method

        If not command, check to see if first word is a service or an author.
        Default program is to send message replies. However, we will also
        check to see if the second word is a command and handle approparitly

        This method does simple string parsing and high level program control"
"def setexternalfile(self, filename, offset=0):
        """"""Store the dataset data in an external file.

        Args::

          filename    external file name
          offset      offset in bytes where to start writing in
                      the external file

        Returns::

            None

        C library equivalent : SDsetexternalfile
                                                  """"""

        status = _C.SDsetexternalfile(self._id, filename, offset)
        _checkErr('setexternalfile', status, 'execution error')","Store the dataset data in an external file.

        Args::

          filename    external file name
          offset      offset in bytes where to start writing in
                      the external file

        Returns::

            None

        C library equivalent : SDsetexternalfile"
"def complex_el_from_dict(parent, data, key):
    """"""Create element from a dict definition and add it to ``parent``.

    :param parent: parent element
    :type parent: Element
    :param data: dictionary with elements definitions, it can be a simple \
    {element_name: 'element_value'} or complex \
    {element_name: {_attr: {name: value, name1: value1}, _text: 'text'}}
    :param key: element name and key in ``data``
    :return: created element
    """"""
    el = ET.SubElement(parent, key)
    value = data[key]

    if isinstance(value, dict):
        if '_attr' in value:
            for a_name, a_value in viewitems(value['_attr']):
                el.set(a_name, a_value)

        if '_text' in value:
            el.text = value['_text']

    else:
        el.text = value

    return el","Create element from a dict definition and add it to ``parent``.

    :param parent: parent element
    :type parent: Element
    :param data: dictionary with elements definitions, it can be a simple \
    {element_name: 'element_value'} or complex \
    {element_name: {_attr: {name: value, name1: value1}, _text: 'text'}}
    :param key: element name and key in ``data``
    :return: created element"
"def build_query(self,
                    product=None,
                    component=None,
                    version=None,
                    long_desc=None,
                    bug_id=None,
                    short_desc=None,
                    cc=None,
                    assigned_to=None,
                    reporter=None,
                    qa_contact=None,
                    status=None,
                    blocked=None,
                    dependson=None,
                    keywords=None,
                    keywords_type=None,
                    url=None,
                    url_type=None,
                    status_whiteboard=None,
                    status_whiteboard_type=None,
                    fixed_in=None,
                    fixed_in_type=None,
                    flag=None,
                    alias=None,
                    qa_whiteboard=None,
                    devel_whiteboard=None,
                    boolean_query=None,
                    bug_severity=None,
                    priority=None,
                    target_release=None,
                    target_milestone=None,
                    emailtype=None,
                    booleantype=None,
                    include_fields=None,
                    quicksearch=None,
                    savedsearch=None,
                    savedsearch_sharer_id=None,
                    sub_component=None,
                    tags=None,
                    exclude_fields=None,
                    extra_fields=None):
        """"""
        Build a query string from passed arguments. Will handle
        query parameter differences between various bugzilla versions.

        Most of the parameters should be self-explanatory. However,
        if you want to perform a complex query, and easy way is to
        create it with the bugzilla web UI, copy the entire URL it
        generates, and pass it to the static method

        Bugzilla.url_to_query

        Then pass the output to Bugzilla.query()

        For details about the specific argument formats, see the bugzilla docs:
        https://bugzilla.readthedocs.io/en/latest/api/core/v1/bug.html#search-bugs
        """"""
        if boolean_query or booleantype:
            raise RuntimeError(""boolean_query format is no longer supported. ""
                ""If you need complicated URL queries, look into ""
                ""query --from-url/url_to_query()."")

        query = {
            ""alias"": alias,
            ""product"": self._listify(product),
            ""component"": self._listify(component),
            ""version"": version,
            ""id"": bug_id,
            ""short_desc"": short_desc,
            ""bug_status"": status,
            ""bug_severity"": bug_severity,
            ""priority"": priority,
            ""target_release"": target_release,
            ""target_milestone"": target_milestone,
            ""tag"": self._listify(tags),
            ""quicksearch"": quicksearch,
            ""savedsearch"": savedsearch,
            ""sharer_id"": savedsearch_sharer_id,

            # RH extensions... don't add any more. See comment below
            ""sub_components"": self._listify(sub_component),
        }

        def add_bool(bzkey, value, bool_id, booltype=None):
            value = self._listify(value)
            if value is None:
                return bool_id

            query[""query_format""] = ""advanced""
            for boolval in value:
                def make_bool_str(prefix):
                    # pylint: disable=cell-var-from-loop
                    return ""%s%i-0-0"" % (prefix, bool_id)

                query[make_bool_str(""field"")] = bzkey
                query[make_bool_str(""value"")] = boolval
                query[make_bool_str(""type"")] = booltype or ""substring""

                bool_id += 1
            return bool_id

        # RH extensions that we have to maintain here for back compat,
        # but all future custom fields should be specified via
        # cli --field option, or via extending the query dict() manually.
        # No more supporting custom fields in this API
        bool_id = 0
        bool_id = add_bool(""keywords"", keywords, bool_id, keywords_type)
        bool_id = add_bool(""blocked"", blocked, bool_id)
        bool_id = add_bool(""dependson"", dependson, bool_id)
        bool_id = add_bool(""bug_file_loc"", url, bool_id, url_type)
        bool_id = add_bool(""cf_fixed_in"", fixed_in, bool_id, fixed_in_type)
        bool_id = add_bool(""flagtypes.name"", flag, bool_id)
        bool_id = add_bool(""status_whiteboard"",
                           status_whiteboard, bool_id, status_whiteboard_type)
        bool_id = add_bool(""cf_qa_whiteboard"", qa_whiteboard, bool_id)
        bool_id = add_bool(""cf_devel_whiteboard"", devel_whiteboard, bool_id)

        def add_email(key, value, count):
            if value is None:
                return count
            if not emailtype:
                query[key] = value
                return count

            query[""query_format""] = ""advanced""
            query['email%i' % count] = value
            query['email%s%i' % (key, count)] = True
            query['emailtype%i' % count] = emailtype
            return count + 1

        email_count = 1
        email_count = add_email(""cc"", cc, email_count)
        email_count = add_email(""assigned_to"", assigned_to, email_count)
        email_count = add_email(""reporter"", reporter, email_count)
        email_count = add_email(""qa_contact"", qa_contact, email_count)

        if long_desc is not None:
            query[""query_format""] = ""advanced""
            query[""longdesc""] = long_desc
            query[""longdesc_type""] = ""allwordssubstr""

        # 'include_fields' only available for Bugzilla4+
        # 'extra_fields' is an RHBZ extension
        query.update(self._process_include_fields(
            include_fields, exclude_fields, extra_fields))

        # Strip out None elements in the dict
        for k, v in query.copy().items():
            if v is None:
                del(query[k])

        self.pre_translation(query)
        return query","Build a query string from passed arguments. Will handle
        query parameter differences between various bugzilla versions.

        Most of the parameters should be self-explanatory. However,
        if you want to perform a complex query, and easy way is to
        create it with the bugzilla web UI, copy the entire URL it
        generates, and pass it to the static method

        Bugzilla.url_to_query

        Then pass the output to Bugzilla.query()

        For details about the specific argument formats, see the bugzilla docs:
        https://bugzilla.readthedocs.io/en/latest/api/core/v1/bug.html#search-bugs"
"def edit(self, layer, item, delete=False):
        """"""
        Edit model.

        :param layer: Layer of model to edit
        :type layer: str
        :param item: Items to edit.
        :type item: dict
        :param delete: Flag to return
            :class:`~simkit.core.layers.Layer` to delete item.
        :type delete: bool
        """"""
        # get layer attribute with model data
        if hasattr(self, layer):
            layer_obj = getattr(self, layer)
        else:
            raise AttributeError('missing layer: %s', layer)
        if delete:
            return layer_obj
        # iterate over items and edit layer
        for k, v in item.iteritems():
            if k in layer_obj.layer:
                layer_obj.edit(k, v)  # edit layer
            else:
                raise AttributeError('missing layer item: %s', k)
            # update model data
            if k in self.model[layer]:
                self.model[layer][k].update(v)
            else:
                raise AttributeError('missing model layer item: %s', k)","Edit model.

        :param layer: Layer of model to edit
        :type layer: str
        :param item: Items to edit.
        :type item: dict
        :param delete: Flag to return
            :class:`~simkit.core.layers.Layer` to delete item.
        :type delete: bool"
"def create_from_json(cls, json_data):
        """"""Deserialize msa json data into a Msa object

        Args:
            json_data (dict): The json data for this msa

        Returns:
            Msa object

        """"""
        msa = Msa()
        msa.msa = json_data[""msa_info""][""msa""]
        msa.meta = json_data[""meta""] if ""meta"" in json_data else None

        msa.component_results = _create_component_results(json_data, ""msa_info"")

        return msa","Deserialize msa json data into a Msa object

        Args:
            json_data (dict): The json data for this msa

        Returns:
            Msa object"
"def magic_help(self, line):
        """"""
        Print out the help for magics

        Usage:
        Call help with no arguments to list all magics,
        or call it with a magic to print out it's help info.

        `%help`
        or
        `%help run
        """"""
        line = line.strip()
        if not line:
            for magic in self.magics:
                stream_content = {'name': 'stdout', 'text': ""%{}\n"".format(magic)}
                self.send_response(self.iopub_socket, 'stream', stream_content)
        elif line in self.magics:
            # its a magic
            stream_content = {'name': 'stdout', 'text': ""{}\n{}"".format(line, self.magics[line].__doc__)}
            self.send_response(self.iopub_socket, 'stream', stream_content)
        elif line in self.interpreter.ops:
            # it's an instruction
            stream_content = {'name': 'stdout', 'text': ""{}\n{}"".format(line, self.interpreter.ops[line].__doc__)}
            self.send_response(self.iopub_socket, 'stream', stream_content)
        else:
            stream_content = {'name': 'stderr', 'text': ""'{}' not a known magic or instruction"".format(line)}
            self.send_response(self.iopub_socket, 'stream', stream_content)","Print out the help for magics

        Usage:
        Call help with no arguments to list all magics,
        or call it with a magic to print out it's help info.

        `%help`
        or
        `%help run"
"def require_auth(function):
    """"""
    A decorator that wraps the passed in function and raises exception
    if access token is missing
    """"""
    @functools.wraps(function)
    def wrapper(self, *args, **kwargs):
        if not self.access_token():
            raise MissingAccessTokenError
        return function(self, *args, **kwargs)
    return wrapper","A decorator that wraps the passed in function and raises exception
    if access token is missing"
"def caom2(mpc_filename, search_date=""2014 07 24.0""):
    """"""
    builds a TSV file in the format of SSOIS by querying for possilbe observations in CADC/CAOM2.

    This is a fall back program, should only be useful when SSOIS is behind.
    """"""
    columns = ('Image',
               'Ext',
               'X',
                'Y',
                'MJD',
                'Filter',
                'Exptime',
                'Object_RA',
                'Object_Dec',
                'Image_target',
                'Telescope/Instrument',
                'MetaData',
                'Datalink')

    ephem_table = Table(names=columns,
                         dtypes=('S10', 'i4', 'f8', 'f8',
                                 'f8', 'S10', 'f8', 'f8', 'f8', 'S20', 'S20', 'S20', 'S50'))

    ephem_table.pprint()

    o = orbfit.Orbfit(mpc.MPCReader(mpc_filename).mpc_observations)
    o.predict(search_date)
    fields = storage.cone_search(o.coordinate.ra.degrees, o.coordinate.dec.degrees, dra=0.3, ddec=0.3,
                                 calibration_level=1)
    mjdates = numpy.unique(fields['mjdate'])

    collectionIDs = []
    for mjdate in mjdates:
        jd = 2400000.5 + mjdate
        o.predict(jd)
        for field in storage.cone_search(o.coordinate.ra.degrees, o.coordinate.dec.degrees,
                                         dra=30./3600.0, ddec=30./3600.0,
                                         mjdate=mjdate, calibration_level=1):
            collectionIDs.append(field['collectionID'])

    expnums = numpy.unique(numpy.array(collectionIDs))

    for expnum in expnums:
        header = storage.get_astheader(expnum, 22)
        o.predict(header['MJDATE']+2400000.5)
        print o.time.iso, o.coordinate.ra.degrees, o.coordinate.dec.degrees
        for ccd in range(36):
            header = storage.get_astheader(expnum, ccd)
            w = wcs.WCS(header)
            (x, y) = w.sky2xy(o.coordinate.ra.degrees, o.coordinate.dec.degrees)
            print ccd, x, y
            if 0 < x < header['NAXIS1'] and 0 < y < header['NAXIS2']:
                ephem_table.add_row([expnum, ccd+1, x, y,
                                     header['MJDATE'], header['FILTER'], header['EXPTIME'],
                                     o.coordinate.ra.degrees, o.coordinate.dec.degrees,
                                     header['OBJECT'],
                                     'CFHT/MegaCam',
                                     None,
                                     ""http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/data/pub/CFHT/{}p[{}]"".format(expnum, ccd)])
                break

    ephem_table.pprint()
    ephem_table.write('backdoor.tsv', format='ascii', delimiter='\t')","builds a TSV file in the format of SSOIS by querying for possilbe observations in CADC/CAOM2.

    This is a fall back program, should only be useful when SSOIS is behind."
"def SelectGlyph(aFont, message=""Select a glyph:"", title='FontParts'):
    """"""
    Select a glyph for a given font.
    Optionally a `message` and `title` can be provided.

    ::

        from fontParts.ui import SelectGlyph
        font = CurrentFont()
        glyph = SelectGlyph(font)
        print(glyph)

    """"""
    return dispatcher[""SelectGlyph""](aFont=aFont, message=message, title=title)","Select a glyph for a given font.
    Optionally a `message` and `title` can be provided.

    ::

        from fontParts.ui import SelectGlyph
        font = CurrentFont()
        glyph = SelectGlyph(font)
        print(glyph)"
"def _match(string, pattern):
    """""" Returns True if the pattern matches the given word string.
        The pattern can include a wildcard (*front, back*, *both*, in*side),
        or it can be a compiled regular expression.
    """"""
    p = pattern
    try:
        if p[:1] == WILDCARD and (p[-1:] == WILDCARD and p[1:-1] in string or string.endswith(p[1:])):
            return True
        if p[-1:] == WILDCARD and not p[-2:-1] == ""\\"" and string.startswith(p[:-1]):
            return True
        if p == string:
            return True
        if WILDCARD in p[1:-1]:
            p = p.split(WILDCARD)
            return string.startswith(p[0]) and string.endswith(p[-1])
    except:
        # For performance, calling isinstance() last is 10% faster for plain strings.
        if isinstance(p, regexp):
            return p.search(string) is not None
    return False","Returns True if the pattern matches the given word string.
        The pattern can include a wildcard (*front, back*, *both*, in*side),
        or it can be a compiled regular expression."
"def change_nick(self, nick):
        """"""Update this user's nick in all joined channels.""""""
        
        old_nick = self.nick
        self.nick = IRCstr(nick)
        
        for c in self.channels:
            c.users.remove(old_nick)
            c.users.add(self.nick)",Update this user's nick in all joined channels.
"def draw(self, x, y, char=None, colour=7, bg=0, thin=False):
        """"""
        Draw a line from drawing cursor to the specified position.

        This uses a modified Bressenham algorithm, interpolating twice as many points to
        render down to anti-aliased characters when no character is specified,
        or uses standard algorithm plotting with the specified character.

        :param x: The column (x coord) for the location to check.
        :param y: The line (y coord) for the location to check.
        :param char: Optional character to use to draw the line.
        :param colour: Optional colour for plotting the line.
        :param bg: Optional background colour for plotting the line.
        :param thin: Optional width of anti-aliased line.
        """"""
        # Decide what type of line drawing to use.
        line_chars = (self._uni_line_chars if self._unicode_aware else
                      self._line_chars)

        # Define line end points.
        x0 = self._x
        y0 = self._y
        x1 = int(round(x * 2, 0))
        y1 = int(round(y * 2, 0))

        # Remember last point for next line.
        self._x = x1
        self._y = y1

        # Don't bother drawing anything if we're guaranteed to be off-screen
        if ((x0 < 0 and x1 < 0) or (x0 >= self.width * 2 and x1 >= self.width * 2) or
                (y0 < 0 and y1 < 0) or (y0 >= self.height * 2 and y1 >= self.height * 2)):
            return

        dx = abs(x1 - x0)
        dy = abs(y1 - y0)

        sx = -1 if x0 > x1 else 1
        sy = -1 if y0 > y1 else 1

        def _get_start_char(cx, cy):
            needle = self.get_from(cx, cy)
            if needle is not None:
                letter, cfg, _, cbg = needle
                if colour == cfg and bg == cbg and chr(letter) in line_chars:
                    return line_chars.find(chr(letter))
            return 0

        def _fast_fill(start_x, end_x, iy):
            next_char = -1
            for ix in range(start_x, end_x):
                if ix % 2 == 0 or next_char == -1:
                    next_char = _get_start_char(ix // 2, iy // 2)
                next_char |= 2 ** abs(ix % 2) * 4 ** (iy % 2)
                if ix % 2 == 1:
                    self.print_at(line_chars[next_char], ix // 2, iy // 2, colour, bg=bg)
            if end_x % 2 == 1:
                self.print_at(line_chars[next_char], end_x // 2, iy // 2, colour, bg=bg)

        def _draw_on_x(ix, iy):
            err = dx
            px = ix - 2
            py = iy - 2
            next_char = 0
            while ix != x1:
                if ix < px or ix - px >= 2 or iy < py or iy - py >= 2:
                    px = ix & ~1
                    py = iy & ~1
                    next_char = _get_start_char(px // 2, py // 2)
                next_char |= 2 ** abs(ix % 2) * 4 ** (iy % 2)
                err -= 2 * dy
                if err < 0:
                    iy += sy
                    err += 2 * dx
                ix += sx

                if char is None:
                    self.print_at(line_chars[next_char],
                                  px // 2, py // 2, colour, bg=bg)
                else:
                    self.print_at(char, px // 2, py // 2, colour, bg=bg)

        def _draw_on_y(ix, iy):
            err = dy
            px = ix - 2
            py = iy - 2
            next_char = 0
            while iy != y1:
                if ix < px or ix - px >= 2 or iy < py or iy - py >= 2:
                    px = ix & ~1
                    py = iy & ~1
                    next_char = _get_start_char(px // 2, py // 2)
                next_char |= 2 ** abs(ix % 2) * 4 ** (iy % 2)
                err -= 2 * dx
                if err < 0:
                    ix += sx
                    err += 2 * dy
                iy += sy

                if char is None:
                    self.print_at(line_chars[next_char],
                                  px // 2, py // 2, colour, bg=bg)
                else:
                    self.print_at(char, px // 2, py // 2, colour, bg=bg)

        if dy == 0 and thin and char is None:
            # Fast-path for polygon filling
            _fast_fill(min(x0, x1), max(x0, x1), y0)
        elif dx > dy:
            _draw_on_x(x0, y0)
            if not thin:
                _draw_on_x(x0, y0 + 1)
        else:
            _draw_on_y(x0, y0)
            if not thin:
                _draw_on_y(x0 + 1, y0)","Draw a line from drawing cursor to the specified position.

        This uses a modified Bressenham algorithm, interpolating twice as many points to
        render down to anti-aliased characters when no character is specified,
        or uses standard algorithm plotting with the specified character.

        :param x: The column (x coord) for the location to check.
        :param y: The line (y coord) for the location to check.
        :param char: Optional character to use to draw the line.
        :param colour: Optional colour for plotting the line.
        :param bg: Optional background colour for plotting the line.
        :param thin: Optional width of anti-aliased line."
"def set_time(self, time):
        '''
        Converts time data into a pandas date object.

        Parameters
        ----------
        time: netcdf
            Contains time information.

        Returns
        -------
        pandas.DatetimeIndex
        '''
        times = num2date(time[:].squeeze(), time.units)
        self.time = pd.DatetimeIndex(pd.Series(times), tz=self.location.tz)","Converts time data into a pandas date object.

        Parameters
        ----------
        time: netcdf
            Contains time information.

        Returns
        -------
        pandas.DatetimeIndex"
"def assert_shape_match(shape1, shape2):
  """"""Ensure the shape1 match the pattern given by shape2.

  Ex:
    assert_shape_match((64, 64, 3), (None, None, 3))

  Args:
    shape1 (tuple): Static shape
    shape2 (tuple): Dynamic shape (can contain None)
  """"""
  shape1 = tf.TensorShape(shape1)
  shape2 = tf.TensorShape(shape2)
  if shape1.ndims is None or shape2.ndims is None:
    raise ValueError('Shapes must have known rank. Got %s and %s.' %
                     (shape1.ndims, shape2.ndims))
  shape1.assert_same_rank(shape2)
  shape1.assert_is_compatible_with(shape2)","Ensure the shape1 match the pattern given by shape2.

  Ex:
    assert_shape_match((64, 64, 3), (None, None, 3))

  Args:
    shape1 (tuple): Static shape
    shape2 (tuple): Dynamic shape (can contain None)"
"def reindex(args):
    """"""
    %prog agpfile

    assume the component line order is correct, modify coordinates, this is
    necessary mostly due to manual edits (insert/delete) that disrupts
    the target coordinates.
    """"""
    p = OptionParser(reindex.__doc__)
    p.add_option(""--nogaps"", default=False, action=""store_true"",
                 help=""Remove all gap lines [default: %default]"")
    p.add_option(""--inplace"", default=False, action=""store_true"",
                 help=""Replace input file [default: %default]"")
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(p.print_help())

    agpfile, = args
    inplace = opts.inplace
    agp = AGP(agpfile, validate=False)
    pf = agpfile.rsplit(""."", 1)[0]
    newagpfile = pf + "".reindexed.agp""

    fw = open(newagpfile, ""w"")
    agp.transfer_header(fw)
    for chr, chr_agp in groupby(agp, lambda x: x.object):
        chr_agp = list(chr_agp)
        object_beg = 1
        for i, b in enumerate(chr_agp):
            b.object_beg = object_beg
            b.part_number = i + 1
            if opts.nogaps and b.is_gap:
                continue

            if b.is_gap:
                b.object_end = object_beg + b.gap_length - 1
            else:
                b.object_end = object_beg + b.component_span - 1

            object_beg = b.object_end + 1

            print(str(b), file=fw)

    # Last step: validate the new agpfile
    fw.close()
    agp = AGP(newagpfile, validate=True)

    if inplace:
        shutil.move(newagpfile, agpfile)
        logging.debug(""Rename file `{0}` to `{1}`"".format(newagpfile, agpfile))
        newagpfile = agpfile

    return newagpfile","%prog agpfile

    assume the component line order is correct, modify coordinates, this is
    necessary mostly due to manual edits (insert/delete) that disrupts
    the target coordinates."
"def compat_kwargs(kwargs):
    """"""To keep backwards compat change the kwargs to new names""""""
    warn_deprecations(kwargs)
    for old, new in RENAMED_VARS.items():
        if old in kwargs:
            kwargs[new] = kwargs[old]
            # update cross references
            for c_old, c_new in RENAMED_VARS.items():
                if c_new == new:
                    kwargs[c_old] = kwargs[new]",To keep backwards compat change the kwargs to new names
"def _already_in_album(self,fullfile,pid,album_id):
        """"""Check to see if photo with given pid is already
        in the album_id, returns true if this is the case
        """"""

        logger.debug(""fb: Checking if pid %s in album %s"",pid,album_id)
        pid_in_album=[]
        # Get all photos in album
        photos = self.fb.get_connections(str(album_id),""photos"")['data']

        # Get all pids in fb album
        for photo in photos:
            pid_in_album.append(photo['id'])

        logger.debug(""fb: album %d contains these photos: %s"",album_id,pid_in_album)

        # Check if our pid matches
        if pid in pid_in_album:
            return True

        return False","Check to see if photo with given pid is already
        in the album_id, returns true if this is the case"
"def touch(self, mode=0o666, exist_ok=True):
        """"""
        Create a file if it doesn't exist.
        Mode is ignored by Artifactory.
        """"""
        if self.exists() and not exist_ok:
            raise OSError(17, ""File exists"", str(self))

        self._accessor.touch(self)","Create a file if it doesn't exist.
        Mode is ignored by Artifactory."
"def extract_src_loss_table(dstore, loss_type):
    """"""
    Extract the source loss table for a give loss type, ordered in decreasing
    order. Example:
    http://127.0.0.1:8800/v1/calc/30/extract/src_loss_table/structural
    """"""
    oq = dstore['oqparam']
    li = oq.lti[loss_type]
    source_ids = dstore['source_info']['source_id']
    idxs = dstore['ruptures'].value[['srcidx', 'grp_id']]
    losses = dstore['rup_loss_table'][:, li]
    slt = numpy.zeros(len(source_ids), [('grp_id', U32), (loss_type, F32)])
    for loss, (srcidx, grp_id) in zip(losses, idxs):
        slt[srcidx][loss_type] += loss
        slt[srcidx]['grp_id'] = grp_id
    slt = util.compose_arrays(source_ids, slt, 'source_id')
    slt.sort(order=loss_type)
    return slt[::-1]","Extract the source loss table for a give loss type, ordered in decreasing
    order. Example:
    http://127.0.0.1:8800/v1/calc/30/extract/src_loss_table/structural"
"def remove_disk_encryption_password(self, id_p):
        """"""Removes a password used for hard disk encryption/decryption from
        the running VM. As soon as the medium requiring this password
        is accessed the VM is paused with an error and the password must be
        provided again.

        in id_p of type str
            The identifier used for the password. Must match the identifier
            used when the encrypted medium was created.

        """"""
        if not isinstance(id_p, basestring):
            raise TypeError(""id_p can only be an instance of type basestring"")
        self._call(""removeDiskEncryptionPassword"",
                     in_p=[id_p])","Removes a password used for hard disk encryption/decryption from
        the running VM. As soon as the medium requiring this password
        is accessed the VM is paused with an error and the password must be
        provided again.

        in id_p of type str
            The identifier used for the password. Must match the identifier
            used when the encrypted medium was created."
"def _get_weights(max_length):
  """"""Get weights for each offset in str of certain max length.

  Args:
    max_length: max length of the strings.

  Returns:
    A list of ints as weights.

  Example:
    If max_length is 2 and alphabet is ""ab"", then we have order """", ""a"", ""aa"",
  ""ab"", ""b"", ""ba"", ""bb"". So the weight for the first char is 3.
  """"""
  weights = [1]
  for i in range(1, max_length):
    weights.append(weights[i-1] * len(_ALPHABET) + 1)
  weights.reverse()
  return weights","Get weights for each offset in str of certain max length.

  Args:
    max_length: max length of the strings.

  Returns:
    A list of ints as weights.

  Example:
    If max_length is 2 and alphabet is ""ab"", then we have order """", ""a"", ""aa"",
  ""ab"", ""b"", ""ba"", ""bb"". So the weight for the first char is 3."
"def syncBuffer( self ):
        """"""
        I detect and correct corruption in the buffer.
        
        Corruption in the buffer is defined as the following conditions
        both being true:
        
            1. The buffer contains at least one newline;
            2. The text until the first newline is not a STOMP command.
        
        In this case, we heuristically try to flush bits of the buffer until
        one of the following conditions becomes true:
        
            1. the buffer starts with a STOMP command;
            2. the buffer does not contain a newline.
            3. the buffer is empty;

        If the buffer is deemed corrupt, the first step is to flush the buffer
        up to and including the first occurrence of the string '\x00\n', which
        is likely to be a frame boundary.

        Note that this is not guaranteed to be a frame boundary, as a binary
        payload could contain the string '\x00\n'. That condition would get
        handled on the next loop iteration.
        
        If the string '\x00\n' does not occur, the entire buffer is cleared.
        An earlier version progressively removed strings until the next newline,
        but this gets complicated because the body could contain strings that
        look like STOMP commands.
        
        Note that we do not check ""partial"" strings to see if they *could*
        match a command; that would be too resource-intensive. In other words,
        a buffer containing the string 'BUNK' with no newline is clearly
        corrupt, but we sit and wait until the buffer contains a newline before
        attempting to see if it's a STOMP command.
        """"""
        while True:
            if not self.buffer:
                # Buffer is empty; no need to do anything.
                break
            m = command_re.match ( self.buffer )
            if m is None:
                # Buffer doesn't even contain a single newline, so we can't
                # determine whether it's corrupt or not. Assume it's OK.
                break
            cmd = m.groups()[0]
            if cmd in stomper.VALID_COMMANDS:
                # Good: the buffer starts with a command.
                break
            else:
                # Bad: the buffer starts with bunk, so strip it out. We first
                # try to strip to the first occurrence of '\x00\n', which
                # is likely to be a frame boundary, but if this fails, we
                # strip until the first newline.
                ( self.buffer, nsubs ) = sync_re.subn ( '', self.buffer )

                if nsubs:
                    # Good: we managed to strip something out, so restart the
                    # loop to see if things look better.
                    continue
                else:
                    # Bad: we failed to strip anything out, so kill the
                    # entire buffer. Since this resets the buffer to a
                    # known good state, we can break out of the loop.
                    self.buffer = ''
                    break","I detect and correct corruption in the buffer.
        
        Corruption in the buffer is defined as the following conditions
        both being true:
        
            1. The buffer contains at least one newline;
            2. The text until the first newline is not a STOMP command.
        
        In this case, we heuristically try to flush bits of the buffer until
        one of the following conditions becomes true:
        
            1. the buffer starts with a STOMP command;
            2. the buffer does not contain a newline.
            3. the buffer is empty;

        If the buffer is deemed corrupt, the first step is to flush the buffer
        up to and including the first occurrence of the string '\x00\n', which
        is likely to be a frame boundary.

        Note that this is not guaranteed to be a frame boundary, as a binary
        payload could contain the string '\x00\n'. That condition would get
        handled on the next loop iteration.
        
        If the string '\x00\n' does not occur, the entire buffer is cleared.
        An earlier version progressively removed strings until the next newline,
        but this gets complicated because the body could contain strings that
        look like STOMP commands.
        
        Note that we do not check ""partial"" strings to see if they *could*
        match a command; that would be too resource-intensive. In other words,
        a buffer containing the string 'BUNK' with no newline is clearly
        corrupt, but we sit and wait until the buffer contains a newline before
        attempting to see if it's a STOMP command."
"def get_stars(self):
        """"""
        get 'component' of all stars in order primary -> secondary
        """"""
        l = re.findall(r""[\w']+"", self.get_value())
        # now search for indices of star and take the next entry from this flat list
        return [l[i+1] for i,s in enumerate(l) if s=='star']",get 'component' of all stars in order primary -> secondary
"def verify_headers(self, check_presence=True, **kwargs):
        """"""
        Check that a set of particular header claim are present and has
        specific values

        :param kwargs: The claim/value sets as a dictionary
        :return: True if the claim that appears in the header has the
            prescribed values. If a claim is not present in the header and
            check_presence is True then False is returned.
        """"""
        for key, val in kwargs.items():
            try:
                _ok = self.verify_header(key, val)
            except KeyError:
                if check_presence:
                    return False
                else:
                    pass
            else:
                if not _ok:
                    return False
        return True","Check that a set of particular header claim are present and has
        specific values

        :param kwargs: The claim/value sets as a dictionary
        :return: True if the claim that appears in the header has the
            prescribed values. If a claim is not present in the header and
            check_presence is True then False is returned."
"def hedonic_value(self, novelty):
        '''Given the agent's desired novelty, how good the novelty value is.

        Not used if *desired_novelty*=-1
        '''
        lmax = gaus_pdf(self.desired_novelty, self.desired_novelty, 4)
        pdf = gaus_pdf(novelty, self.desired_novelty, 4)
        return pdf / lmax","Given the agent's desired novelty, how good the novelty value is.

        Not used if *desired_novelty*=-1"
"def dssps(self):
        """""" Dict of filepaths for all dssp files associated with code.

        Notes
        -----
        Runs dssp and stores writes output to files if not already present.
        Also downloads mmol files if not already present.
        Calls isambard.external_programs.dssp and so needs dssp to be installed.

        Returns
        -------
        dssps_dict : dict, or None.
            Keys : int
                mmol number
            Values : str
                Filepath for the corresponding dssp file.

        Raises
        ------
        Warning
            If any of the dssp files are empty.
        """"""
        dssps_dict = {}
        dssp_dir = os.path.join(self.parent_dir, 'dssp')
        if not os.path.exists(dssp_dir):
            os.makedirs(dssp_dir)
        for i, mmol_file in self.mmols.items():
            dssp_file_name = '{0}.dssp'.format(os.path.basename(mmol_file))
            dssp_file = os.path.join(dssp_dir, dssp_file_name)
            if not os.path.exists(dssp_file):
                dssp_out = run_dssp(pdb=mmol_file, path=True, outfile=dssp_file)
                if len(dssp_out) == 0:
                    raise Warning(""dssp file {0} is empty"".format(dssp_file))
            dssps_dict[i] = dssp_file
        return dssps_dict","Dict of filepaths for all dssp files associated with code.

        Notes
        -----
        Runs dssp and stores writes output to files if not already present.
        Also downloads mmol files if not already present.
        Calls isambard.external_programs.dssp and so needs dssp to be installed.

        Returns
        -------
        dssps_dict : dict, or None.
            Keys : int
                mmol number
            Values : str
                Filepath for the corresponding dssp file.

        Raises
        ------
        Warning
            If any of the dssp files are empty."
"def get_rank(self, entity, criteria, condition = True):
        """"""
        Get the rank of a person within an entity according to a criteria.
        The person with rank 0 has the minimum value of criteria.
        If condition is specified, then the persons who don't respect it are not taken into account and their rank is -1.

        Example:

        >>> age = person('age', period)  # e.g [32, 34, 2, 8, 1]
        >>> person.get_rank(household, age)
        >>> [3, 4, 0, 2, 1]

        >>> is_child = person.has_role(Household.CHILD)  # [False, False, True, True, True]
        >>> person.get_rank(household, - age, condition = is_child)  # Sort in reverse order so that the eldest child gets the rank 0.
        >>> [-1, -1, 1, 0, 2]
        """"""

        # If entity is for instance 'person.household', we get the reference entity 'household' behind the projector
        entity = entity if not isinstance(entity, Projector) else entity.reference_entity

        positions = entity.members_position
        biggest_entity_size = np.max(positions) + 1
        filtered_criteria = np.where(condition, criteria, np.inf)
        ids = entity.members_entity_id

        # Matrix: the value in line i and column j is the value of criteria for the jth person of the ith entity
        matrix = np.asarray([
            entity.value_nth_person(k, filtered_criteria, default = np.inf)
            for k in range(biggest_entity_size)
            ]).transpose()

        # We double-argsort all lines of the matrix.
        # Double-argsorting gets the rank of each value once sorted
        # For instance, if x = [3,1,6,4,0], y = np.argsort(x) is [4, 1, 0, 3, 2] (because the value with index 4 is the smallest one, the value with index 1 the second smallest, etc.) and z = np.argsort(y) is [2, 1, 4, 3, 0], the rank of each value.
        sorted_matrix = np.argsort(np.argsort(matrix))

        # Build the result vector by taking for each person the value in the right line (corresponding to its household id) and the right column (corresponding to its position)
        result = sorted_matrix[ids, positions]

        # Return -1 for the persons who don't respect the condition
        return np.where(condition, result, -1)","Get the rank of a person within an entity according to a criteria.
        The person with rank 0 has the minimum value of criteria.
        If condition is specified, then the persons who don't respect it are not taken into account and their rank is -1.

        Example:

        >>> age = person('age', period)  # e.g [32, 34, 2, 8, 1]
        >>> person.get_rank(household, age)
        >>> [3, 4, 0, 2, 1]

        >>> is_child = person.has_role(Household.CHILD)  # [False, False, True, True, True]
        >>> person.get_rank(household, - age, condition = is_child)  # Sort in reverse order so that the eldest child gets the rank 0.
        >>> [-1, -1, 1, 0, 2]"
"def file_should_be_skipped(
    filename: str,
    config: Mapping[str, Any],
    path: str = ''
) -> bool:
    """"""Returns True if the file and/or folder should be skipped based on the passed in settings.""""""
    os_path = os.path.join(path, filename)

    normalized_path = os_path.replace('\\', '/')
    if normalized_path[1:2] == ':':
        normalized_path = normalized_path[2:]

    if path and config['safety_excludes']:
        check_exclude = '/' + filename.replace('\\', '/') + '/'
        if path and os.path.basename(path) in ('lib', ):
            check_exclude = '/' + os.path.basename(path) + check_exclude
        if safety_exclude_re.search(check_exclude):
            return True

    for skip_path in config['skip']:
        if posixpath.abspath(normalized_path) == posixpath.abspath(skip_path.replace('\\', '/')):
            return True

    position = os.path.split(filename)
    while position[1]:
        if position[1] in config['skip']:
            return True
        position = os.path.split(position[0])

    for glob in config['skip_glob']:
        if fnmatch.fnmatch(filename, glob) or fnmatch.fnmatch('/' + filename, glob):
            return True

    if not (os.path.isfile(os_path) or os.path.isdir(os_path) or os.path.islink(os_path)):
        return True

    return False",Returns True if the file and/or folder should be skipped based on the passed in settings.
"def kline_echarts(self, code=None):

        def kline_formater(param):
            return param.name + ':' + vars(param)

        """"""plot the market_data""""""
        if code is None:
            path_name = '.' + os.sep + 'QA_' + self.type + \
                '_codepackage_' + self.if_fq + '.html'
            kline = Kline(
                'CodePackage_' + self.if_fq + '_' + self.type,
                width=1360,
                height=700,
                page_title='QUANTAXIS'
            )

            bar = Bar()
            data_splits = self.splits()

            for ds in data_splits:
                data = []
                axis = []
                if ds.type[-3:] == 'day':
                    datetime = np.array(ds.date.map(str))
                else:
                    datetime = np.array(ds.datetime.map(str))
                ohlc = np.array(
                    ds.data.loc[:,
                                ['open',
                                 'close',
                                 'low',
                                 'high']]
                )

                kline.add(
                    ds.code[0],
                    datetime,
                    ohlc,
                    mark_point=[""max"",
                                ""min""],
                    is_datazoom_show=True,
                    datazoom_orient='horizontal'
                )
            return kline

        else:
            data = []
            axis = []
            ds = self.select_code(code)
            data = []
            #axis = []
            if self.type[-3:] == 'day':
                datetime = np.array(ds.date.map(str))
            else:
                datetime = np.array(ds.datetime.map(str))

            ohlc = np.array(ds.data.loc[:, ['open', 'close', 'low', 'high']])
            vol = np.array(ds.volume)
            kline = Kline(
                '{}__{}__{}'.format(code,
                                    self.if_fq,
                                    self.type),
                width=1360,
                height=700,
                page_title='QUANTAXIS'
            )
            bar = Bar()
            kline.add(self.code, datetime, ohlc,
                      mark_point=[""max"", ""min""],
                      # is_label_show=True,
                      is_datazoom_show=True,
                      is_xaxis_show=False,
                      # is_toolbox_show=True,
                      tooltip_formatter='{b}:{c}',  # kline_formater,
                      # is_more_utils=True,
                      datazoom_orient='horizontal')

            bar.add(
                self.code,
                datetime,
                vol,
                is_datazoom_show=True,
                datazoom_xaxis_index=[0,
                                      1]
            )

            grid = Grid(width=1360, height=700, page_title='QUANTAXIS')
            grid.add(bar, grid_top=""80%"")
            grid.add(kline, grid_bottom=""30%"")
            return grid",plot the market_data
"def to_path_value(self, obj):
        """"""
        Takes value and turn it into a string suitable for inclusion in
        the path, by url-encoding.

        :param obj: object or string value.

        :return string: quoted value.
        """"""
        if type(obj) == list:
            return ','.join(obj)
        else:
            return str(obj)","Takes value and turn it into a string suitable for inclusion in
        the path, by url-encoding.

        :param obj: object or string value.

        :return string: quoted value."
"def actually_mount(self, vault_client, resource, active_mounts):
        """"""Handle the actual (potential) mounting of a secret backend.
        This is called in multiple contexts, but the action will always
        be the same. If we were not aware of the mountpoint at the start
        and it has not already been mounted, then mount it.""""""
        a_mounts = list(active_mounts)
        if isinstance(resource, Secret) and resource.mount == 'cubbyhole':
            return a_mounts

        active_mount = find_backend(resource.mount, active_mounts)
        if not active_mount:
            actual_mount = find_backend(resource.mount, self._mounts)
            a_mounts.append(actual_mount)
            actual_mount.sync(vault_client)

        return a_mounts","Handle the actual (potential) mounting of a secret backend.
        This is called in multiple contexts, but the action will always
        be the same. If we were not aware of the mountpoint at the start
        and it has not already been mounted, then mount it."
"def _basename(fname):
    """"""Return file name without path.""""""
    if not isinstance(fname, Path):
        fname = Path(fname)
    path, name, ext = fname.parent, fname.stem, fname.suffix
    return path, name, ext",Return file name without path.
"def _resolve_parameters(template_dict, parameter_overrides):
        """"""
        In the given template, apply parameter values to resolve intrinsic functions

        Parameters
        ----------
        template_dict : dict
            SAM Template

        parameter_overrides : dict
            Values for template parameters provided by user

        Returns
        -------
        dict
            Resolved SAM template
        """"""

        parameter_values = SamBaseProvider._get_parameter_values(template_dict, parameter_overrides)

        supported_intrinsics = {action.intrinsic_name: action() for action in SamBaseProvider._SUPPORTED_INTRINSICS}

        # Intrinsics resolver will mutate the original template
        return IntrinsicsResolver(parameters=parameter_values, supported_intrinsics=supported_intrinsics)\
            .resolve_parameter_refs(template_dict)","In the given template, apply parameter values to resolve intrinsic functions

        Parameters
        ----------
        template_dict : dict
            SAM Template

        parameter_overrides : dict
            Values for template parameters provided by user

        Returns
        -------
        dict
            Resolved SAM template"
"def register_interaction(key=None):
    """"""Decorator registering an interaction class in the registry.

    If no key is provided, the class name is used as a key. A key is provided
    for each core bqplot interaction type so that the frontend can use this
    key regardless of the kernal language.
    """"""
    def wrap(interaction):
        name = key if key is not None else interaction.__module__ + \
            interaction.__name__
        interaction.types[name] = interaction
        return interaction
    return wrap","Decorator registering an interaction class in the registry.

    If no key is provided, the class name is used as a key. A key is provided
    for each core bqplot interaction type so that the frontend can use this
    key regardless of the kernal language."
"def find_version(include_dev_version=True, root='%(pwd)s',
                 version_file='%(root)s/version.txt', version_module_paths=(),
                 git_args=None, vcs_args=None, decrement_dev_version=None,
                 strip_prefix='v',
                 Popen=subprocess.Popen, open=open):
    """"""Find an appropriate version number from version control.

    It's much more convenient to be able to use your version control system's
    tagging mechanism to derive a version number than to have to duplicate that
    information all over the place.

    The default behavior is to write out a ``version.txt`` file which contains
    the VCS output, for systems where the appropriate VCS is not installed or
    there is no VCS metadata directory present. ``version.txt`` can (and
    probably should!) be packaged in release tarballs by way of the
    ``MANIFEST.in`` file.

    :param include_dev_version: By default, if there are any commits after the
        most recent tag (as reported by the VCS), that number will be included
        in the version number as a ``.post`` suffix. For example, if the most
        recent tag is ``1.0`` and there have been three commits after that tag,
        the version number will be ``1.0.post3``. This behavior can be disabled
        by setting this parameter to ``False``.

    :param root: The directory of the repository root. The default value is the
        current working directory, since when running ``setup.py``, this is
        often (but not always) the same as the current working directory.
        Standard substitutions are performed on this value.

    :param version_file: The name of the file where version information will be
        saved. Reading and writing version files can be disabled altogether by
        setting this parameter to ``None``. Standard substitutions are
        performed on this value.

    :param version_module_paths: A list of python modules which will be
        automatically generated containing ``__version__`` and ``__sha__``
        attributes. For example, with ``package/_version.py`` as a version
        module path, ``package/__init__.py`` could do ``from package._version
        import __version__, __sha__``.

    :param git_args: **Deprecated.** Please use *vcs_args* instead.

    :param vcs_args: The command to run to get a version. By default, this is
        automatically guessed from directories present in the repository root.
        Specify this as a list of string arguments including the program to
        run, e.g. ``['git', 'describe']``. Standard substitutions are performed
        on each value in the provided list.

    :param decrement_dev_version: If ``True``, subtract one from the number of
        commits after the most recent tag. This is primarily for hg, as hg
        requires a commit to make a tag. If the VCS used is hg (i.e. the
        revision starts with ``'hg'``) and *decrement_dev_version* is not
        specified as ``False``, *decrement_dev_version* will be set to
        ``True``.

    :param strip_prefix: A string which will be stripped from the start of
        version number tags. By default this is ``'v'``, but could be
        ``'debian/'`` for compatibility with ``git-dch``.

    :param Popen: Defaults to ``subprocess.Popen``. This is for testing.

    :param open: Defaults to ``open``. This is for testing.

    *root*, *version_file*, and *git_args* each support some substitutions:

    ``%(root)s``
      The value provided for *root*. This is not available for the *root*
      parameter itself.

    ``%(pwd)s``
      The current working directory.

    ``/`` will automatically be translated into the correct path separator for
    the current platform, such as ``:`` or ``\``.

    ``vcversioner`` will perform automatic VCS detection with the following
    directories, in order, and run the specified commands.

       ``%(root)s/.git``

          ``git --git-dir %(root)s/.git describe --tags --long``. ``--git-dir``
          is used to prevent contamination from git repositories which aren't
          the git repository of your project.

       ``%(root)s/.hg``

          ``hg log -R %(root)s -r . --template
          '{latesttag}-{latesttagdistance}-hg{node|short}'``. ``-R`` is
          similarly used to prevent contamination.

    """"""

    substitutions = {'pwd': os.getcwd()}
    substitutions['root'] = root % substitutions
    def substitute(val):
        return _fix_path(val % substitutions)
    if version_file is not None:
        version_file = substitute(version_file)

    if git_args is not None:
        warnings.warn(
            'passing `git_args is deprecated; please use vcs_args',
            DeprecationWarning)
        vcs_args = git_args

    if vcs_args is None:
        for path, args in _vcs_args_by_path:
            if os.path.exists(substitute(path)):
                vcs_args = args
                break

    raw_version = None
    vcs_output = []

    if vcs_args is not None:
        vcs_args = [substitute(arg) for arg in vcs_args]

        # try to pull the version from some VCS, or (perhaps) fall back on a
        # previously-saved version.
        try:
            proc = Popen(vcs_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except OSError:
            pass
        else:
            stdout, stderr = proc.communicate()
            raw_version = stdout.strip().decode()
            vcs_output = stderr.decode().splitlines()
            version_source = 'VCS'
        failure = '%r failed' % (vcs_args,)
    else:
        failure = 'no VCS could be detected in %(root)r' % substitutions

    def show_vcs_output():
        if not vcs_output:
            return
        print('-- VCS output follows --')
        for line in vcs_output:
            print(line)

    # VCS failed if the string is empty
    if not raw_version:
        if version_file is None:
            print('%s.' % (failure,))
            show_vcs_output()
            raise SystemExit(2)
        elif not os.path.exists(version_file):
            print(""%s and %r isn't present."" % (failure, version_file))
            print(""are you installing from a github tarball?"")
            show_vcs_output()
            raise SystemExit(2)
        with open(version_file, 'rb') as infile:
            raw_version = infile.read().decode()
        version_source = repr(version_file)


    # try to parse the version into something usable.
    try:
        tag_version, commits, sha = raw_version.rsplit('-', 2)
    except ValueError:
        print(""%r (from %s) couldn't be parsed into a version."" % (
            raw_version, version_source))
        show_vcs_output()
        raise SystemExit(2)

    # remove leading prefix
    if tag_version.startswith(strip_prefix):
        tag_version = tag_version[len(strip_prefix):]

    if version_file is not None:
        with open(version_file, 'w') as outfile:
            outfile.write(raw_version)

    if sha.startswith('hg') and decrement_dev_version is None:
        decrement_dev_version = True

    if decrement_dev_version:
        commits = str(int(commits) - 1)

    if commits == '0' or not include_dev_version:
        version = tag_version
    else:
        version = '%s.post%s' % (tag_version, commits)

    for path in version_module_paths:
        with open(path, 'w') as outfile:
            outfile.write(""""""
# This file is automatically generated by setup.py.
__version__ = {0}
__sha__ = {1}
__revision__ = {1}
"""""".format(repr(version).lstrip('u'), repr(sha).lstrip('u')))

    return Version(version, commits, sha)","Find an appropriate version number from version control.

    It's much more convenient to be able to use your version control system's
    tagging mechanism to derive a version number than to have to duplicate that
    information all over the place.

    The default behavior is to write out a ``version.txt`` file which contains
    the VCS output, for systems where the appropriate VCS is not installed or
    there is no VCS metadata directory present. ``version.txt`` can (and
    probably should!) be packaged in release tarballs by way of the
    ``MANIFEST.in`` file.

    :param include_dev_version: By default, if there are any commits after the
        most recent tag (as reported by the VCS), that number will be included
        in the version number as a ``.post`` suffix. For example, if the most
        recent tag is ``1.0`` and there have been three commits after that tag,
        the version number will be ``1.0.post3``. This behavior can be disabled
        by setting this parameter to ``False``.

    :param root: The directory of the repository root. The default value is the
        current working directory, since when running ``setup.py``, this is
        often (but not always) the same as the current working directory.
        Standard substitutions are performed on this value.

    :param version_file: The name of the file where version information will be
        saved. Reading and writing version files can be disabled altogether by
        setting this parameter to ``None``. Standard substitutions are
        performed on this value.

    :param version_module_paths: A list of python modules which will be
        automatically generated containing ``__version__`` and ``__sha__``
        attributes. For example, with ``package/_version.py`` as a version
        module path, ``package/__init__.py`` could do ``from package._version
        import __version__, __sha__``.

    :param git_args: **Deprecated.** Please use *vcs_args* instead.

    :param vcs_args: The command to run to get a version. By default, this is
        automatically guessed from directories present in the repository root.
        Specify this as a list of string arguments including the program to
        run, e.g. ``['git', 'describe']``. Standard substitutions are performed
        on each value in the provided list.

    :param decrement_dev_version: If ``True``, subtract one from the number of
        commits after the most recent tag. This is primarily for hg, as hg
        requires a commit to make a tag. If the VCS used is hg (i.e. the
        revision starts with ``'hg'``) and *decrement_dev_version* is not
        specified as ``False``, *decrement_dev_version* will be set to
        ``True``.

    :param strip_prefix: A string which will be stripped from the start of
        version number tags. By default this is ``'v'``, but could be
        ``'debian/'`` for compatibility with ``git-dch``.

    :param Popen: Defaults to ``subprocess.Popen``. This is for testing.

    :param open: Defaults to ``open``. This is for testing.

    *root*, *version_file*, and *git_args* each support some substitutions:

    ``%(root)s``
      The value provided for *root*. This is not available for the *root*
      parameter itself.

    ``%(pwd)s``
      The current working directory.

    ``/`` will automatically be translated into the correct path separator for
    the current platform, such as ``:`` or ``\``.

    ``vcversioner`` will perform automatic VCS detection with the following
    directories, in order, and run the specified commands.

       ``%(root)s/.git``

          ``git --git-dir %(root)s/.git describe --tags --long``. ``--git-dir``
          is used to prevent contamination from git repositories which aren't
          the git repository of your project.

       ``%(root)s/.hg``

          ``hg log -R %(root)s -r . --template
          '{latesttag}-{latesttagdistance}-hg{node|short}'``. ``-R`` is
          similarly used to prevent contamination."
"def to_host(self):
        """"""Copy or coerce to a Host.""""""
        return Host(self.address,
                    alias=self.alias,
                    user=self.user,
                    keyfile=self.keyfile,
                    port=self.port,
                    extra=self.extra)",Copy or coerce to a Host.
"def nla_put_string(msg, attrtype, value):
    """"""Add string attribute to Netlink message.

    https://github.com/thom311/libnl/blob/libnl3_2_25/lib/attr.c#L674

    Positional arguments:
    msg -- Netlink message (nl_msg class instance).
    attrtype -- attribute type (integer).
    value -- bytes() or bytearray() value (e.g. 'Test'.encode('ascii')).

    Returns:
    0 on success or a negative error code.
    """"""
    data = bytearray(value) + bytearray(b'\0')
    return nla_put(msg, attrtype, len(data), data)","Add string attribute to Netlink message.

    https://github.com/thom311/libnl/blob/libnl3_2_25/lib/attr.c#L674

    Positional arguments:
    msg -- Netlink message (nl_msg class instance).
    attrtype -- attribute type (integer).
    value -- bytes() or bytearray() value (e.g. 'Test'.encode('ascii')).

    Returns:
    0 on success or a negative error code."
"def get_repository(self, entity_cls):
        """"""Return a repository object configured with a live connection""""""
        model_cls = self.get_model(entity_cls)
        return DictRepository(self, entity_cls, model_cls)",Return a repository object configured with a live connection
"def list_contrib(name=None, ret=False, _debug=False):
    """"""Show the list of all existing contribs.
    Params:
     - name: filter to search the contribs
     - ret: whether the function should return a dict instead of printing it
    """"""
    # _debug: checks that all contrib modules have correctly defined:
    # # scapy.contrib.description = [...]
    # # scapy.contrib.status = [...]
    # # scapy.contrib.name = [...] (optional)
    # or set the flag:
    # # scapy.contrib.description = skip
    # to skip the file
    if name is None:
        name = ""*.py""
    elif ""*"" not in name and ""?"" not in name and not name.endswith("".py""):
        name += "".py""
    results = []
    dir_path = os.path.join(os.path.dirname(__file__), ""contrib"")
    if sys.version_info >= (3, 5):
        name = os.path.join(dir_path, ""**"", name)
        iterator = glob.iglob(name, recursive=True)
    else:
        name = os.path.join(dir_path, name)
        iterator = glob.iglob(name)
    for f in iterator:
        mod = f.replace(os.path.sep, ""."").partition(""contrib."")[2]
        if mod.startswith(""__""):
            continue
        if mod.endswith("".py""):
            mod = mod[:-3]
        desc = {""description"": None, ""status"": None, ""name"": mod}
        for l in io.open(f, errors=""replace""):
            if l[0] != ""#"":
                continue
            p = l.find(""scapy.contrib."")
            if p >= 0:
                p += 14
                q = l.find(""="", p)
                key = l[p:q].strip()
                value = l[q + 1:].strip()
                desc[key] = value
            if desc[""status""] == ""skip"":
                break
            if desc[""description""] and desc[""status""]:
                results.append(desc)
                break
        if _debug:
            if desc[""status""] == ""skip"":
                pass
            elif not desc[""description""] or not desc[""status""]:
                raise Scapy_Exception(""Module %s is missing its ""
                                      ""contrib infos !"" % mod)
    results.sort(key=lambda x: x[""name""])
    if ret:
        return results
    else:
        for desc in results:
            print(""%(name)-20s: %(description)-40s status=%(status)s"" % desc)","Show the list of all existing contribs.
    Params:
     - name: filter to search the contribs
     - ret: whether the function should return a dict instead of printing it"
"def run_nose(self, params):
        """"""
        :type params: Params
        """"""
        thread.set_index(params.thread_index)
        log.debug(""[%s] Starting nose iterations: %s"", params.worker_index, params)
        assert isinstance(params.tests, list)
        # argv.extend(['--with-apiritif', '--nocapture', '--exe', '--nologcapture'])

        end_time = self.params.ramp_up + self.params.hold_for
        end_time += time.time() if end_time else 0
        time.sleep(params.delay)

        plugin = ApiritifPlugin(self._writer)
        self._writer.concurrency += 1

        config = Config(env=os.environ, files=all_config_files(), plugins=DefaultPluginManager())
        config.plugins.addPlugins(extraplugins=[plugin])
        config.testNames = params.tests
        config.verbosity = 3 if params.verbose else 0
        if params.verbose:
            config.stream = open(os.devnull, ""w"")  # FIXME: use ""with"", allow writing to file/log

        iteration = 0
        try:
            while True:
                log.debug(""Starting iteration:: index=%d,start_time=%.3f"", iteration, time.time())
                thread.set_iteration(iteration)
                ApiritifTestProgram(config=config)
                log.debug(""Finishing iteration:: index=%d,end_time=%.3f"", iteration, time.time())

                iteration += 1

                # reasons to stop
                if plugin.stop_reason:
                    log.debug(""[%s] finished prematurely: %s"", params.worker_index, plugin.stop_reason)
                elif iteration >= params.iterations:
                    log.debug(""[%s] iteration limit reached: %s"", params.worker_index, params.iterations)
                elif 0 < end_time <= time.time():
                    log.debug(""[%s] duration limit reached: %s"", params.worker_index, params.hold_for)
                else:
                    continue  # continue if no one is faced

                break
        finally:
            self._writer.concurrency -= 1

            if params.verbose:
                config.stream.close()",:type params: Params
"def generate_pipeline_code(pipeline_tree, operators):
    """"""Generate code specific to the construction of the sklearn Pipeline.

    Parameters
    ----------
    pipeline_tree: list
        List of operators in the current optimized pipeline

    Returns
    -------
    Source code for the sklearn pipeline

    """"""
    steps = _process_operator(pipeline_tree, operators)
    pipeline_text = ""make_pipeline(\n{STEPS}\n)"".format(STEPS=_indent("",\n"".join(steps), 4))
    return pipeline_text","Generate code specific to the construction of the sklearn Pipeline.

    Parameters
    ----------
    pipeline_tree: list
        List of operators in the current optimized pipeline

    Returns
    -------
    Source code for the sklearn pipeline"
"def get_tri_area(pts):
    """"""
    Given a list of coords for 3 points,
    Compute the area of this triangle.

    Args:
        pts: [a, b, c] three points
    """"""
    a, b, c = pts[0], pts[1], pts[2]
    v1 = np.array(b) - np.array(a)
    v2 = np.array(c) - np.array(a)
    area_tri = abs(sp.linalg.norm(sp.cross(v1, v2)) / 2)
    return area_tri","Given a list of coords for 3 points,
    Compute the area of this triangle.

    Args:
        pts: [a, b, c] three points"
"def _F_outdegree(H, F):
    """"""Returns the result of a function F applied to the set of outdegrees in
    in the hypergraph.

    :param H: the hypergraph whose outdegrees will be operated on.
    :param F: function to execute on the list of outdegrees in the hypergraph.
    :returns: result of the given function F.
    :raises: TypeError -- Algorithm only applicable to directed hypergraphs

    """"""
    if not isinstance(H, DirectedHypergraph):
        raise TypeError(""Algorithm only applicable to directed hypergraphs"")

    return F([len(H.get_forward_star(node))
             for node in H.get_node_set()])","Returns the result of a function F applied to the set of outdegrees in
    in the hypergraph.

    :param H: the hypergraph whose outdegrees will be operated on.
    :param F: function to execute on the list of outdegrees in the hypergraph.
    :returns: result of the given function F.
    :raises: TypeError -- Algorithm only applicable to directed hypergraphs"
"def set_api_url(self, api_url=""https://{lang}.wikipedia.org/w/api.php"", lang=""en""):
        """""" Set the API URL and language

            Args:
                api_url (str): API URL to use
                lang (str): Language of the API URL
            Raises:
                :py:func:`mediawiki.exceptions.MediaWikiAPIURLError`: if the \
                url is not a valid MediaWiki site """"""
        old_api_url = self._api_url
        old_lang = self._lang
        self._lang = lang.lower()
        self._api_url = api_url.format(lang=self._lang)
        try:
            self._get_site_info()
            self.__supported_languages = None  # reset this
        except MediaWikiException:
            # reset api url and lang in the event that the exception was caught
            self._api_url = old_api_url
            self._lang = old_lang
            raise MediaWikiAPIURLError(api_url)
        self.clear_memoized()","Set the API URL and language

            Args:
                api_url (str): API URL to use
                lang (str): Language of the API URL
            Raises:
                :py:func:`mediawiki.exceptions.MediaWikiAPIURLError`: if the \
                url is not a valid MediaWiki site"
"def average_patterson_fst(ac1, ac2, blen):
    """"""Estimate average Fst between two populations and standard error using
    the block-jackknife.

    Parameters
    ----------
    ac1 : array_like, int, shape (n_variants, n_alleles)
        Allele counts array from the first population.
    ac2 : array_like, int, shape (n_variants, n_alleles)
        Allele counts array from the second population.
    blen : int
        Block size (number of variants).

    Returns
    -------
    fst : float
        Estimated value of the statistic using all data.
    se : float
        Estimated standard error.
    vb : ndarray, float, shape (n_blocks,)
        Value of the statistic in each block.
    vj : ndarray, float, shape (n_blocks,)
        Values of the statistic from block-jackknife resampling.

    """"""

    # calculate per-variant values
    num, den = patterson_fst(ac1, ac2)

    # calculate overall estimate
    fst = np.nansum(num) / np.nansum(den)

    # compute the numerator and denominator within each block
    num_bsum = moving_statistic(num, statistic=np.nansum, size=blen)
    den_bsum = moving_statistic(den, statistic=np.nansum, size=blen)

    # calculate the statistic values in each block
    vb = num_bsum / den_bsum

    # estimate standard error
    _, se, vj = jackknife((num_bsum, den_bsum),
                          statistic=lambda n, d: np.sum(n) / np.sum(d))

    return fst, se, vb, vj","Estimate average Fst between two populations and standard error using
    the block-jackknife.

    Parameters
    ----------
    ac1 : array_like, int, shape (n_variants, n_alleles)
        Allele counts array from the first population.
    ac2 : array_like, int, shape (n_variants, n_alleles)
        Allele counts array from the second population.
    blen : int
        Block size (number of variants).

    Returns
    -------
    fst : float
        Estimated value of the statistic using all data.
    se : float
        Estimated standard error.
    vb : ndarray, float, shape (n_blocks,)
        Value of the statistic in each block.
    vj : ndarray, float, shape (n_blocks,)
        Values of the statistic from block-jackknife resampling."
"def from_ic50(ic50, max_ic50=50000.0):
    """"""
    Convert ic50s to regression targets in the range [0.0, 1.0].
    
    Parameters
    ----------
    ic50 : numpy.array of float

    Returns
    -------
    numpy.array of float

    """"""
    x = 1.0 - (numpy.log(ic50) / numpy.log(max_ic50))
    return numpy.minimum(
        1.0,
        numpy.maximum(0.0, x))","Convert ic50s to regression targets in the range [0.0, 1.0].
    
    Parameters
    ----------
    ic50 : numpy.array of float

    Returns
    -------
    numpy.array of float"
"def recipe_zheng17(adata, n_top_genes=1000, log=True, plot=False, copy=False):
    """"""Normalization and filtering as of [Zheng17]_.

    Reproduces the preprocessing of [Zheng17]_ - the Cell Ranger R Kit of 10x
    Genomics.

    Expects non-logarithmized data. If using logarithmized data, pass `log=False`.

    The recipe runs the following steps

    .. code:: python

        sc.pp.filter_genes(adata, min_counts=1)  # only consider genes with more than 1 count
        sc.pp.normalize_per_cell(                # normalize with total UMI count per cell
             adata, key_n_counts='n_counts_all')
        filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes
            adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False)
        adata = adata[:, filter_result.gene_subset]     # subset the genes
        sc.pp.normalize_per_cell(adata)          # renormalize after filtering
        if log: sc.pp.log1p(adata)               # log transform: adata.X = log(adata.X + 1)
        sc.pp.scale(adata)                       # scale to unit variance and shift to zero mean


    Parameters
    ----------
    adata : :class:`~anndata.AnnData`
        Annotated data matrix.
    n_top_genes : `int`, optional (default: 1000)
        Number of genes to keep.
    log : `bool`, optional (default: `True`)
        Take logarithm.
    plot : `bool`, optional (default: `True`)
        Show a plot of the gene dispersion vs. mean relation.
    copy : `bool`, optional (default: `False`)
        Return a copy of `adata` instead of updating it.

    Returns
    -------
    Returns or updates `adata` depending on `copy`.
    """"""
    logg.info('running recipe zheng17', reset=True)
    if copy: adata = adata.copy()
    pp.filter_genes(adata, min_counts=1)  # only consider genes with more than 1 count
    pp.normalize_per_cell(adata,  # normalize with total UMI count per cell
                          key_n_counts='n_counts_all')
    filter_result = filter_genes_dispersion(
        adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False)
    if plot:
        from ..plotting import _preprocessing as ppp  # should not import at the top of the file
        ppp.filter_genes_dispersion(filter_result, log=True)
    # actually filter the genes, the following is the inplace version of
    #     adata = adata[:, filter_result.gene_subset]
    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes
    pp.normalize_per_cell(adata)  # renormalize after filtering
    if log: pp.log1p(adata)  # log transform: X = log(X + 1)
    pp.scale(adata)
    logg.info('    finished', time=True)
    return adata if copy else None","Normalization and filtering as of [Zheng17]_.

    Reproduces the preprocessing of [Zheng17]_ - the Cell Ranger R Kit of 10x
    Genomics.

    Expects non-logarithmized data. If using logarithmized data, pass `log=False`.

    The recipe runs the following steps

    .. code:: python

        sc.pp.filter_genes(adata, min_counts=1)  # only consider genes with more than 1 count
        sc.pp.normalize_per_cell(                # normalize with total UMI count per cell
             adata, key_n_counts='n_counts_all')
        filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes
            adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False)
        adata = adata[:, filter_result.gene_subset]     # subset the genes
        sc.pp.normalize_per_cell(adata)          # renormalize after filtering
        if log: sc.pp.log1p(adata)               # log transform: adata.X = log(adata.X + 1)
        sc.pp.scale(adata)                       # scale to unit variance and shift to zero mean


    Parameters
    ----------
    adata : :class:`~anndata.AnnData`
        Annotated data matrix.
    n_top_genes : `int`, optional (default: 1000)
        Number of genes to keep.
    log : `bool`, optional (default: `True`)
        Take logarithm.
    plot : `bool`, optional (default: `True`)
        Show a plot of the gene dispersion vs. mean relation.
    copy : `bool`, optional (default: `False`)
        Return a copy of `adata` instead of updating it.

    Returns
    -------
    Returns or updates `adata` depending on `copy`."
"def _get_k8s_model_dict(model_type, model):
    """"""
    Returns a dictionary representation of a provided model type
    """"""
    model = copy.deepcopy(model)

    if isinstance(model, model_type):
        return model.to_dict()
    elif isinstance(model, dict):
        return _map_dict_keys_to_model_attributes(model_type, model)
    else:
        raise AttributeError(""Expected object of type '{}' (or 'dict') but got '{}'."".format(model_type.__name__, type(model).__name__))",Returns a dictionary representation of a provided model type
"def download(url):
    """"""
    Download `url` and return it as utf-8 encoded text.

    Args:
        url (str): What should be downloaded?

    Returns:
        str: Content of the page.
    """"""
    headers = {""User-Agent"": USER_AGENT}
    resp = requests.get(
        url,
        timeout=REQUEST_TIMEOUT,
        headers=headers,
        allow_redirects=True,
        verify=False,
    )

    def decode(st, alt_encoding=None):
        encodings = ['ascii', 'utf-8', 'iso-8859-1', 'iso-8859-15']

        if alt_encoding:
            if isinstance(alt_encoding, basestring):
                encodings.append(alt_encoding)
            else:
                encodings.extend(alt_encoding)

        for encoding in encodings:
            try:
                return st.encode(encoding).decode(""utf-8"")
            except UnicodeEncodeError, UnicodeDecodeError:
                pass

        raise UnicodeError('Could not find encoding.')

    return decode(resp.text, resp.encoding)","Download `url` and return it as utf-8 encoded text.

    Args:
        url (str): What should be downloaded?

    Returns:
        str: Content of the page."
"def execute(self, eopatch):
        """"""Returns the EOPatch with renamed features.

        :param eopatch: input EOPatch
        :type eopatch: EOPatch
        :return: input EOPatch with the renamed features
        :rtype: EOPatch
        """"""
        for feature_type, feature_name, new_feature_name in self.feature_gen(eopatch):
            eopatch[feature_type][new_feature_name] = eopatch[feature_type][feature_name]
            del eopatch[feature_type][feature_name]

        return eopatch","Returns the EOPatch with renamed features.

        :param eopatch: input EOPatch
        :type eopatch: EOPatch
        :return: input EOPatch with the renamed features
        :rtype: EOPatch"
"def get_section_by_rva(self, rva):
        """"""Get the section containing the given address.""""""

        for section in self.sections:
            if section.contains_rva(rva):
                return section

        return None",Get the section containing the given address.
"def update(self, ignore_warnings=None):
        """"""Update this app_profile.

        .. note::

            Update any or all of the following values:
            ``routing_policy_type``
            ``description``
            ``cluster_id``
            ``allow_transactional_writes``

        """"""
        update_mask_pb = field_mask_pb2.FieldMask()

        if self.description is not None:
            update_mask_pb.paths.append(""description"")

        if self.routing_policy_type == RoutingPolicyType.ANY:
            update_mask_pb.paths.append(""multi_cluster_routing_use_any"")
        else:
            update_mask_pb.paths.append(""single_cluster_routing"")

        return self.instance_admin_client.update_app_profile(
            app_profile=self._to_pb(),
            update_mask=update_mask_pb,
            ignore_warnings=ignore_warnings,
        )","Update this app_profile.

        .. note::

            Update any or all of the following values:
            ``routing_policy_type``
            ``description``
            ``cluster_id``
            ``allow_transactional_writes``"
"def _secret_generator(baseline):
    """"""Generates secrets to audit, from the baseline""""""
    for filename, secrets in baseline['results'].items():
        for secret in secrets:
            yield filename, secret","Generates secrets to audit, from the baseline"
"def get_grades(self, login=None, promotion=None, **kwargs):
        """"""Get a user's grades on a single promotion based on his login.

        Either use the `login` param, or the client's login if unset.
        :return: JSON
        """"""

        _login = kwargs.get(
            'login',
            login or self._login
        )
        _promotion_id = kwargs.get('promotion', promotion)
        _grades_url = GRADES_URL.format(login=_login, promo_id=_promotion_id)
        return self._request_api(url=_grades_url).json()","Get a user's grades on a single promotion based on his login.

        Either use the `login` param, or the client's login if unset.
        :return: JSON"
"def get_rendered_transform_path_relative(self, relative_transform_ref):
        """"""
        Generates a rendered transform path relative to
        parent.
        :param relative_transform_ref:
        :return:
        """"""
        path = self.transform_path
        parent = self.parent

        while parent is not None and parent is not relative_transform_ref:
            path = ""{0}/{1}"".format(parent.transform_path, path)
            parent = parent.parent

        return path","Generates a rendered transform path relative to
        parent.
        :param relative_transform_ref:
        :return:"
"def get_next_task(self):
        """"""get the next task if there's one that should be processed,
        and return how long it will be until the next one should be
        processed.""""""
        if _debug: TaskManager._debug(""get_next_task"")

        # get the time
        now = _time()

        task = None
        delta = None

        if self.tasks:
            # look at the first task
            when, n, nxttask = self.tasks[0]
            if when <= now:
                # pull it off the list and mark that it's no longer scheduled
                heappop(self.tasks)
                task = nxttask
                task.isScheduled = False

                if self.tasks:
                    when, n, nxttask = self.tasks[0]
                    # peek at the next task, return how long to wait
                    delta = max(when - now, 0.0)
            else:
                delta = when - now

        # return the task to run and how long to wait for the next one
        return (task, delta)","get the next task if there's one that should be processed,
        and return how long it will be until the next one should be
        processed."
"def submit_coroutine(self, coro, loop):
        """"""Schedule and await a coroutine on the specified loop

        The coroutine is wrapped and scheduled using
        :func:`asyncio.run_coroutine_threadsafe`. While the coroutine is
        ""awaited"", the result is not available as method returns immediately.

        Args:
            coro: The :term:`coroutine` to schedule
            loop: The :class:`event loop <asyncio.BaseEventLoop>` on which to
                schedule the coroutine

        Note:
            This method is used internally by :meth:`__call__` and is not meant
            to be called directly.
        """"""
        async def _do_call(_coro):
            with _IterationGuard(self):
                await _coro
        asyncio.run_coroutine_threadsafe(_do_call(coro), loop=loop)","Schedule and await a coroutine on the specified loop

        The coroutine is wrapped and scheduled using
        :func:`asyncio.run_coroutine_threadsafe`. While the coroutine is
        ""awaited"", the result is not available as method returns immediately.

        Args:
            coro: The :term:`coroutine` to schedule
            loop: The :class:`event loop <asyncio.BaseEventLoop>` on which to
                schedule the coroutine

        Note:
            This method is used internally by :meth:`__call__` and is not meant
            to be called directly."
"def __validation_callback(self, event):
        # type: (str) -> Any
        """"""
        Specific handling for the ``@ValidateComponent`` and
        ``@InvalidateComponent`` callback, as it requires checking arguments
        count and order

        :param event: The kind of life-cycle callback (in/validation)
        :return: The callback result, or None
        :raise Exception: Something went wrong
        """"""
        comp_callback = self.context.get_callback(event)
        if not comp_callback:
            # No registered callback
            return True

        # Get the list of arguments
        try:
            args = getattr(comp_callback, constants.IPOPO_VALIDATE_ARGS)
        except AttributeError:
            raise TypeError(
                ""@ValidateComponent callback is missing internal description""
            )

        # Associate values to arguments
        mapping = {
            constants.ARG_BUNDLE_CONTEXT: self.bundle_context,
            constants.ARG_COMPONENT_CONTEXT: self.context,
            constants.ARG_PROPERTIES: self.context.properties.copy(),
        }
        mapped_args = [mapping[arg] for arg in args]

        # Call it
        result = comp_callback(self.instance, *mapped_args)
        if result is None:
            # Special case, if the call back returns nothing
            return True

        return result","Specific handling for the ``@ValidateComponent`` and
        ``@InvalidateComponent`` callback, as it requires checking arguments
        count and order

        :param event: The kind of life-cycle callback (in/validation)
        :return: The callback result, or None
        :raise Exception: Something went wrong"
"def frg(args):
    """"""
    %prog frg frgfile

    Extract FASTA sequences from frg reads.
    """"""
    p = OptionParser(frg.__doc__)
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(p.print_help())

    frgfile, = args
    fastafile = frgfile.rsplit(""."", 1)[0] + "".fasta""
    fp = open(frgfile)
    fw = open(fastafile, ""w"")

    for rec in iter_records(fp):
        if rec.type != ""FRG"":
            continue
        id = rec.get_field(""acc"")
        seq = rec.get_field(""seq"")
        s = SeqRecord(Seq(seq), id=id, description="""")
        SeqIO.write([s], fw, ""fasta"")

    fw.close()","%prog frg frgfile

    Extract FASTA sequences from frg reads."
"def _send_flow(self, active):
        '''
        Send a flow control command.
        '''
        args = Writer()
        args.write_bit(active)
        self.send_frame(MethodFrame(self.channel_id, 20, 20, args))
        self.channel.add_synchronous_cb(self._recv_flow_ok)",Send a flow control command.
"def set(self, name, value, ex=None, px=None, nx=False, xx=False):
        """"""Set the value at key ``name`` to ``value``

        :param ex: sets an expire flag on key ``name`` for ``ex`` seconds.
        :param px: sets an expire flag on key ``name`` for ``px`` milliseconds.
        :param nx: if set to True, set the value at key ``name`` to ``value``
            if it does not already exist.
        :param xx: if set to True, set the value at key ``name`` to ``value``
            if it already exists.
        """"""
        pieces = [name, value]
        if ex:
            pieces.append('EX')
            if isinstance(ex, datetime.timedelta):
                ex = ex.seconds + ex.days * 24 * 3600
            pieces.append(ex)
        if px:
            pieces.append('PX')
            if isinstance(px, datetime.timedelta):
                ms = int(px.microseconds / 1000)
                px = (px.seconds + px.days * 24 * 3600) * 1000 + ms
            pieces.append(px)

        if nx:
            pieces.append('NX')
        if xx:
            pieces.append('XX')
        return self.execute('set', *pieces)","Set the value at key ``name`` to ``value``

        :param ex: sets an expire flag on key ``name`` for ``ex`` seconds.
        :param px: sets an expire flag on key ``name`` for ``px`` milliseconds.
        :param nx: if set to True, set the value at key ``name`` to ``value``
            if it does not already exist.
        :param xx: if set to True, set the value at key ``name`` to ``value``
            if it already exists."
"def runs_per_second(generator, seconds=3):
    ''' 
use this function as a profiler for both functions and generators
to see how many iterations or cycles they can run per second 

Example usage for timing simple operations/functions:

``` 
    print(runs_per_second(lambda:1+2))
    # 2074558
    print(runs_per_second(lambda:1-2))
    # 2048523
    print(runs_per_second(lambda:1/2))
    # 2075186
    print(runs_per_second(lambda:1*2))
    # 2101722
    print(runs_per_second(lambda:1**2))
    # 2104572
```

Example usage for timing iteration speed of generators:
  
``` 
    def counter():
        c = 0
        while 1:
            yield c
            c+=1

    print(runs_per_second(counter()))
    # 1697328
    print(runs_per_second((i for i in range(2000))))
    # 1591301
``` 
'''
    assert isinstance(seconds, int), 'runs_per_second needs seconds to be an int, not {}'.format(repr(seconds))
    assert seconds>0, 'runs_per_second needs seconds to be positive, not {}'.format(repr(seconds))
    # if generator is a function, turn it into a generator for testing
    if callable(generator) and not any(i in ('next', '__next__', '__iter__') for i in dir(generator)):
        try:
            # get the output of the function
            output = generator()
        except:
            # if the function crashes without any arguments
            raise Exception('runs_per_second needs a working function that accepts no arguments')
        else:
            # this usage of iter infinitely calls a function until the second argument is the output
            # so I set the second argument to something that isnt what output was.
            generator = iter(generator, (1 if output is None else None))
            del output
    c=0  # run counter, keep this one short for performance reasons
    entire_test_time_used = False
    start = ts()
    end = start+seconds
    for _ in generator:
        if ts()>end:
            entire_test_time_used = True
            break
        else:
            c += 1
    duration = (ts())-start  # the ( ) around ts ensures that it will be the first thing calculated
    return int(c/(seconds if entire_test_time_used else duration))","use this function as a profiler for both functions and generators
to see how many iterations or cycles they can run per second 

Example usage for timing simple operations/functions:

``` 
    print(runs_per_second(lambda:1+2))
    # 2074558
    print(runs_per_second(lambda:1-2))
    # 2048523
    print(runs_per_second(lambda:1/2))
    # 2075186
    print(runs_per_second(lambda:1*2))
    # 2101722
    print(runs_per_second(lambda:1**2))
    # 2104572
```

Example usage for timing iteration speed of generators:
  
``` 
    def counter():
        c = 0
        while 1:
            yield c
            c+=1

    print(runs_per_second(counter()))
    # 1697328
    print(runs_per_second((i for i in range(2000))))
    # 1591301
```"
"def clone(local_root, new_root, remote, branch, rel_dest, exclude):
    """"""Clone ""local_root"" origin into a new directory and check out a specific branch. Optionally run ""git rm"".

    :raise CalledProcessError: Unhandled git command failure.
    :raise GitError: Handled git failures.

    :param str local_root: Local path to git root directory.
    :param str new_root: Local path empty directory in which branch will be cloned into.
    :param str remote: The git remote to clone from to.
    :param str branch: Checkout this branch.
    :param str rel_dest: Run ""git rm"" on this directory if exclude is truthy.
    :param iter exclude: List of strings representing relative file paths to exclude from ""git rm"".
    """"""
    log = logging.getLogger(__name__)
    output = run_command(local_root, ['git', 'remote', '-v'])
    remotes = dict()
    for match in RE_ALL_REMOTES.findall(output):
        remotes.setdefault(match[0], [None, None])
        if match[2] == 'fetch':
            remotes[match[0]][0] = match[1]
        else:
            remotes[match[0]][1] = match[1]
    if not remotes:
        raise GitError('Git repo has no remotes.', output)
    if remote not in remotes:
        raise GitError('Git repo missing remote ""{}"".'.format(remote), output)

    # Clone.
    try:
        run_command(new_root, ['git', 'clone', remotes[remote][0], '--depth=1', '--branch', branch, '.'])
    except CalledProcessError as exc:
        raise GitError('Failed to clone from remote repo URL.', exc.output)

    # Make sure user didn't select a tag as their DEST_BRANCH.
    try:
        run_command(new_root, ['git', 'symbolic-ref', 'HEAD'])
    except CalledProcessError as exc:
        raise GitError('Specified branch is not a real branch.', exc.output)

    # Copy all remotes from original repo.
    for name, (fetch, push) in remotes.items():
        try:
            run_command(new_root, ['git', 'remote', 'set-url' if name == 'origin' else 'add', name, fetch], retry=3)
            run_command(new_root, ['git', 'remote', 'set-url', '--push', name, push], retry=3)
        except CalledProcessError as exc:
            raise GitError('Failed to set git remote URL.', exc.output)

    # Done if no exclude.
    if not exclude:
        return

    # Resolve exclude paths.
    exclude_joined = [
        os.path.relpath(p, new_root) for e in exclude for p in glob.glob(os.path.join(new_root, rel_dest, e))
    ]
    log.debug('Expanded %s to %s', repr(exclude), repr(exclude_joined))

    # Do ""git rm"".
    try:
        run_command(new_root, ['git', 'rm', '-rf', rel_dest])
    except CalledProcessError as exc:
        raise GitError('""git rm"" failed to remove ' + rel_dest, exc.output)

    # Restore files in exclude.
    run_command(new_root, ['git', 'reset', 'HEAD'] + exclude_joined)
    run_command(new_root, ['git', 'checkout', '--'] + exclude_joined)","Clone ""local_root"" origin into a new directory and check out a specific branch. Optionally run ""git rm"".

    :raise CalledProcessError: Unhandled git command failure.
    :raise GitError: Handled git failures.

    :param str local_root: Local path to git root directory.
    :param str new_root: Local path empty directory in which branch will be cloned into.
    :param str remote: The git remote to clone from to.
    :param str branch: Checkout this branch.
    :param str rel_dest: Run ""git rm"" on this directory if exclude is truthy.
    :param iter exclude: List of strings representing relative file paths to exclude from ""git rm""."
"def _diff(state_data, resource_object):
    '''helper method to compare salt state info with the PagerDuty API json structure,
    and determine if we need to update.

    returns the dict to pass to the PD API to perform the update, or empty dict if no update.
    '''

    state_data['id'] = resource_object['schedule']['id']
    objects_differ = None

    # first check all the easy top-level properties: everything except the schedule_layers.
    for k, v in state_data['schedule'].items():
        if k == 'schedule_layers':
            continue
        if v != resource_object['schedule'][k]:
            objects_differ = '{0} {1} {2}'.format(k, v, resource_object['schedule'][k])
            break

    # check schedule_layers
    if not objects_differ:
        for layer in state_data['schedule']['schedule_layers']:
            # find matching layer name
            resource_layer = None
            for resource_layer in resource_object['schedule']['schedule_layers']:
                found = False
                if layer['name'] == resource_layer['name']:
                    found = True
                    break
            if not found:
                objects_differ = 'layer {0} missing'.format(layer['name'])
                break
            # set the id, so that we will update this layer instead of creating a new one
            layer['id'] = resource_layer['id']
            # compare contents of layer and resource_layer
            for k, v in layer.items():
                if k == 'users':
                    continue
                if k == 'start':
                    continue
                if v != resource_layer[k]:
                    objects_differ = 'layer {0} key {1} {2} != {3}'.format(layer['name'], k, v, resource_layer[k])
                    break
            if objects_differ:
                break
            # compare layer['users']
            if len(layer['users']) != len(resource_layer['users']):
                objects_differ = 'num users in layer {0} {1} != {2}'.format(layer['name'], len(layer['users']), len(resource_layer['users']))
                break

            for user1 in layer['users']:
                found = False
                user2 = None
                for user2 in resource_layer['users']:
                    # deal with PD API bug: when you submit member_order=N, you get back member_order=N+1
                    if user1['member_order'] == user2['member_order'] - 1:
                        found = True
                        break
                if not found:
                    objects_differ = 'layer {0} no one with member_order {1}'.format(layer['name'], user1['member_order'])
                    break
                if user1['user']['id'] != user2['user']['id']:
                    objects_differ = 'layer {0} user at member_order {1} {2} != {3}'.format(layer['name'],
                                                                                            user1['member_order'],
                                                                                            user1['user']['id'],
                                                                                            user2['user']['id'])
                    break
    if objects_differ:
        return state_data
    else:
        return {}","helper method to compare salt state info with the PagerDuty API json structure,
    and determine if we need to update.

    returns the dict to pass to the PD API to perform the update, or empty dict if no update."
"def setColor(self, poiID, color):
        """"""setColor(string, (integer, integer, integer, integer)) -> None

        Sets the rgba color of the poi.
        """"""
        self._connection._beginMessage(
            tc.CMD_SET_POI_VARIABLE, tc.VAR_COLOR, poiID, 1 + 1 + 1 + 1 + 1)
        self._connection._string += struct.pack(""!BBBBB"", tc.TYPE_COLOR, int(
            color[0]), int(color[1]), int(color[2]), int(color[3]))
        self._connection._sendExact()","setColor(string, (integer, integer, integer, integer)) -> None

        Sets the rgba color of the poi."
"def round_to(self, dt, hour, minute, second, mode=""floor""):
        """"""Round the given datetime to specified hour, minute and second.

        :param mode: 'floor' or 'ceiling'

        ****

        , , 
        """"""
        mode = mode.lower()

        new_dt = datetime(dt.year, dt.month, dt.day, hour, minute, second)
        if mode == ""floor"":
            if new_dt <= dt:
                return new_dt
            else:
                return rolex.add_days(new_dt, -1)
        elif mode == ""ceiling"":
            if new_dt >= dt:
                return new_dt
            else:
                return rolex.add_days(new_dt, 1)
        else:
            raise ValueError(""'mode' has to be 'floor' or 'ceiling'!"")","Round the given datetime to specified hour, minute and second.

        :param mode: 'floor' or 'ceiling'

        ****

        , , "
"def get_mac_address_table_input_request_type_get_request_mac_address(self, **kwargs):
        """"""Auto Generated Code
        """"""
        config = ET.Element(""config"")
        get_mac_address_table = ET.Element(""get_mac_address_table"")
        config = get_mac_address_table
        input = ET.SubElement(get_mac_address_table, ""input"")
        request_type = ET.SubElement(input, ""request-type"")
        get_request = ET.SubElement(request_type, ""get-request"")
        mac_address = ET.SubElement(get_request, ""mac-address"")
        mac_address.text = kwargs.pop('mac_address')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)",Auto Generated Code
"def _get_firmware_file(path):
    """"""Gets the raw firmware file

    Gets the raw firmware file from the extracted directory structure
    :param path: the directory structure to search for
    :returns: the raw firmware file with the complete path
    """"""
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            file_name, file_ext = os.path.splitext(os.path.basename(filename))
            if file_ext in RAW_FIRMWARE_EXTNS:
                # return filename
                return os.path.join(dirpath, filename)","Gets the raw firmware file

    Gets the raw firmware file from the extracted directory structure
    :param path: the directory structure to search for
    :returns: the raw firmware file with the complete path"
"def process_document(self, doc):
        """"""
        Add your code for processing the document
        """"""

        raw = doc.select_segments(""$.raw_content"")[0]
        extractions = doc.extract(self.inferlink_extractor, raw)
        doc.store(extractions, ""inferlink_extraction"")
        return list()",Add your code for processing the document
"def polygonVertices(x, y, radius, sides, rotationDegrees=0, stretchHorizontal=1.0, stretchVertical=1.0):
    """"""
    Returns a generator that produces the (x, y) points of the vertices of a regular polygon.
    `x` and `y` mark the center of the polygon, `radius` indicates the size,
    `sides` specifies what kind of polygon it is.

    Odd-sided polygons have a pointed corner at the top and flat horizontal
    side at the bottom. The `rotationDegrees` argument will rotate the polygon
    counterclockwise.

    The polygon can be stretched by passing `stretchHorizontal` or `stretchVertical`
    arguments. Passing `2.0` for `stretchHorizontal`, for example, will double with
    width of the polygon.

    If `filled` is set to `True`, the generator will also produce the interior
    (x, y) points.

    (Note: The `thickness` parameter is not yet implemented.)

    >>> list(polygonVertices(10, 10, 8, 5))
    [(10, 2.0), (3, 8.0), (6, 16.0), (14, 16.0), (17, 8.0)]
    >>> drawPoints(polygonVertices(10, 10, 8, 5))
    ,,,,,,,O,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    O,,,,,,,,,,,,,O
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,O,,,,,,,O,,,
    >>> drawPoints(polygonVertices(10, 10, 8, 5, rotationDegrees=20))
    ,,,,,O,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,O
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    O,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,O
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,O,,,,,,,,
    """"""

    # TODO - validate x, y, radius, sides

    # Setting the start point like this guarantees a flat side will be on the ""bottom"" of the polygon.
    if sides % 2 == 1:
        angleOfStartPointDegrees = 90 + rotationDegrees
    else:
        angleOfStartPointDegrees = 90 + rotationDegrees - (180 / sides)

    for sideNum in range(sides):
        angleOfPointRadians = math.radians(angleOfStartPointDegrees + (360 / sides * sideNum))

        yield (  int(math.cos(angleOfPointRadians) * radius  * stretchHorizontal) + x,
               -(int(math.sin(angleOfPointRadians) * radius) * stretchVertical)   + y)","Returns a generator that produces the (x, y) points of the vertices of a regular polygon.
    `x` and `y` mark the center of the polygon, `radius` indicates the size,
    `sides` specifies what kind of polygon it is.

    Odd-sided polygons have a pointed corner at the top and flat horizontal
    side at the bottom. The `rotationDegrees` argument will rotate the polygon
    counterclockwise.

    The polygon can be stretched by passing `stretchHorizontal` or `stretchVertical`
    arguments. Passing `2.0` for `stretchHorizontal`, for example, will double with
    width of the polygon.

    If `filled` is set to `True`, the generator will also produce the interior
    (x, y) points.

    (Note: The `thickness` parameter is not yet implemented.)

    >>> list(polygonVertices(10, 10, 8, 5))
    [(10, 2.0), (3, 8.0), (6, 16.0), (14, 16.0), (17, 8.0)]
    >>> drawPoints(polygonVertices(10, 10, 8, 5))
    ,,,,,,,O,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    O,,,,,,,,,,,,,O
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,,
    ,,,O,,,,,,,O,,,
    >>> drawPoints(polygonVertices(10, 10, 8, 5, rotationDegrees=20))
    ,,,,,O,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,O
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    O,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,O
    ,,,,,,,,,,,,,,
    ,,,,,,,,,,,,,,
    ,,,,,O,,,,,,,,"
"def _get_batch(self):
    """"""Returns a batch of sequences.

    Returns:
      obs: np.int32 array of size [Time, Batch]
      target: np.int32 array of size [Time, Batch]
    """"""
    batch_indices = np.mod(
        np.array([
            np.arange(head_index, head_index + self._num_steps + 1) for
            head_index in self._head_indices]),
        self._n_flat_elements)

    obs = np.array([
        self._flat_data[indices[:self._num_steps]]
        for indices in batch_indices]).T
    target = np.array([
        self._flat_data[indices[1:self._num_steps + 1]]
        for indices in batch_indices]).T

    if self._random_sampling:
      self._reset_head_indices()
    else:
      self._head_indices = np.mod(
          self._head_indices + self._num_steps, self._n_flat_elements)
    return obs, target","Returns a batch of sequences.

    Returns:
      obs: np.int32 array of size [Time, Batch]
      target: np.int32 array of size [Time, Batch]"
"def _build_schema(self, s):
        """"""Recursive schema builder, called by `json_schema`.
        """"""
        w = self._whatis(s)
        if w == self.IS_LIST:
            w0 = self._whatis(s[0])
            js = {""type"": ""array"",
                  ""items"": {""type"": self._jstype(w0, s[0])}}
        elif w == self.IS_DICT:
            js = {""type"": ""object"",
                  ""properties"": {key: self._build_schema(val) for key, val in s.items()}}
            req = [key for key, val in s.items() if not val.is_optional]
            if req:
                js[""required""] = req
        else:
            js = {""type"": self._jstype(w, s)}
        for k, v in self._json_schema_keys.items():
            if k not in js:
                js[k] = v
        return js","Recursive schema builder, called by `json_schema`."
"def check_ups_input_frequency(the_session, the_helper, the_snmp_value):
    """"""
    OID .1.3.6.1.2.1.33.1.3.3.1.2.1
    MIB excerpt
    The present input frequency.
    """"""
    a_frequency = calc_frequency_from_snmpvalue(the_snmp_value)
    the_helper.add_metric(
        label=the_helper.options.type,
        value=a_frequency,
        uom='Hz')

    the_helper.set_summary(""Input Frequency is {} Hz"".format(a_frequency))","OID .1.3.6.1.2.1.33.1.3.3.1.2.1
    MIB excerpt
    The present input frequency."
"def tag(self, repository_tag, tags=[]):
        """"""
        Tags image with one or more tags.

        Raises exception on failure.
        """"""
        if not isinstance(repository_tag, six.string_types):
            raise TypeError('repository_tag must be a string')

        if not isinstance(tags, list):
            raise TypeError('tags must be a list.')

        if ':' in repository_tag:
            repository, tag = repository_tag.split(':')
            tags.append(tag)
        else:
            repository = repository_tag

            if not tags:
                tags.append('latest')

        for tag in tags:
            repo_tag = ""{0}:{1}"".format(repository, tag)

            if repo_tag not in self.repo_tags:
                logger.info(""Tagging Image: {0} Repo Tag: {1}"".format(self.identifier, repo_tag))
                self.repo_tags = self.repo_tags + (repo_tag, )

                # always going to force tags until a feature is added to allow users to specify.
                try:
                    self.client.tag(self.id, repository, tag)
                except:
                    self.client.tag(self.id, repository, tag, force=True)","Tags image with one or more tags.

        Raises exception on failure."
"def is_versionable(brain_or_object, policy='at_edit_autoversion'):
    """"""Checks if the passed in object is versionable.

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: True if the object is versionable
    :rtype: bool
    """"""
    pr = get_tool(""portal_repository"")
    obj = get_object(brain_or_object)
    return pr.supportsPolicy(obj, 'at_edit_autoversion') \
        and pr.isVersionable(obj)","Checks if the passed in object is versionable.

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: True if the object is versionable
    :rtype: bool"
"def _get_hit_count(self, database, enquire):
        """"""
        Given a database and enquire instance, returns the estimated number
        of matches.

        Required arguments:
            `database` -- The database to be queried
            `enquire` -- The enquire instance
        """"""
        return self._get_enquire_mset(
            database, enquire, 0, database.get_doccount()
        ).size()","Given a database and enquire instance, returns the estimated number
        of matches.

        Required arguments:
            `database` -- The database to be queried
            `enquire` -- The enquire instance"
"def return_dat(self, chan, begsam, endsam):
        """"""Return the data as 2D numpy.ndarray.

        Parameters
        ----------
        chan : int or list
            index (indices) of the channels to read
        begsam : int
            index of the first sample
        endsam : int
            index of the last sample

        Returns
        -------
        numpy.ndarray
            A 2d matrix, with dimension chan X samples. To save memory, the
            data are memory-mapped, and you cannot change the values on disk.

        Notes
        -----
        When asking for an interval outside the data boundaries, it returns NaN
        for those values.
        """"""
        data = memmap(self.filename, dtype=self.dtype, mode='r', order='F',
                      shape=(self.n_chan, self.n_samples), offset=self.head)

        dat = data[chan, max((begsam, 0)):min((endsam, self.n_samples))].astype(float64)
        dat = (dat + self.offset[chan, :]) * self.gain[chan, :]

        if begsam < 0:

            pad = empty((dat.shape[0], 0 - begsam))
            pad.fill(NaN)
            dat = c_[pad, dat]

        if endsam >= self.n_samples:

            pad = empty((dat.shape[0], endsam - self.n_samples))
            pad.fill(NaN)
            dat = c_[dat, pad]

        return dat","Return the data as 2D numpy.ndarray.

        Parameters
        ----------
        chan : int or list
            index (indices) of the channels to read
        begsam : int
            index of the first sample
        endsam : int
            index of the last sample

        Returns
        -------
        numpy.ndarray
            A 2d matrix, with dimension chan X samples. To save memory, the
            data are memory-mapped, and you cannot change the values on disk.

        Notes
        -----
        When asking for an interval outside the data boundaries, it returns NaN
        for those values."
"def save_state(state, output_dir, keep=False):
  """"""Save State and optionally gin config.""""""
  params_file = os.path.join(output_dir, ""model.pkl"")
  with gfile.GFile(params_file, ""wb"") as f:
    pickle.dump((state.params, state.step, state.history), f)
  if keep:
    params_file = os.path.join(output_dir, ""model_{}.pkl"".format(state.step))
    with gfile.GFile(params_file, ""wb"") as f:
      pickle.dump((state.params, state.step, state.history), f)
  log(""Model saved to %s"" % params_file, stdout=False)",Save State and optionally gin config.
"def _upgrade_db(self):
        """"""upgrade db using scripts for specified (current) schema version""""""
        migration_path = ""_data/migrations""
        sqlite3.connect(self._db_path).close()    # ensure that it exists
        db_url = ""sqlite:///"" + self._db_path
        backend = yoyo.get_backend(db_url)
        migration_dir = pkg_resources.resource_filename(__package__, migration_path)
        migrations = yoyo.read_migrations(migration_dir)
        assert len(migrations) > 0, ""no migration scripts found -- wrong migraion path for "" + __package__
        migrations_to_apply = backend.to_apply(migrations)
        backend.apply_migrations(migrations_to_apply)",upgrade db using scripts for specified (current) schema version
"def cache_files(self, paths, saltenv='base', cachedir=None):
        '''
        Download a list of files stored on the master and put them in the
        minion file cache
        '''
        ret = []
        if isinstance(paths, six.string_types):
            paths = paths.split(',')
        for path in paths:
            ret.append(self.cache_file(path, saltenv, cachedir=cachedir))
        return ret","Download a list of files stored on the master and put them in the
        minion file cache"
"def analyze_log(fp, configs, url_rules):
    """"""Analyze log file""""""
    url_classifier = URLClassifier(url_rules)
    analyzer = LogAnalyzer(url_classifier=url_classifier, min_msecs=configs.min_msecs)
    for line in fp:
        analyzer.analyze_line(line)
    return analyzer.get_data()",Analyze log file
"def encode(data):
    '''
    bytes -> str
    '''
    if riemann.network.CASHADDR_PREFIX is None:
        raise ValueError('Network {} does not support cashaddresses.'
                         .format(riemann.get_current_network_name()))

    data = convertbits(data, 8, 5)
    checksum = calculate_checksum(riemann.network.CASHADDR_PREFIX, data)

    payload = b32encode(data + checksum)

    form = '{prefix}:{payload}'
    return form.format(
        prefix=riemann.network.CASHADDR_PREFIX,
        payload=payload)",bytes -> str
"def add_done_callback(self, callback, *args):
        """"""Add a callback that gets called when the future completes.

        The callback will be called in the context of the fiber that sets the
        future's result. The callback is called with the positional arguments
        *args* provided to this method.

        The return value is an opaque handle that can be used with
        :meth:`~gruvi.Future.remove_done_callback` to remove the callback.

        If the future has already completed, then the callback is called
        immediately from this method and the return value will be ``None``.
        """"""
        with self._lock:
            if self._state not in (self.S_DONE, self.S_EXCEPTION):
                return add_callback(self, callback, args)
        callback(*args)","Add a callback that gets called when the future completes.

        The callback will be called in the context of the fiber that sets the
        future's result. The callback is called with the positional arguments
        *args* provided to this method.

        The return value is an opaque handle that can be used with
        :meth:`~gruvi.Future.remove_done_callback` to remove the callback.

        If the future has already completed, then the callback is called
        immediately from this method and the return value will be ``None``."
"def merge(self, query):
        """"""
        Merge current query with another.

        :param query: The query to merge with
        :type query: QueryBuilder
        """"""
        self.columns += query.columns
        self.joins += query.joins
        self.wheres += query.wheres
        self.groups += query.groups
        self.havings += query.havings
        self.orders += query.orders
        self.distinct_ = query.distinct_

        if self.columns:
            self.columns = Collection(self.columns).unique().all()

        if query.limit_:
            self.limit_ = query.limit_

        if query.offset_:
            self.offset_ = None

        self.unions += query.unions

        if query.union_limit:
            self.union_limit = query.union_limit

        if query.union_offset:
            self.union_offset = query.union_offset

        self.union_orders += query.union_orders

        self.merge_bindings(query)","Merge current query with another.

        :param query: The query to merge with
        :type query: QueryBuilder"
"def is_handleable(self, device):
        # TODO: handle pathes in first argument
        """"""
        Check whether this device should be handled by udiskie.

        :param device: device object, block device path or mount path
        :returns: handleability

        Currently this just means that the device is removable and holds a
        filesystem or the device is a LUKS encrypted volume.
        """"""
        ignored = self._ignore_device(device)
        # propagate handleability of parent devices:
        if ignored is None and device is not None:
            return self.is_handleable(_get_parent(device))
        return not ignored","Check whether this device should be handled by udiskie.

        :param device: device object, block device path or mount path
        :returns: handleability

        Currently this just means that the device is removable and holds a
        filesystem or the device is a LUKS encrypted volume."
"def convert_to_ns(self, value):
        ''' converts a value to the prefixed rdf ns equivalent. If not found
            returns the value as is

        args:
            value: the value to convert
        '''
        parsed = self.parse_uri(value)

        try:
            rtn_val = ""%s_%s"" % (self.uri_dict[parsed[0]], parsed[1])
        except KeyError:
            rtn_val = self.pyhttp(value)
        return rtn_val","converts a value to the prefixed rdf ns equivalent. If not found
            returns the value as is

        args:
            value: the value to convert"
"def get_series_by_name(self, series_name):
        """"""Perform lookup for series

        :param str series_name: series name found within filename
        :returns: instance of series
        :rtype: object
        """"""
        series = trakt.Trakt['search'].query(series_name, 'show')
        if not series:
            return None, 'Not Found'
        return series[0], None","Perform lookup for series

        :param str series_name: series name found within filename
        :returns: instance of series
        :rtype: object"
"def raw(self, command, arguments, queue=None, max_time=None, stream=False, tags=None, id=None):
        """"""
        Implements the low level command call, this needs to build the command structure
        and push it on the correct queue.

        :param command: Command name to execute supported by the node (ex: core.system, info.cpu, etc...)
                        check documentation for list of built in commands
        :param arguments: A dict of required command arguments depends on the command name.
        :param queue: command queue (commands on the same queue are executed sequentially)
        :param max_time: kill job server side if it exceeded this amount of seconds
        :param stream: If True, process stdout and stderr are pushed to a special queue (stream:<id>) so
            client can stream output
        :param tags: job tags
        :param id: job id. Generated if not supplied
        :return: Response object
        """"""
        args = {
            'container': self._container,
            'command': {
                'command': command,
                'arguments': arguments,
                'queue': queue,
                'max_time': max_time,
                'stream': stream,
                'tags': tags,
                'id': id,
            },
        }

        # check input
        self._raw_chk.check(args)

        response = self._client.raw('corex.dispatch', args)

        result = response.get()
        if result.state != 'SUCCESS':
            raise RuntimeError('failed to dispatch command to container: %s' % result.data)

        cmd_id = json.loads(result.data)
        return self._client.response_for(cmd_id)","Implements the low level command call, this needs to build the command structure
        and push it on the correct queue.

        :param command: Command name to execute supported by the node (ex: core.system, info.cpu, etc...)
                        check documentation for list of built in commands
        :param arguments: A dict of required command arguments depends on the command name.
        :param queue: command queue (commands on the same queue are executed sequentially)
        :param max_time: kill job server side if it exceeded this amount of seconds
        :param stream: If True, process stdout and stderr are pushed to a special queue (stream:<id>) so
            client can stream output
        :param tags: job tags
        :param id: job id. Generated if not supplied
        :return: Response object"
"def _EntriesGenerator(self):
    """"""Retrieves directory entries.

    Since a directory can contain a vast number of entries using
    a generator is more memory efficient.

    Yields:
      NTFSPathSpec: NTFS path specification.
    """"""
    try:
      fsntfs_file_entry = self._file_system.GetNTFSFileEntryByPathSpec(
          self.path_spec)
    except errors.PathSpecError:
      fsntfs_file_entry = None

    if fsntfs_file_entry:
      location = getattr(self.path_spec, 'location', None)

      for fsntfs_sub_file_entry in fsntfs_file_entry.sub_file_entries:
        directory_entry = fsntfs_sub_file_entry.name

        # Ignore references to self or parent.
        if directory_entry in ('.', '..'):
          continue

        file_reference = fsntfs_sub_file_entry.file_reference
        directory_entry_mft_entry = (
            file_reference & _FILE_REFERENCE_MFT_ENTRY_BITMASK)

        if location == self._file_system.PATH_SEPARATOR:
          directory_entry = self._file_system.JoinPath([directory_entry])
        else:
          directory_entry = self._file_system.JoinPath([
              location, directory_entry])

        yield ntfs_path_spec.NTFSPathSpec(
            location=directory_entry,
            mft_attribute=fsntfs_sub_file_entry.name_attribute_index,
            mft_entry=directory_entry_mft_entry, parent=self.path_spec.parent)","Retrieves directory entries.

    Since a directory can contain a vast number of entries using
    a generator is more memory efficient.

    Yields:
      NTFSPathSpec: NTFS path specification."
"def is_syscall_addr(self, addr):
        """"""
        Return whether or not the given address corresponds to a syscall implementation.
        """"""
        if self.kernel_base is None or addr < self.kernel_base:
            return False

        addr -= self.kernel_base

        if addr % self.syscall_addr_alignment != 0:
            return False

        addr //= self.syscall_addr_alignment
        return addr <= self.unknown_syscall_number",Return whether or not the given address corresponds to a syscall implementation.
"def compare_variant_type_plot(data):
    """""" Return HTML for the Variant Counts barplot """"""
    keys = OrderedDict()
    keys['snps'] = {'name': 'SNPs', 'color': '#7cb5ec'}
    keys['indels'] = {'name': 'InDels', 'color': '#90ed7d'}
    keys['multiallelic_snps'] = {'name': 'multi-allelic SNP', 'color': 'orange'}
    keys['complex_indels'] = {'name': 'Complex InDels', 'color': '#8085e9'}

    total_variants = dict()
    known_variants = dict()
    novel_variants = dict()
    for s_name, values in data.items():
        total_variants[s_name] = {
            'snps': values['TOTAL_SNPS'],
            'indels': values['TOTAL_INDELS'],
            'multiallelic_snps': values['TOTAL_MULTIALLELIC_SNPS'],
            'complex_indels': values['TOTAL_COMPLEX_INDELS'],
        }

        known_variants[s_name] = {
            'snps': values['NUM_IN_DB_SNP'],
            'indels': int(values['TOTAL_INDELS'])-int(values['NOVEL_INDELS']),
            'multiallelic_snps': values['NUM_IN_DB_SNP_MULTIALLELIC'],
            'complex_indels': values['NUM_IN_DB_SNP_COMPLEX_INDELS'],
        }

        novel_variants[s_name] = {
            'snps': values['NOVEL_SNPS'],
            'indels': values['NOVEL_INDELS'],
            'multiallelic_snps': int(values['TOTAL_MULTIALLELIC_SNPS'])-int(values['NUM_IN_DB_SNP_MULTIALLELIC']),
            'complex_indels': int(values['TOTAL_COMPLEX_INDELS'])-int(values['NUM_IN_DB_SNP_COMPLEX_INDELS']),
        }

    plot_conf = {
        'id': 'picard_variantCallingMetrics_variant_type',
        'title': 'Picard: Variants Called',
        'ylab': 'Counts of Variants',
        'hide_zero_cats': False,
        'data_labels': [
            {'name': 'Total'},
            {'name': 'Known'},
            {'name': 'Novel'}
        ],
    }
    return bargraph.plot(data=[total_variants, known_variants, novel_variants],
                         cats=[keys, keys, keys],
                         pconfig=plot_conf)",Return HTML for the Variant Counts barplot
"def mpris(self):
        """"""
        Get the current output format and return it.
        """"""
        if self._kill:
            raise KeyboardInterrupt
        current_player_id = self._player_details.get(""id"")
        cached_until = self.py3.CACHE_FOREVER

        if self._player is None:
            text = self.format_none
            color = self.py3.COLOR_BAD
            composite = [{""full_text"": text, ""color"": color}]
            self._data = {}
        else:
            self._init_data()
            (text, color, cached_until) = self._get_text()
            self._control_states = self._get_control_states()
            buttons = self._get_response_buttons()
            composite = self.py3.safe_format(self.format, dict(text, **buttons))

        if self._data.get(
            ""error_occurred""
        ) or current_player_id != self._player_details.get(""id""):
            # Something went wrong or the player changed during our processing
            # This is usually due to something like a player being killed
            # whilst we are checking its details
            # Retry but limit the number of attempts
            self._tries += 1
            if self._tries < 3:
                return self.mpris()
            # Max retries hit we need to output something
            composite = [
                {""full_text"": ""Something went wrong"", ""color"": self.py3.COLOR_BAD}
            ]
            cached_until = self.py3.time_in(1)

        response = {
            ""cached_until"": cached_until,
            ""color"": color,
            ""composite"": composite,
        }

        # we are outputing so reset tries
        self._tries = 0
        return response",Get the current output format and return it.
"def _parse_authors(html_chunk):
    """"""
    Parse authors of the book.

    Args:
        html_chunk (obj): HTMLElement containing slice of the page with details.

    Returns:
        list: List of :class:`structures.Author` objects. Blank if no author \
              found.
    """"""
    authors = html_chunk.match(
        [""div"", {""class"": ""comment""}],
        ""h3"",
        ""a"",
    )

    if not authors:
        return []

    authors = map(
        lambda x: Author(                            # create Author objects
            x.getContent().strip(),
            normalize_url(BASE_URL, x.params.get(""href"", None))
        ),
        authors
    )

    return filter(lambda x: x.name.strip(), authors)","Parse authors of the book.

    Args:
        html_chunk (obj): HTMLElement containing slice of the page with details.

    Returns:
        list: List of :class:`structures.Author` objects. Blank if no author \
              found."
"def get_byte_array(integer):
    """"""Return the variable length bytes corresponding to the given int""""""
    # Operate in big endian (unlike most of Telegram API) since:
    # > ""...pq is a representation of a natural number
    #    (in binary *big endian* format)...""
    # > ""...current value of dh_prime equals
    #    (in *big-endian* byte order)...""
    # Reference: https://core.telegram.org/mtproto/auth_key
    return int.to_bytes(
        integer,
        (integer.bit_length() + 8 - 1) // 8,  # 8 bits per byte,
        byteorder='big',
        signed=False
    )",Return the variable length bytes corresponding to the given int
"def cart2dir(self,cart):
        """"""
        converts a direction to cartesian coordinates
        """"""
#        print ""calling cart2dir(), not in anything""
        cart=numpy.array(cart)
        rad=old_div(numpy.pi,180.) # constant to convert degrees to radians
        if len(cart.shape)>1:
            Xs,Ys,Zs=cart[:,0],cart[:,1],cart[:,2]
        else: #single vector
            Xs,Ys,Zs=cart[0],cart[1],cart[2]
        Rs=numpy.sqrt(Xs**2+Ys**2+Zs**2) # calculate resultant vector length
        Decs=(old_div(numpy.arctan2(Ys,Xs),rad))%360. # calculate declination taking care of correct quadrants (arctan2) and making modulo 360.
        try:
            Incs=old_div(numpy.arcsin(old_div(Zs,Rs)),rad) # calculate inclination (converting to degrees) #
        except:
            print('trouble in cart2dir') # most likely division by zero somewhere
            return numpy.zeros(3)

        return numpy.array([Decs,Incs,Rs]).transpose()",converts a direction to cartesian coordinates
"def get_partial_object(self, bucket_name, object_name, offset=0, length=0, request_headers=None, sse=None):
        """"""
        Retrieves an object from a bucket.

        Optionally takes an offset and length of data to retrieve.

        This function returns an object that contains an open network
        connection to enable incremental consumption of the
        response. To re-use the connection (if desired) on subsequent
        requests, the user needs to call `release_conn()` on the
        returned object after processing.

        Examples:
            partial_object = minio.get_partial_object('foo', 'bar', 2, 4)

        :param bucket_name: Bucket to retrieve object from
        :param object_name: Name of object to retrieve
        :param offset: Optional offset to retrieve bytes from.
           Must be >= 0.
        :param length: Optional number of bytes to retrieve.
           Must be an integer.
        :param request_headers: Any additional headers to be added with GET request.
        :return: :class:`urllib3.response.HTTPResponse` object.

        """"""
        is_valid_bucket_name(bucket_name)
        is_non_empty_string(object_name)

        return self._get_partial_object(bucket_name,
                                        object_name,
                                        offset, length,
                                        request_headers=request_headers,
                                        sse=sse)","Retrieves an object from a bucket.

        Optionally takes an offset and length of data to retrieve.

        This function returns an object that contains an open network
        connection to enable incremental consumption of the
        response. To re-use the connection (if desired) on subsequent
        requests, the user needs to call `release_conn()` on the
        returned object after processing.

        Examples:
            partial_object = minio.get_partial_object('foo', 'bar', 2, 4)

        :param bucket_name: Bucket to retrieve object from
        :param object_name: Name of object to retrieve
        :param offset: Optional offset to retrieve bytes from.
           Must be >= 0.
        :param length: Optional number of bytes to retrieve.
           Must be an integer.
        :param request_headers: Any additional headers to be added with GET request.
        :return: :class:`urllib3.response.HTTPResponse` object."
"def add_user_role(self, user, role_name):
        """"""Associate a role name with a user.""""""

        # For SQL: user.roles is list of pointers to Role objects
        if isinstance(self.db_adapter, SQLDbAdapter):
            # user.roles is a list of Role IDs
            # Get or add role
            role = self.db_adapter.find_first_object(self.RoleClass, name=role_name)
            if not role:
                role = self.RoleClass(name=role_name)
                self.db_adapter.add_object(role)
            user.roles.append(role)

        # For others: user.roles is a list of role names
        else:
            # user.roles is a list of role names
            user.roles.append(role_name)",Associate a role name with a user.
"def ExtractEvents(self, parser_mediator, registry_key, **kwargs):
    """"""Extracts events from a Windows Registry key.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.
    """"""
    values_dict = {}
    for registry_value in registry_key.GetValues():
      if not registry_value.name or not registry_value.data:
        continue

      if registry_value.name == 'UpdateKey':
        self._ParseUpdateKeyValue(
            parser_mediator, registry_value, registry_key.path)
      else:
        values_dict[registry_value.name] = registry_value.GetDataAsObject()

    event_data = windows_events.WindowsRegistryEventData()
    event_data.key_path = registry_key.path
    event_data.offset = registry_key.offset
    event_data.regvalue = values_dict
    event_data.source_append = self._SOURCE_APPEND
    event_data.urls = self.URLS

    event = time_events.DateTimeValuesEvent(
        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)
    parser_mediator.ProduceEventWithEventData(event, event_data)","Extracts events from a Windows Registry key.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      registry_key (dfwinreg.WinRegistryKey): Windows Registry key."
"def set_controller(self, controllers):
        """"""
        Sets the OpenFlow controller address.

        This method is corresponding to the following ovs-vsctl command::

            $ ovs-vsctl set-controller <bridge> <target>...
        """"""
        command = ovs_vsctl.VSCtlCommand('set-controller', [self.br_name])
        command.args.extend(controllers)
        self.run_command([command])","Sets the OpenFlow controller address.

        This method is corresponding to the following ovs-vsctl command::

            $ ovs-vsctl set-controller <bridge> <target>..."
"def check_for_lounge_upgrade(self, email, password):
        """"""Check the CDRouter Support Lounge for eligible upgrades using your
        Support Lounge email & password.

        :param email: CDRouter Support Lounge email as a string.
        :param password: CDRouter Support Lounge password as a string.
        :return: :class:`system.Release <system.Release>` object
        :rtype: system.Release
        """"""
        schema = ReleaseSchema()
        resp = self.service.post(self.base+'lounge/check/',
                                 json={'email': email, 'password': password})
        return self.service.decode(schema, resp)","Check the CDRouter Support Lounge for eligible upgrades using your
        Support Lounge email & password.

        :param email: CDRouter Support Lounge email as a string.
        :param password: CDRouter Support Lounge password as a string.
        :return: :class:`system.Release <system.Release>` object
        :rtype: system.Release"
"def monitor(name):
    '''
    Get the summary from module monit and try to see if service is
    being monitored. If not then monitor the service.
    '''
    ret = {'result': None,
           'name': name,
           'comment': '',
           'changes': {}
           }
    result = __salt__['monit.summary'](name)

    try:
        for key, value in result.items():
            if 'Running' in value[name]:
                ret['comment'] = ('{0} is being being monitored.').format(name)
                ret['result'] = True
            else:
                if __opts__['test']:
                    ret['comment'] = 'Service {0} is set to be monitored.'.format(name)
                    ret['result'] = None
                    return ret
                __salt__['monit.monitor'](name)
                ret['comment'] = ('{0} started to be monitored.').format(name)
                ret['changes'][name] = 'Running'
                ret['result'] = True
                break
    except KeyError:
        ret['comment'] = ('{0} not found in configuration.').format(name)
        ret['result'] = False

    return ret","Get the summary from module monit and try to see if service is
    being monitored. If not then monitor the service."
"def readehf(filename):
    """"""Read EDF header (ESRF data format, as of beamline ID01 and ID02)

    Input
    -----
    filename: string
        the file name to load

    Output
    ------
    the EDF header structure in a dictionary
    """"""
    f = open(filename, 'r')
    edf = {}
    if not f.readline().strip().startswith('{'):
        raise ValueError('Invalid file format.')
    for l in f:
        l = l.strip()
        if not l:
            continue
        if l.endswith('}'):
            break  # last line of header
        try:
            left, right = l.split('=', 1)
        except ValueError:
            raise ValueError('Invalid line: ' + l)
        left = left.strip()
        right = right.strip()
        if not right.endswith(';'):
            raise ValueError(
                'Invalid line (does not end with a semicolon): ' + l)
        right = right[:-1].strip()
        m = re.match('^(?P<left>.*)~(?P<continuation>\d+)$', left)
        if m is not None:
            edf[m.group('left')] = edf[m.group('left')] + right
        else:
            edf[left] = _readedf_extractline(left, right)
    f.close()
    edf['FileName'] = filename
    edf['__Origin__'] = 'EDF ID02'
    edf['__particle__'] = 'photon'
    return edf","Read EDF header (ESRF data format, as of beamline ID01 and ID02)

    Input
    -----
    filename: string
        the file name to load

    Output
    ------
    the EDF header structure in a dictionary"
"def user_login_failed(
            self,
            sender,
            credentials: dict,
            request: AxesHttpRequest = None,
            **kwargs
    ):  # pylint: disable=too-many-locals
        """"""
        When user login fails, save attempt record in cache and lock user out if necessary.

        :raises AxesSignalPermissionDenied: if user should be locked out.
        """"""

        if request is None:
            log.error('AXES: AxesCacheHandler.user_login_failed does not function without a request.')
            return

        username = get_client_username(request, credentials)
        client_str = get_client_str(username, request.axes_ip_address, request.axes_user_agent, request.axes_path_info)

        if self.is_whitelisted(request, credentials):
            log.info('AXES: Login failed from whitelisted client %s.', client_str)
            return

        failures_since_start = 1 + self.get_failures(request, credentials)

        if failures_since_start > 1:
            log.warning(
                'AXES: Repeated login failure by %s. Count = %d of %d. Updating existing record in the cache.',
                client_str,
                failures_since_start,
                settings.AXES_FAILURE_LIMIT,
            )
        else:
            log.warning(
                'AXES: New login failure by %s. Creating new record in the cache.',
                client_str,
            )

        cache_key = get_client_cache_key(request, credentials)
        self.cache.set(cache_key, failures_since_start, self.cache_timeout)

        if failures_since_start >= settings.AXES_FAILURE_LIMIT:
            log.warning('AXES: Locking out %s after repeated login failures.', client_str)

            user_locked_out.send(
                'axes',
                request=request,
                username=username,
                ip_address=request.axes_ip_address,
            )

            raise AxesSignalPermissionDenied('Locked out due to repeated login failures.')","When user login fails, save attempt record in cache and lock user out if necessary.

        :raises AxesSignalPermissionDenied: if user should be locked out."
"def atoms_iter(self):
        """"""Iterate over atoms.""""""
        for n, atom in self.graph.nodes.data(""atom""):
            yield n, atom",Iterate over atoms.
"def run_step(context):
    """"""pypyr step saves current utc datetime to context.

    Args:
        context: pypyr.context.Context. Mandatory.
                 The following context key is optional:
                - nowUtcIn. str. Datetime formatting expression. For full list
                  of possible expressions, check here:
                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior

    All inputs support pypyr formatting expressions.

    This step creates now in context, containing a string representation of the
    timestamp. If input formatting not specified, defaults to ISO8601.

    Default is:
    YYYY-MM-DDTHH:MM:SS.ffffff+00:00, or, if microsecond is 0,
    YYYY-MM-DDTHH:MM:SS

    Returns:
        None. updates context arg.

    """"""
    logger.debug(""started"")

    format_expression = context.get('nowUtcIn', None)

    if format_expression:
        formatted_expression = context.get_formatted_string(format_expression)
        context['nowUtc'] = datetime.now(
            timezone.utc).strftime(formatted_expression)
    else:
        context['nowUtc'] = datetime.now(timezone.utc).isoformat()

    logger.info(f""timestamp {context['nowUtc']} saved to context nowUtc"")
    logger.debug(""done"")","pypyr step saves current utc datetime to context.

    Args:
        context: pypyr.context.Context. Mandatory.
                 The following context key is optional:
                - nowUtcIn. str. Datetime formatting expression. For full list
                  of possible expressions, check here:
                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior

    All inputs support pypyr formatting expressions.

    This step creates now in context, containing a string representation of the
    timestamp. If input formatting not specified, defaults to ISO8601.

    Default is:
    YYYY-MM-DDTHH:MM:SS.ffffff+00:00, or, if microsecond is 0,
    YYYY-MM-DDTHH:MM:SS

    Returns:
        None. updates context arg."
"def set_object(self, object, logmsg=None):  # @ReservedAssignment
        """"""Set the object we point to, possibly dereference our symbolic reference first.
        If the reference does not exist, it will be created

        :param object: a refspec, a SymbolicReference or an Object instance. SymbolicReferences
            will be dereferenced beforehand to obtain the object they point to
        :param logmsg: If not None, the message will be used in the reflog entry to be
            written. Otherwise the reflog is not altered
        :note: plain SymbolicReferences may not actually point to objects by convention
        :return: self""""""
        if isinstance(object, SymbolicReference):
            object = object.object  # @ReservedAssignment
        # END resolve references

        is_detached = True
        try:
            is_detached = self.is_detached
        except ValueError:
            pass
        # END handle non-existing ones

        if is_detached:
            return self.set_reference(object, logmsg)

        # set the commit on our reference
        return self._get_reference().set_object(object, logmsg)","Set the object we point to, possibly dereference our symbolic reference first.
        If the reference does not exist, it will be created

        :param object: a refspec, a SymbolicReference or an Object instance. SymbolicReferences
            will be dereferenced beforehand to obtain the object they point to
        :param logmsg: If not None, the message will be used in the reflog entry to be
            written. Otherwise the reflog is not altered
        :note: plain SymbolicReferences may not actually point to objects by convention
        :return: self"
"def _get_intra_event_phi(self, C, mag, rjb, vs30, num_sites):
        """"""
        Returns the intra-event standard deviation (phi), dependent on
        magnitude, distance and vs30
        """"""
        base_vals = np.zeros(num_sites)
        # Magnitude Dependent phi (Equation 17)
        if mag <= 4.5:
            base_vals += C[""f1""]
        elif mag >= 5.5:
            base_vals += C[""f2""]
        else:
            base_vals += (C[""f1""] + (C[""f2""] - C[""f1""]) * (mag - 4.5))
        # Distance dependent phi (Equation 16)
        idx1 = rjb > C[""R2""]
        base_vals[idx1] += C[""DfR""]
        idx2 = np.logical_and(rjb > C[""R1""], rjb <= C[""R2""])
        base_vals[idx2] += (C[""DfR""] * (np.log(rjb[idx2] / C[""R1""]) /
                                        np.log(C[""R2""] / C[""R1""])))
        # Site-dependent phi (Equation 15)
        idx1 = vs30 <= self.CONSTS[""v1""]
        base_vals[idx1] -= C[""DfV""]
        idx2 = np.logical_and(vs30 >= self.CONSTS[""v1""],
                              vs30 <= self.CONSTS[""v2""])
        base_vals[idx2] -= (
            C[""DfV""] * (np.log(self.CONSTS[""v2""] / vs30[idx2]) /
                        np.log(self.CONSTS[""v2""] / self.CONSTS[""v1""])))
        return base_vals","Returns the intra-event standard deviation (phi), dependent on
        magnitude, distance and vs30"
"def user_is(role, get_user=import_user):
    """"""
    Takes an role (a string name of either a role or an ability) and returns the function if the user has that role
    """"""
    def wrapper(func):
        @wraps(func)
        def inner(*args, **kwargs):
            from .models import Role
            current_user = get_user()
            if role in current_user.roles:
                return func(*args, **kwargs)
            raise Forbidden(""You do not have access"")
        return inner
    return wrapper",Takes an role (a string name of either a role or an ability) and returns the function if the user has that role
"def _add_bad_rc(self, rc, result):
        """"""
        Sets an error with a bad return code. Handles 'quiet' logic
        :param rc: The error code
        """"""
        if not rc:
            return

        self.all_ok = False
        if rc == C.LCB_KEY_ENOENT and self._quiet:
            return

        try:
            raise pycbc_exc_lcb(rc)
        except PyCBC.default_exception as e:
            e.all_results = self
            e.key = result.key
            e.result = result
            self._add_err(sys.exc_info())","Sets an error with a bad return code. Handles 'quiet' logic
        :param rc: The error code"
"def p_statement_randomize_expr(p):
    """""" statement : RANDOMIZE expr
    """"""
    p[0] = make_sentence('RANDOMIZE', make_typecast(TYPE.ulong, p[2], p.lineno(1)))",statement : RANDOMIZE expr
"def _set_remote(self, name, url):
        """"""Add remote to the repository. It's equivalent to the command
        git remote add <name> <url>

        If the remote already exists with an other url, it's removed
        and added aggain
        """"""
        remotes = self._get_remotes()
        exising_url = remotes.get(name)
        if exising_url == url:
            logger.info('Remote already exists %s <%s>', name, url)
            return
        if not exising_url:
            logger.info('Adding remote %s <%s>', name, url)
            self.log_call(['git', 'remote', 'add', name, url], cwd=self.cwd)
        else:
            logger.info('Remote remote %s <%s> -> <%s>',
                        name, exising_url, url)
            self.log_call(['git', 'remote', 'rm', name], cwd=self.cwd)
            self.log_call(['git', 'remote', 'add', name, url], cwd=self.cwd)","Add remote to the repository. It's equivalent to the command
        git remote add <name> <url>

        If the remote already exists with an other url, it's removed
        and added aggain"
"def get(self, timeout=None):
        """"""
        Returns the result when it arrives. If timeout is not None and
        the result does not arrive within timeout seconds then
        TimeoutError is raised. If the remote call raised an exception
        then that exception will be reraised by get().
        """"""
        if not self.wait(timeout):
            raise TimeoutError(""Result not available within %fs"" % timeout)
        if self._success:
            return self._data
        raise self._data[0]","Returns the result when it arrives. If timeout is not None and
        the result does not arrive within timeout seconds then
        TimeoutError is raised. If the remote call raised an exception
        then that exception will be reraised by get()."
"def angle2d(self):
        """"""determine the angle of this point on a circle, measured in radians (presume values represent a Vector)""""""
        if self.x==0:
            if   self.y<0:  return       math.pi/2.0*3
            elif self.y>0:  return       math.pi/2.0
            else:           return 0
        elif self.y==0:
            if   self.x<0:  return       math.pi
           #elif self.x>0:  return 0
            else:           return 0
        ans = math.atan( self.y / self.x )
        if self.x > 0:
            if self.y>0:    return ans
            else:           return ans + math.pi*2.0
        else:               return ans + math.pi","determine the angle of this point on a circle, measured in radians (presume values represent a Vector)"
"def pages():
    """"""Load pages.""""""
    p1 = Page(
        url='/example1',
        title='My page with default template',
        description='my description',
        content='hello default page',
        template_name='invenio_pages/default.html',
    )
    p2 = Page(
        url='/example2',
        title='My page with my template',
        description='my description',
        content='hello my page',
        template_name='app/mytemplate.html',
    )
    with db.session.begin_nested():
        db.session.add(p1)
        db.session.add(p2)
    db.session.commit()",Load pages.
"def parse_dict_strings(code):
    """"""Generator of elements of a dict that is given in the code string

    Parsing is shallow, i.e. all content is yielded as strings

    Parameters
    ----------
    code: String
    \tString that contains a dict

    """"""

    i = 0
    level = 0
    chunk_start = 0
    curr_paren = None

    for i, char in enumerate(code):
        if char in [""("", ""["", ""{""] and curr_paren is None:
            level += 1
        elif char in ["")"", ""]"", ""}""] and curr_paren is None:
            level -= 1
        elif char in ['""', ""'""]:
            if curr_paren == char:
                curr_paren = None
            elif curr_paren is None:
                curr_paren = char

        if level == 0 and char in [':', ','] and curr_paren is None:
            yield code[chunk_start: i].strip()
            chunk_start = i + 1

    yield code[chunk_start:i + 1].strip()","Generator of elements of a dict that is given in the code string

    Parsing is shallow, i.e. all content is yielded as strings

    Parameters
    ----------
    code: String
    \tString that contains a dict"
"def add_to_playlist(self, items, playlist='video'):
        '''Adds the provided list of items to the specified playlist.
        Available playlists include *video* and *music*.
        '''
        playlists = {'music': 0, 'video': 1}
        assert playlist in playlists.keys(), ('Playlist ""%s"" is invalid.' %
                                              playlist)
        selected_playlist = xbmc.PlayList(playlists[playlist])

        _items = []
        for item in items:
            if not hasattr(item, 'as_xbmc_listitem'):
                if 'info_type' in item.keys():
                    log.warning('info_type key has no affect for playlist '
                                'items as the info_type is inferred from the '
                                'playlist type.')
                # info_type has to be same as the playlist type
                item['info_type'] = playlist
                item = xbmcswift2.ListItem.from_dict(**item)
            _items.append(item)
            selected_playlist.add(item.get_path(), item.as_xbmc_listitem())
        return _items","Adds the provided list of items to the specified playlist.
        Available playlists include *video* and *music*."
"def convert_vec2_to_vec4(scale, data):
    """"""transforms an array of 2d coords into 4d""""""
    it = iter(data)
    while True:
        yield next(it) * scale  # x
        yield next(it) * scale  # y
        yield 0.0       # z
        yield 1.0",transforms an array of 2d coords into 4d
"def extend_and_specialize(items, loader):
    # type: (List[Dict[Text, Any]], Loader) -> List[Dict[Text, Any]]
    """"""
    Apply 'extend' and 'specialize' to fully materialize derived record types.
    """"""

    items = deepcopy_strip(items)
    types = {i[""name""]: i for i in items}  # type: Dict[Text, Any]
    results = []

    for stype in items:
        if ""extends"" in stype:
            specs = {}  # type: Dict[Text, Text]
            if ""specialize"" in stype:
                for spec in aslist(stype[""specialize""]):
                    specs[spec[""specializeFrom""]] = spec[""specializeTo""]

            exfields = []  # type: List[Text]
            exsym = []  # type: List[Text]
            for ex in aslist(stype[""extends""]):
                if ex not in types:
                    raise Exception(
                        ""Extends {} in {} refers to invalid base type."".format(
                            stype[""extends""], stype[""name""]))

                basetype = copy.copy(types[ex])

                if stype[""type""] == ""record"":
                    if specs:
                        basetype[""fields""] = replace_type(
                            basetype.get(""fields"", []), specs, loader, set())

                    for field in basetype.get(""fields"", []):
                        if ""inherited_from"" not in field:
                            field[""inherited_from""] = ex

                    exfields.extend(basetype.get(""fields"", []))
                elif stype[""type""] == ""enum"":
                    exsym.extend(basetype.get(""symbols"", []))

            if stype[""type""] == ""record"":
                stype = copy.copy(stype)
                exfields.extend(stype.get(""fields"", []))
                stype[""fields""] = exfields

                fieldnames = set()  # type: Set[Text]
                for field in stype[""fields""]:
                    if field[""name""] in fieldnames:
                        raise validate.ValidationException(
                            ""Field name {} appears twice in {}"".format(
                                field[""name""], stype[""name""]))
                    else:
                        fieldnames.add(field[""name""])
            elif stype[""type""] == ""enum"":
                stype = copy.copy(stype)
                exsym.extend(stype.get(""symbols"", []))
                stype[""symbol""] = exsym

            types[stype[""name""]] = stype

        results.append(stype)

    ex_types = {}
    for result in results:
        ex_types[result[""name""]] = result

    extended_by = {}  # type: Dict[Text, Text]
    for result in results:
        if ""extends"" in result:
            for ex in aslist(result[""extends""]):
                if ex_types[ex].get(""abstract""):
                    add_dictlist(extended_by, ex, ex_types[result[""name""]])
                    add_dictlist(extended_by, avro_name(ex), ex_types[ex])

    for result in results:
        if result.get(""abstract"") and result[""name""] not in extended_by:
            raise validate.ValidationException(
                ""{} is abstract but missing a concrete subtype"".format(
                    result[""name""]))

    for result in results:
        if ""fields"" in result:
            result[""fields""] = replace_type(
                result[""fields""], extended_by, loader, set())

    return results",Apply 'extend' and 'specialize' to fully materialize derived record types.
"def set_file(self, filename):
        """""" Analyse the file with the captured content """"""
        # Use the file name as prefix if none is given
        if self.output_prefix is None:
            _, self.output_prefix = os.path.split(filename)
        # Check if the file is present, since rdpcap will not do that
        if not (os.path.isfile(filename) and os.access(filename, os.R_OK)):
            print 'The file \'{0}\' is either not present or not readable. '\
                  'Exiting!'.format(filename)
            sys.exit(1)
        try:
            packets = rdpcap(filename)
        except NameError:
            # Due probably to a bug in rdpcap, this kind of error raises a
            # NameError, because the exception that is tried to raise, is not
            # defined
            print 'The file \'{}\' is not a pcap capture file. Exiting!'\
                .format(filename)
            sys.exit(2)

        for number, packet in enumerate(packets):
            # See if there is a field called load
            self._debug('\nNUMBER {0}'.format(number), no_prefix=True)
            try:
                # Will cause AttributeError if there is no load
                packet.getfieldval('load')
                # Get the full load
                load = packet.sprintf('%TCP.payload%')
                self._debug('PAYLOAD LENGTH {0}'.format(len(load)),
                            no_prefix=True)
                self._debug(load, load=True)
                self._parse_load(load)
            except AttributeError:
                self._debug('LOAD EXCEPTION', no_prefix=True)
        if len(self.messages) > 0 and not self.messages[-1].write_closed:
            self._debug('DELETE LAST OPEN FILE')
            del self.messages[-1]

        if self.args.debug_analysis:
            sys.exit(0)",Analyse the file with the captured content
"def alwaysCalledWithExactly(cls, spy, *args, **kwargs): #pylint: disable=invalid-name
        """"""
        Checking the inspector is always called with exactly args/kwargs
        Args: SinonSpy, args/kwargs
        """"""
        cls.__is_spy(spy)
        if not (spy.alwaysCalledWithExactly(*args, **kwargs)):
            raise cls.failException(cls.message)","Checking the inspector is always called with exactly args/kwargs
        Args: SinonSpy, args/kwargs"
"def plot_points(points, show=True):
    """"""
    Plot an (n,3) list of points using matplotlib

    Parameters
    -------------
    points : (n, 3) float
      Points in space
    show : bool
      If False, will not show until plt.show() is called
    """"""
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D  # NOQA

    points = np.asanyarray(points, dtype=np.float64)

    if len(points.shape) != 2:
        raise ValueError('Points must be (n, 2|3)!')

    if points.shape[1] == 3:
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(*points.T)
    elif points.shape[1] == 2:
        plt.scatter(*points.T)
    else:
        raise ValueError('points not 2D/3D: {}'.format(
            points.shape))

    if show:
        plt.show()","Plot an (n,3) list of points using matplotlib

    Parameters
    -------------
    points : (n, 3) float
      Points in space
    show : bool
      If False, will not show until plt.show() is called"
"def deliver_hook(self, instance, payload_override=None):
        """"""
        Deliver the payload to the target URL.

        By default it serializes to JSON and POSTs.
        """"""
        payload = payload_override or self.serialize_hook(instance)
        if getattr(settings, 'HOOK_DELIVERER', None):
            deliverer = get_module(settings.HOOK_DELIVERER)
            deliverer(self.target, payload, instance=instance, hook=self)
        else:
            client.post(
                url=self.target,
                data=json.dumps(payload, cls=DjangoJSONEncoder),
                headers={'Content-Type': 'application/json'}
            )

        signals.hook_sent_event.send_robust(sender=self.__class__, payload=payload, instance=instance, hook=self)
        return None","Deliver the payload to the target URL.

        By default it serializes to JSON and POSTs."
"def saveToObject(self):
        """"""Re-implemented from :meth:`AbstractComponentWidget<sparkle.gui.stim.abstract_component_editor.AbstractComponentWidget.saveToObject>`""""""
        details = self._component.auto_details()
        for field, widget in self.inputWidgets.items():
            self._component.set(field, widget.value())
        self.attributesSaved.emit(self._component.__class__.__name__, self._component.stateDict())",Re-implemented from :meth:`AbstractComponentWidget<sparkle.gui.stim.abstract_component_editor.AbstractComponentWidget.saveToObject>`
"def exec(self,
             container: Container,
             command: str,
             context: Optional[str] = None,
             stdout: bool = True,
             stderr: bool = False,
             time_limit: Optional[int] = None
             ) -> ExecResponse:
        """"""
        Executes a given command inside a provided container.

        Parameters:
            container: the container to which the command should be issued.
            command: the command that should be executed.
            context: the working directory that should be used to perform the
                execution. If no context is provided, then the command will be
                executed at the root of the container.
            stdout: specifies whether or not output to the stdout should be
                included in the execution summary.
            stderr: specifies whether or not output to the stderr should be
                included in the execution summary.
            time_limit: an optional time limit that is applied to the
                execution. If the command fails to execute within the time
                limit, the command will be aborted and treated as a failure.

        Returns:
            a summary of the outcome of the execution.

        Raises:
            KeyError: if the container no longer exists on the server.
        """"""
        # FIXME perhaps these should be encoded as path variables?
        payload = {
            'command': command,
            'context': context,
            'stdout': stdout,
            'stderr': stderr,
            'time-limit': time_limit
        }
        path = ""containers/{}/exec"".format(container.uid)
        r = self.__api.post(path, json=payload)

        if r.status_code == 200:
            return ExecResponse.from_dict(r.json())
        if r.status_code == 404:
            raise KeyError(""no container found with given UID: {}"".format(container.uid))

        self.__api.handle_erroneous_response(r)","Executes a given command inside a provided container.

        Parameters:
            container: the container to which the command should be issued.
            command: the command that should be executed.
            context: the working directory that should be used to perform the
                execution. If no context is provided, then the command will be
                executed at the root of the container.
            stdout: specifies whether or not output to the stdout should be
                included in the execution summary.
            stderr: specifies whether or not output to the stderr should be
                included in the execution summary.
            time_limit: an optional time limit that is applied to the
                execution. If the command fails to execute within the time
                limit, the command will be aborted and treated as a failure.

        Returns:
            a summary of the outcome of the execution.

        Raises:
            KeyError: if the container no longer exists on the server."
"def upload_crop(self, ims_host, filename, id_annot, id_storage, 
                id_project=None, sync=False, protocol=None):
        """"""
        Upload the crop associated with an annotation as a new image.

        Parameters
        ----------
        ims_host: str
            Cytomine IMS host, with or without the protocol
        filename: str
            Filename to give to the newly created image
        id_annot: int
            Identifier of the annotation to crop
        id_storage: int
            Identifier of the storage to use to upload the new image
        id_project: int, optional
            Identifier of a project in which the new image should be added
        sync: bool, optional
            True:   the server will answer once the uploaded file is 
                    deployed (response will include the created image)
            False (default): the server will answer as soon as it receives the file
        protocol: str (""http"", ""http://"", ""https"", ""https://"")
            The default protocol - used only if the host value does not specify one

        Return
        ------
        uf: UploadedFile
            The uploaded file. Its images attribute is populated with the collection of created abstract images.
        """"""

        
        if not protocol:
                protocol = self._protocol
        ims_host, protocol = self._parse_url(ims_host, protocol)
        ims_host = ""{}://{}"".format(protocol, ims_host)
    
        query_parameters = {
            ""annotation"" : id_annot,
            ""storage"": id_storage,
            ""cytomine"": ""{}://{}"".format(self._protocol, self._host),
            ""name"": filename,
            ""sync"": sync
        }
    
        if id_project:
            query_parameters[""project""] = id_project
    
        response = self._session.post(""{}/uploadCrop"".format(ims_host),
                                      auth=CytomineAuth(
                                          self._public_key, 
                                          self._private_key,
                                          ims_host, """"),
                                      headers=self._headers(),
                                      params=query_parameters)
    
        if response.status_code == requests.codes.ok:
            uf = self._process_upload_response(response.json())
            self._logger.info(""Image crop uploaded successfully to {}"".format(ims_host))
            return uf
        else:
            self._logger.error(""Error during crop upload. Response: %s"", response)
            return False","Upload the crop associated with an annotation as a new image.

        Parameters
        ----------
        ims_host: str
            Cytomine IMS host, with or without the protocol
        filename: str
            Filename to give to the newly created image
        id_annot: int
            Identifier of the annotation to crop
        id_storage: int
            Identifier of the storage to use to upload the new image
        id_project: int, optional
            Identifier of a project in which the new image should be added
        sync: bool, optional
            True:   the server will answer once the uploaded file is 
                    deployed (response will include the created image)
            False (default): the server will answer as soon as it receives the file
        protocol: str (""http"", ""http://"", ""https"", ""https://"")
            The default protocol - used only if the host value does not specify one

        Return
        ------
        uf: UploadedFile
            The uploaded file. Its images attribute is populated with the collection of created abstract images."
"def to_jsons(graph: BELGraph, **kwargs) -> str:
    """"""Dump this graph as a Node-Link JSON object to a string.""""""
    graph_json_str = to_json(graph)
    return json.dumps(graph_json_str, ensure_ascii=False, **kwargs)",Dump this graph as a Node-Link JSON object to a string.
"def grant_user_access( self, uid, rid, expire_datetime = None, send_email=False ):
        '''
            Takes a user id and resource id and records a grant of access to that reseource for the user.
            If no expire_date is set, we'll default to 24 hours.
            If send_email is set to True, Tinypass will send an email related to the grant.
            No return value, raises ValueError.
        '''
        path =  ""/api/v3/publisher/user/access/grant""
        
        # convert expire_date to gmt seconds
        if expire_datetime:
            expires_seconds = calendar.timegm(expires_datetime.timetuple())
        else:
            expires_seconds = calendar.timegm(datetime.datetime.now().timetuple()) + (60*60*24)

        data = {
            'api_token': self.api_token, 
            'aid': self.app_id,
            'rid': rid,
            'uid': uid,
            'expire_date': expires_seconds,
            'send_email': send_email,
        }

        r = requests.get( self.base_url + path, data=data )

        if r.status_code != 200:
            raise ValueError( path + "":"" + r.reason )","Takes a user id and resource id and records a grant of access to that reseource for the user.
            If no expire_date is set, we'll default to 24 hours.
            If send_email is set to True, Tinypass will send an email related to the grant.
            No return value, raises ValueError."
"def parse_message(message, validation_level=None, find_groups=True, message_profile=None, report_file=None,
                  force_validation=False):
    """"""
    Parse the given ER7-encoded message and return an instance of :class:`Message <hl7apy.core.Message>`.

    :type message: ``str``
    :param message: the ER7-encoded message to be parsed

    :type validation_level: ``int``
    :param validation_level: the validation level. Possible values are those defined in
        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default
        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)

    :type find_groups: ``bool``
    :param find_groups: if ``True``, automatically assign the segments found to the appropriate
        :class:`Groups <hl7apy.core.Group>` instances. If ``False``, the segments found are assigned as
        children of the :class:`Message <hl7apy.core.Message>` instance
        
    :type force_validation: ``bool``
    :type force_validation: if ``True``, automatically forces the message validation after the end of the parsing

    :return: an instance of :class:`Message <hl7apy.core.Message>`

    >>> message = ""MSH|^~\&|GHH_ADT||||20080115153000||OML^O33^OML_O33|0123456789|P|2.5||||AL\\rPID|1||"" \
    ""566-554-3423^^^GHH^MR||EVERYMAN^ADAM^A|||M|||2222 HOME STREET^^ANN ARBOR^MI^^USA||555-555-2004|||M\\r""
    >>> m = parse_message(message)
    >>> print(m)
    <Message OML_O33>
    >>> print(m.msh.sending_application.to_er7())
    GHH_ADT
    >>> print(m.children)
    [<Segment MSH>, <Group OML_O33_PATIENT>]
    """"""
    message = message.lstrip()
    encoding_chars, message_structure, version = get_message_info(message)
    validation_level = _get_validation_level(validation_level)

    try:
        reference = message_profile[message_structure] if message_profile else None
    except KeyError:
        raise MessageProfileNotFound()

    try:
        m = Message(name=message_structure, reference=reference, version=version,
                    validation_level=validation_level, encoding_chars=encoding_chars)
    except InvalidName:
        m = Message(version=version, validation_level=validation_level,
                    encoding_chars=encoding_chars)

    try:
        children = parse_segments(message, m.version, encoding_chars, validation_level, m.reference, find_groups)
    except AttributeError:  # m.reference can raise i
        children = parse_segments(message, m.version, encoding_chars, validation_level, find_groups=False)

    m.children = children

    if force_validation:
        if message_profile is None:
            Validator.validate(m, report_file=report_file)
        else:
            Validator.validate(m, message_profile[message_structure], report_file=report_file)

    return m","Parse the given ER7-encoded message and return an instance of :class:`Message <hl7apy.core.Message>`.

    :type message: ``str``
    :param message: the ER7-encoded message to be parsed

    :type validation_level: ``int``
    :param validation_level: the validation level. Possible values are those defined in
        :class:`VALIDATION_LEVEL <hl7apy.consts.VALIDATION_LEVEL>` class or ``None`` to use the default
        validation level (see :func:`set_default_validation_level <hl7apy.set_default_validation_level>`)

    :type find_groups: ``bool``
    :param find_groups: if ``True``, automatically assign the segments found to the appropriate
        :class:`Groups <hl7apy.core.Group>` instances. If ``False``, the segments found are assigned as
        children of the :class:`Message <hl7apy.core.Message>` instance
        
    :type force_validation: ``bool``
    :type force_validation: if ``True``, automatically forces the message validation after the end of the parsing

    :return: an instance of :class:`Message <hl7apy.core.Message>`

    >>> message = ""MSH|^~\&|GHH_ADT||||20080115153000||OML^O33^OML_O33|0123456789|P|2.5||||AL\\rPID|1||"" \
    ""566-554-3423^^^GHH^MR||EVERYMAN^ADAM^A|||M|||2222 HOME STREET^^ANN ARBOR^MI^^USA||555-555-2004|||M\\r""
    >>> m = parse_message(message)
    >>> print(m)
    <Message OML_O33>
    >>> print(m.msh.sending_application.to_er7())
    GHH_ADT
    >>> print(m.children)
    [<Segment MSH>, <Group OML_O33_PATIENT>]"
"def load_segment(self, f, is_irom_segment=False):
        """""" Load the next segment from the image file """"""
        file_offs = f.tell()
        (offset, size) = struct.unpack('<II', f.read(8))
        self.warn_if_unusual_segment(offset, size, is_irom_segment)
        segment_data = f.read(size)
        if len(segment_data) < size:
            raise FatalError('End of file reading segment 0x%x, length %d (actual length %d)' % (offset, size, len(segment_data)))
        segment = ImageSegment(offset, segment_data, file_offs)
        self.segments.append(segment)
        return segment",Load the next segment from the image file
"def truncate(self, new_count):
        """"""Truncate the filter as if only *new_count* :py:meth:`.predict`,
        :py:meth:`.update` steps had been performed. If *new_count* is greater
        than :py:attr:`.state_count` then this function is a no-op.

        Measurements, state estimates, process matrices and process noises which
        are truncated are discarded.

        Args:
            new_count (int): Number of states to retain.

        """"""
        self.posterior_state_estimates = self.posterior_state_estimates[:new_count]
        self.prior_state_estimates = self.prior_state_estimates[:new_count]
        self.measurements = self.measurements[:new_count]
        self.process_matrices = self.process_matrices[:new_count]
        self.process_covariances = self.process_covariances[:new_count]","Truncate the filter as if only *new_count* :py:meth:`.predict`,
        :py:meth:`.update` steps had been performed. If *new_count* is greater
        than :py:attr:`.state_count` then this function is a no-op.

        Measurements, state estimates, process matrices and process noises which
        are truncated are discarded.

        Args:
            new_count (int): Number of states to retain."
"def get_bool(self, property):
        """"""
        Gets the value of the given property as boolean.

        :param property: (:class:`~hazelcast.config.ClientProperty`), Property to get value from
        :return: (bool), Value of the given property
        """"""
        value = self.get(property)
        if isinstance(value, bool):
            return value
        return value.lower() == ""true""","Gets the value of the given property as boolean.

        :param property: (:class:`~hazelcast.config.ClientProperty`), Property to get value from
        :return: (bool), Value of the given property"
"def discard(self):  # pylint: disable=no-self-use
        """"""
        Clears uncommited changes.

        :raise pyPluribus.exceptions.ConfigurationDiscardError: If the configuration applied cannot be discarded.
        """"""
        try:
            self.rollback(0)
        except pyPluribus.exceptions.RollbackError as rbackerr:
            raise pyPluribus.exceptions.ConfigurationDiscardError(""Cannot discard configuration: {err}.\
                "".format(err=rbackerr))","Clears uncommited changes.

        :raise pyPluribus.exceptions.ConfigurationDiscardError: If the configuration applied cannot be discarded."
"def __verify_auth_params(username, password, access_token):
        """"""Verify that valid authentication parameters were passed to __init__""""""

        if all(v is not None for v in [username, password, access_token]):
            raise ValueError('Cannot supply a username/password and a access token')

        if (username is None or password is None) and access_token is None:
            raise ValueError('Must supply a username/password or access token')",Verify that valid authentication parameters were passed to __init__
"def encode(cls, value):
        """"""
        take a valid unicode string and turn it into utf-8 bytes

        :param value: unicode, str
        :return: bytes
        """"""
        coerced = unicode(value)
        if coerced == value:
            return coerced.encode(cls._encoding)

        raise InvalidValue('not text')","take a valid unicode string and turn it into utf-8 bytes

        :param value: unicode, str
        :return: bytes"
"def _poll_update_interval(self):
        """""" update the polling interval to be used next iteration """"""
        # Increase by 1 second every 3 polls
        if old_div(self.poll_count, 3) > self.poll_interval_level:
            self.poll_interval_level += 1
            self.poll_interval_s += 1
            self.logger.info(
                ""Increased polling interval to %d seconds"", self.poll_interval_s
            )",update the polling interval to be used next iteration
"def get_bgp_neighbors(self):
        def generate_vrf_query(vrf_name):
            """"""
            Helper to provide XML-query for the VRF-type we're interested in.
            """"""
            if vrf_name == ""global"":
                rpc_command = ""<Get><Operational><BGP><InstanceTable><Instance><Naming>\
                <InstanceName>default</InstanceName></Naming><InstanceActive><DefaultVRF>\
                <GlobalProcessInfo></GlobalProcessInfo><NeighborTable></NeighborTable></DefaultVRF>\
                </InstanceActive></Instance></InstanceTable></BGP></Operational></Get>""

            else:
                rpc_command = ""<Get><Operational><BGP><InstanceTable><Instance><Naming>\
                <InstanceName>default</InstanceName></Naming><InstanceActive><VRFTable><VRF>\
                <Naming>{vrf_name}</Naming><GlobalProcessInfo></GlobalProcessInfo><NeighborTable>\
                </NeighborTable></VRF></VRFTable></InstanceActive></Instance></InstanceTable>\
                </BGP></Operational></Get>"".format(
                    vrf_name=vrf_name
                )
            return rpc_command

        """"""
        Initial run to figure out what VRF's are available
        Decided to get this one from Configured-section
        because bulk-getting all instance-data to do the same could get ridiculously heavy
        Assuming we're always interested in the DefaultVRF
        """"""

        active_vrfs = [""global""]

        rpc_command = ""<Get><Operational><BGP><ConfigInstanceTable><ConfigInstance><Naming>\
        <InstanceName>default</InstanceName></Naming><ConfigInstanceVRFTable>\
        </ConfigInstanceVRFTable></ConfigInstance></ConfigInstanceTable></BGP></Operational></Get>""

        result_tree = ETREE.fromstring(self.device.make_rpc_call(rpc_command))

        for node in result_tree.xpath("".//ConfigVRF""):
            active_vrfs.append(napalm.base.helpers.find_txt(node, ""Naming/VRFName""))

        result = {}

        for vrf in active_vrfs:
            rpc_command = generate_vrf_query(vrf)
            result_tree = ETREE.fromstring(self.device.make_rpc_call(rpc_command))

            this_vrf = {}
            this_vrf[""peers""] = {}

            if vrf == ""global"":
                this_vrf[""router_id""] = napalm.base.helpers.convert(
                    text_type,
                    napalm.base.helpers.find_txt(
                        result_tree,
                        ""Get/Operational/BGP/InstanceTable/Instance/InstanceActive/DefaultVRF""
                        ""/GlobalProcessInfo/VRF/RouterID"",
                    ),
                )
            else:
                this_vrf[""router_id""] = napalm.base.helpers.convert(
                    text_type,
                    napalm.base.helpers.find_txt(
                        result_tree,
                        ""Get/Operational/BGP/InstanceTable/Instance/InstanceActive/VRFTable/VRF""
                        ""/GlobalProcessInfo/VRF/RouterID"",
                    ),
                )

            neighbors = {}

            for neighbor in result_tree.xpath("".//Neighbor""):
                this_neighbor = {}
                this_neighbor[""local_as""] = napalm.base.helpers.convert(
                    int, napalm.base.helpers.find_txt(neighbor, ""LocalAS"")
                )
                this_neighbor[""remote_as""] = napalm.base.helpers.convert(
                    int, napalm.base.helpers.find_txt(neighbor, ""RemoteAS"")
                )
                this_neighbor[""remote_id""] = napalm.base.helpers.convert(
                    text_type, napalm.base.helpers.find_txt(neighbor, ""RouterID"")
                )

                if (
                    napalm.base.helpers.find_txt(neighbor, ""ConnectionAdminStatus"")
                    == ""1""
                ):
                    this_neighbor[""is_enabled""] = True

                try:
                    this_neighbor[""description""] = napalm.base.helpers.convert(
                        text_type, napalm.base.helpers.find_txt(neighbor, ""Description"")
                    )
                except AttributeError:
                    this_neighbor[""description""] = """"

                this_neighbor[""is_enabled""] = (
                    napalm.base.helpers.find_txt(neighbor, ""ConnectionAdminStatus"")
                    == ""1""
                )

                if (
                    text_type(
                        napalm.base.helpers.find_txt(neighbor, ""ConnectionAdminStatus"")
                    )
                    == ""1""
                ):
                    this_neighbor[""is_enabled""] = True
                else:
                    this_neighbor[""is_enabled""] = False

                if (
                    text_type(napalm.base.helpers.find_txt(neighbor, ""ConnectionState""))
                    == ""BGP_ST_ESTAB""
                ):
                    this_neighbor[""is_up""] = True
                    this_neighbor[""uptime""] = napalm.base.helpers.convert(
                        int,
                        napalm.base.helpers.find_txt(
                            neighbor, ""ConnectionEstablishedTime""
                        ),
                    )
                else:
                    this_neighbor[""is_up""] = False
                    this_neighbor[""uptime""] = -1

                this_neighbor[""address_family""] = {}

                if (
                    napalm.base.helpers.find_txt(
                        neighbor, ""ConnectionRemoteAddress/AFI""
                    )
                    == ""IPv4""
                ):
                    this_afi = ""ipv4""
                elif (
                    napalm.base.helpers.find_txt(
                        neighbor, ""ConnectionRemoteAddress/AFI""
                    )
                    == ""IPv6""
                ):
                    this_afi = ""ipv6""
                else:
                    this_afi = napalm.base.helpers.find_txt(
                        neighbor, ""ConnectionRemoteAddress/AFI""
                    )

                this_neighbor[""address_family""][this_afi] = {}

                try:
                    this_neighbor[""address_family""][this_afi][
                        ""received_prefixes""
                    ] = napalm.base.helpers.convert(
                        int,
                        napalm.base.helpers.find_txt(
                            neighbor, ""AFData/Entry/PrefixesAccepted""
                        ),
                        0,
                    ) + napalm.base.helpers.convert(
                        int,
                        napalm.base.helpers.find_txt(
                            neighbor, ""AFData/Entry/PrefixesDenied""
                        ),
                        0,
                    )
                    this_neighbor[""address_family""][this_afi][
                        ""accepted_prefixes""
                    ] = napalm.base.helpers.convert(
                        int,
                        napalm.base.helpers.find_txt(
                            neighbor, ""AFData/Entry/PrefixesAccepted""
                        ),
                        0,
                    )
                    this_neighbor[""address_family""][this_afi][
                        ""sent_prefixes""
                    ] = napalm.base.helpers.convert(
                        int,
                        napalm.base.helpers.find_txt(
                            neighbor, ""AFData/Entry/PrefixesAdvertised""
                        ),
                        0,
                    )
                except AttributeError:
                    this_neighbor[""address_family""][this_afi][""received_prefixes""] = -1
                    this_neighbor[""address_family""][this_afi][""accepted_prefixes""] = -1
                    this_neighbor[""address_family""][this_afi][""sent_prefixes""] = -1

                neighbor_ip = napalm.base.helpers.ip(
                    napalm.base.helpers.find_txt(
                        neighbor, ""Naming/NeighborAddress/IPV4Address""
                    )
                    or napalm.base.helpers.find_txt(
                        neighbor, ""Naming/NeighborAddress/IPV6Address""
                    )
                )

                neighbors[neighbor_ip] = this_neighbor

            this_vrf[""peers""] = neighbors
            result[vrf] = this_vrf

        return result","Initial run to figure out what VRF's are available
        Decided to get this one from Configured-section
        because bulk-getting all instance-data to do the same could get ridiculously heavy
        Assuming we're always interested in the DefaultVRF"
"def includeme(config):
    """""" Add pyramid_webpack methods and config to the app """"""
    settings = config.registry.settings
    root_package_name = config.root_package.__name__
    config.registry.webpack = {
        'DEFAULT': WebpackState(settings, root_package_name)
    }
    for extra_config in aslist(settings.get('webpack.configs', [])):
        state = WebpackState(settings, root_package_name, name=extra_config)
        config.registry.webpack[extra_config] = state

    # Set up any static views
    for state in six.itervalues(config.registry.webpack):
        if state.static_view:
            config.add_static_view(name=state.static_view_name,
                                   path=state.static_view_path,
                                   cache_max_age=state.cache_max_age)

    config.add_request_method(get_webpack, 'webpack')",Add pyramid_webpack methods and config to the app
"def p_save_code(p):
    """""" statement : SAVE expr CODE expr COMMA expr
                   | SAVE expr ID
                   | SAVE expr ARRAY_ID
    """"""
    if p[2].type_ != TYPE.string:
        api.errmsg.syntax_error_expected_string(p.lineno(1), p[2].type_)

    if len(p) == 5:
        if p[3].upper() not in ('SCREEN', 'SCREEN$'):
            syntax_error(p.lineno(3), 'Unexpected ""%s"" ID. Expected ""SCREEN$"" instead' % p[3])
            return None
        else:
            # ZX Spectrum screen start + length
            # This should be stored in a architecture-dependant file
            start = make_number(16384, lineno=p.lineno(1))
            length = make_number(6912, lineno=p.lineno(1))
    else:
        start = p[4]
        length = p[6]

    p[0] = make_sentence(p[1], p[2], start, length)","statement : SAVE expr CODE expr COMMA expr
                   | SAVE expr ID
                   | SAVE expr ARRAY_ID"
"def create_session(username, password):
    """"""
    Create a session for the user, and then return the key.
    """"""
    user = User.objects.get_user_by_password(username, password)
    auth_session_engine = get_config('auth_session_engine')
    if not user:
        raise InvalidInput('Username or password incorrect')
    session_key = random_string(15)
    while auth_session_engine.get(session_key):
        session_key = random_string(15)
    auth_session_engine.set(session_key, user.username, get_config('auth_session_expire'))
    return {'session_key': session_key, 'user': user}","Create a session for the user, and then return the key."
"def get_preprocessed_statement(self, input_statement):
        """"""
        Preprocess the input statement.
        """"""
        for preprocessor in self.chatbot.preprocessors:
            input_statement = preprocessor(input_statement)

        return input_statement",Preprocess the input statement.
"def activate(self, target=None, **options):
        """"""Activate DEP communication with a target.""""""
        log.debug(""initiator options: {0}"".format(options))

        self.did = options.get('did', None)
        self.nad = options.get('nad', None)
        self.gbi = options.get('gbi', '')[0:48]
        self.brs = min(max(0, options.get('brs', 2)), 2)
        self.lri = min(max(0, options.get('lri', 3)), 3)
        if self._acm is None or 'acm' in options:
            self._acm = bool(options.get('acm', True))

        assert self.did is None or 0 <= self.did <= 255
        assert self.nad is None or 0 <= self.nad <= 255

        ppi = (self.lri << 4) | (bool(self.gbi) << 1) | int(bool(self.nad))
        did = 0 if self.did is None else self.did
        atr_req = ATR_REQ(os.urandom(10), did, 0, 0, ppi, self.gbi)
        psl_req = PSL_REQ(did, (0, 9, 18)[self.brs], self.lri)
        atr_res = psl_res = None
        self.target = target

        if self.target is None and self.acm is True:
            log.debug(""searching active communication mode target at 106A"")
            tg = nfc.clf.RemoteTarget(""106A"", atr_req=atr_req.encode())
            try:
                self.target = self.clf.sense(tg, iterations=2, interval=0.1)
            except nfc.clf.UnsupportedTargetError:
                self._acm = False
            except nfc.clf.CommunicationError:
                pass
            else:
                if self.target:
                    atr_res = ATR_RES.decode(self.target.atr_res)
                else:
                    self._acm = None

        if self.target is None:
            log.debug(""searching passive communication mode target at 106A"")
            target = nfc.clf.RemoteTarget(""106A"")
            target = self.clf.sense(target, iterations=2, interval=0.1)
            if target and target.sel_res and bool(target.sel_res[0] & 0x40):
                self.target = target

        if self.target is None and self.brs > 0:
            log.debug(""searching passive communication mode target at 212F"")
            target = nfc.clf.RemoteTarget(""212F"", sensf_req=b'\0\xFF\xFF\0\0')
            target = self.clf.sense(target, iterations=2, interval=0.1)
            if target and target.sensf_res.startswith(b'\1\1\xFE'):
                atr_req.nfcid3 = target.sensf_res[1:9] + b'ST'
                self.target = target

        if self.target and self.target.atr_res is None:
            try:
                atr_res = self.send_req_recv_res(atr_req, 1.0)
            except nfc.clf.CommunicationError:
                pass
            if atr_res is None:
                log.debug(""NFC-DEP Attribute Request failed"")
                return None

        if self.target and atr_res:
            if self.brs > ('106A', '212F', '424F').index(self.target.brty):
                try:
                    psl_res = self.send_req_recv_res(psl_req, 0.1)
                except nfc.clf.CommunicationError:
                    pass
                if psl_res is None:
                    log.debug(""NFC-DEP Parameter Selection failed"")
                    return None
                self.target.brty = ('212F', '424F')[self.brs-1]

            self.rwt = (4096/13.56E6
                        * 2**(atr_res.wt if atr_res.wt < 15 else 14))
            self.miu = (atr_res.lr-3 - int(self.did is not None)
                        - int(self.nad is not None))
            self.gbt = atr_res.gb
            self.pni = 0

            log.info(""running as "" + str(self))
            return self.gbt",Activate DEP communication with a target.
"async def async_determine_channel(channel):
    '''
    Check whether the current channel is correct. If not try to determine it
    using fuzzywuzzy
    '''
    from fuzzywuzzy import process
    channel_data = await async_get_channels()
    if not channel_data:
        _LOGGER.error('No channel data. Cannot determine requested channel.')
        return
    channels = [c for c in channel_data.get('data', {}).keys()]
    if channel in channels:
        return channel
    else:
        res = process.extractOne(channel, channels)[0]
        _LOGGER.debug('No direct match found for %s. Resort to guesswork.'
                      'Guessed %s', channel, res)
        return res","Check whether the current channel is correct. If not try to determine it
    using fuzzywuzzy"
"def __flush_data(self, data):
        """"""Flush `data` to a chunk.
        """"""
        # Ensure the index, even if there's nothing to write, so
        # the filemd5 command always succeeds.
        self.__ensure_indexes()
        self._file['md5'].update(data)

        if not data:
            return
        assert(len(data) <= self.chunk_size)

        chunk = {""files_id"": self._file[""_id""],
                 ""n"": self._chunk_number,
                 ""data"": Binary(data)}

        try:
            self._chunks.insert_one(chunk)
        except DuplicateKeyError:
            self._raise_file_exists(self._file['_id'])
        self._chunk_number += 1
        self._position += len(data)",Flush `data` to a chunk.
"def install_config_elastic(self):
        """"""
        install and config elasticsearch
        :return:
        """"""
        if self.prompt_check(""Download and install elasticsearch""):
            self.elastic_install()

        if self.prompt_check(""Configure and autostart elasticsearch""):
            self.elastic_config()","install and config elasticsearch
        :return:"
"def transform_key(startkey, seed_key, seed_rand, rounds):
    """"""
    This method creates the key to decrypt the database.
    """"""
    masterkey = startkey
    aes = AES.new(seed_key, AES.MODE_ECB)

    # Encrypt the created hash <rounds> times
    for _i in range(rounds):
        masterkey = aes.encrypt(masterkey)

    # Finally, hash it again...
    masterkey = hashlib.sha256(masterkey).digest()
    # ...and hash the result together with the randomseed
    return hashlib.sha256(seed_rand + masterkey).digest()",This method creates the key to decrypt the database.
"def add_led(self, led):
    """"""Add an LED that's part of this keypad.""""""
    self._leds.append(led)
    self._components[led.component_number] = led",Add an LED that's part of this keypad.
"def com_google_fonts_check_family_naming_recommendations(ttFont):
  """"""Font follows the family naming recommendations?""""""
  # See http://forum.fontlab.com/index.php?topic=313.0
  import re
  from fontbakery.utils import get_name_entry_strings
  bad_entries = []

  # <Postscript name> may contain only a-zA-Z0-9
  # and one hyphen
  bad_psname = re.compile(""[^A-Za-z0-9-]"")
  for string in get_name_entry_strings(ttFont,
                                       NameID.POSTSCRIPT_NAME):
    if bad_psname.search(string):
      bad_entries.append({
          'field':
          'PostScript Name',
          'value':
          string,
          'rec': ('May contain only a-zA-Z0-9'
                  ' characters and an hyphen.')
      })
    if string.count('-') > 1:
      bad_entries.append({
          'field': 'Postscript Name',
          'value': string,
          'rec': ('May contain not more'
                  ' than a single hyphen')
      })

  for string in get_name_entry_strings(ttFont,
                                       NameID.FULL_FONT_NAME):
    if len(string) >= 64:
      bad_entries.append({
          'field': 'Full Font Name',
          'value': string,
          'rec': 'exceeds max length (63)'
      })

  for string in get_name_entry_strings(ttFont,
                                       NameID.POSTSCRIPT_NAME):
    if len(string) >= 30:
      bad_entries.append({
          'field': 'PostScript Name',
          'value': string,
          'rec': 'exceeds max length (29)'
      })

  for string in get_name_entry_strings(ttFont,
                                       NameID.FONT_FAMILY_NAME):
    if len(string) >= 32:
      bad_entries.append({
          'field': 'Family Name',
          'value': string,
          'rec': 'exceeds max length (31)'
      })

  for string in get_name_entry_strings(ttFont,
                                       NameID.FONT_SUBFAMILY_NAME):
    if len(string) >= 32:
      bad_entries.append({
          'field': 'Style Name',
          'value': string,
          'rec': 'exceeds max length (31)'
      })

  for string in get_name_entry_strings(ttFont,
                                       NameID.TYPOGRAPHIC_FAMILY_NAME):
    if len(string) >= 32:
      bad_entries.append({
          'field': 'OT Family Name',
          'value': string,
          'rec': 'exceeds max length (31)'
      })

  for string in get_name_entry_strings(ttFont,
                                       NameID.TYPOGRAPHIC_SUBFAMILY_NAME):
    if len(string) >= 32:
      bad_entries.append({
          'field': 'OT Style Name',
          'value': string,
          'rec': 'exceeds max length (31)'
      })

  if len(bad_entries) > 0:
    table = ""| Field | Value | Recommendation |\n""
    table += ""|:----- |:----- |:-------------- |\n""
    for bad in bad_entries:
      table += ""| {} | {} | {} |\n"".format(bad[""field""],
                                           bad[""value""],
                                           bad[""rec""])
    yield INFO, (""Font does not follow ""
                 ""some family naming recommendations:\n\n""
                 ""{}"").format(table)
  else:
    yield PASS, ""Font follows the family naming recommendations.""",Font follows the family naming recommendations?
"def _unpack_int_base128(varint, offset):
        """"""Implement Perl unpack's 'w' option, aka base 128 decoding.""""""
        res = ord(varint[offset])
        if ord(varint[offset]) >= 0x80:
            offset += 1
            res = ((res - 0x80) << 7) + ord(varint[offset])
            if ord(varint[offset]) >= 0x80:
                offset += 1
                res = ((res - 0x80) << 7) + ord(varint[offset])
                if ord(varint[offset]) >= 0x80:
                    offset += 1
                    res = ((res - 0x80) << 7) + ord(varint[offset])
                    if ord(varint[offset]) >= 0x80:
                        offset += 1
                        res = ((res - 0x80) << 7) + ord(varint[offset])
        return res, offset + 1","Implement Perl unpack's 'w' option, aka base 128 decoding."
"def insertLink(page, lnk, mark = True):
    """""" Insert a new link for the current page. """"""
    CheckParent(page)
    annot = getLinkText(page, lnk)
    if annot == """":
        raise ValueError(""link kind not supported"")

    page._addAnnot_FromString([annot])
    return",Insert a new link for the current page.
"def analyze(self, handle, filename):
        """"""Submit a file for analysis.

        :type  handle:   File handle
        :param handle:   Handle to file to upload for analysis.
        :type  filename: str
        :param filename: File name.

        :rtype:  str
        :return: Task ID as a string
        """"""
        # multipart post files.
        files = {""file"": (filename, handle)}

        # ensure the handle is at offset 0.
        handle.seek(0)

        response = self._request(""tasks/create/file"", method='POST', files=files)

        # return task id; try v1.3 and v2.0 API response formats
        try:
            return str(json.loads(response.content.decode('utf-8'))[""task_id""])
        except KeyError:
            return str(json.loads(response.content.decode('utf-8'))[""task_ids""][0])","Submit a file for analysis.

        :type  handle:   File handle
        :param handle:   Handle to file to upload for analysis.
        :type  filename: str
        :param filename: File name.

        :rtype:  str
        :return: Task ID as a string"
"def simple( child, shared, parent ):
    """"""Return sub-set of children who are ""simple"" in the sense of group_children""""""
    return (
        not child.get('refs',())
        and (
            not shared.get(child['address'])
        or 
            shared.get(child['address']) == [parent['address']]
        )
    )","Return sub-set of children who are ""simple"" in the sense of group_children"
"def udiv(self, o):
        """"""
        Binary operation: unsigned division

        :param o: The divisor
        :return: (self / o) in unsigned arithmetic
        """"""
        #FIXME: copy the code fromm wrapped interval
        splitted_dividends = self._ssplit()
        splitted_divisors = o._ssplit()

        resulting_intervals = set()
        for dividend in splitted_dividends:
            for divisor in splitted_divisors:
                tmp = self._wrapped_unsigned_div(dividend, divisor)
                resulting_intervals.add(tmp)

        return StridedInterval.least_upper_bound(*resulting_intervals).normalize()","Binary operation: unsigned division

        :param o: The divisor
        :return: (self / o) in unsigned arithmetic"
"def save(self):
        """"""
        Saves an object to the database.

        .. code-block:: python

            #create a person instance
            person = Person(first_name='Kimberly', last_name='Eggleston')
            #saves it to Cassandra
            person.save()
        """"""

        # handle polymorphic models
        if self._is_polymorphic:
            if self._is_polymorphic_base:
                raise PolymorphicModelException('cannot save polymorphic base model')
            else:
                setattr(self, self._discriminator_column_name, self.__discriminator_value__)

        self.validate()
        self.__dmlquery__(self.__class__, self,
                          batch=self._batch,
                          ttl=self._ttl,
                          timestamp=self._timestamp,
                          consistency=self.__consistency__,
                          if_not_exists=self._if_not_exists,
                          conditional=self._conditional,
                          timeout=self._timeout,
                          if_exists=self._if_exists).save()

        self._set_persisted()

        self._timestamp = None

        return self","Saves an object to the database.

        .. code-block:: python

            #create a person instance
            person = Person(first_name='Kimberly', last_name='Eggleston')
            #saves it to Cassandra
            person.save()"
"def _start_aux(self):
        """"""
        Start an auxilary console
        """"""

        # We can not use the API because docker doesn't expose a websocket api for exec
        # https://github.com/GNS3/gns3-gui/issues/1039
        process = yield from asyncio.subprocess.create_subprocess_exec(
            ""docker"", ""exec"", ""-i"", self._cid, ""/gns3/bin/busybox"", ""script"", ""-qfc"", ""while true; do TERM=vt100 /gns3/bin/busybox sh; done"", ""/dev/null"",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.STDOUT,
            stdin=asyncio.subprocess.PIPE)
        server = AsyncioTelnetServer(reader=process.stdout, writer=process.stdin, binary=True, echo=True)
        self._telnet_servers.append((yield from asyncio.start_server(server.run, self._manager.port_manager.console_host, self.aux)))
        log.debug(""Docker container '%s' started listen for auxilary telnet on %d"", self.name, self.aux)",Start an auxilary console
"def get_context_data(self, **kwargs):
        """"""
        Returns context dictionary for view.

        :rtype: dict.
        """"""
        #noinspection PyUnresolvedReferences
        query_str           = self.request.GET.get('q', None)
        queryset            = kwargs.pop('object_list', self.object_list)
        context_object_name = self.get_context_object_name(queryset)

        # Build the context dictionary.
        context = {
            'ordering':     self.get_ordering(),
            'query_string': query_str,
            'is_searching': bool(query_str),
        }

        # Add extra variables to context for non-AJAX requests.
        #noinspection PyUnresolvedReferences
        if not self.request.is_ajax() or kwargs.get('force_search', False):
            context.update({
                'search_form':  self.get_search_form(),
                'popular_tags': self.model.popular_tags()
            })

        if context_object_name is not None:
            context[context_object_name] = queryset

        # Update context with any additional keyword arguments.
        context.update(kwargs)

        return super(IndexView, self).get_context_data(**context)","Returns context dictionary for view.

        :rtype: dict."
"def collect_one(self, *args, **kwargs):
        """"""Same as `collect` but expects to have only one result.

        :return: the only result directly, not the generator like `collect`.
        """"""
        generator = self.collect(*args, **kwargs)
        try:
            value = next(generator)
        except StopIteration:
            raise Exception(""Expected exactly one value don't have any"")
        try:
            next(generator)
        except StopIteration:
            return value
        raise Exception('Expected exactly one value but have more')","Same as `collect` but expects to have only one result.

        :return: the only result directly, not the generator like `collect`."
"async def message(self, target, message):
        """""" Message channel or user. """"""
        hostmask = self._format_user_mask(self.nickname)
        # Leeway.
        chunklen = protocol.MESSAGE_LENGTH_LIMIT - len(
            '{hostmask} PRIVMSG {target} :'.format(hostmask=hostmask, target=target)) - 25

        for line in message.replace('\r', '').split('\n'):
            for chunk in chunkify(line, chunklen):
                # Some IRC servers respond with ""412 Bot :No text to send"" on empty messages.
                await self.rawmsg('PRIVMSG', target, chunk or ' ')",Message channel or user.
"def job_stories(self, raw=False, limit=None):
        """"""Returns list of item ids of latest Job stories

        Args:
            limit (int): specifies the number of stories to be returned.
            raw (bool): Flag to indicate whether to transform all
                objects into raw json.

        Returns:
            `list` object containing ids of Job stories.

        """"""
        job_stories = self._get_stories('jobstories', limit)
        if raw:
            job_stories = [story.raw for story in job_stories]
        return job_stories","Returns list of item ids of latest Job stories

        Args:
            limit (int): specifies the number of stories to be returned.
            raw (bool): Flag to indicate whether to transform all
                objects into raw json.

        Returns:
            `list` object containing ids of Job stories."
"def main() -> int:
    """"""
    Utility to create and publish the Docker cache to Docker Hub
    :return:
    """"""
    # We need to be in the same directory than the script so the commands in the dockerfiles work as
    # expected. But the script can be invoked from a different path
    base = os.path.split(os.path.realpath(__file__))[0]
    os.chdir(base)

    logging.getLogger().setLevel(logging.DEBUG)
    logging.getLogger('botocore').setLevel(logging.INFO)
    logging.getLogger('boto3').setLevel(logging.INFO)
    logging.getLogger('urllib3').setLevel(logging.INFO)
    logging.getLogger('s3transfer').setLevel(logging.INFO)

    def script_name() -> str:
        return os.path.split(sys.argv[0])[1]

    logging.basicConfig(format='{}: %(asctime)-15s %(message)s'.format(script_name()))

    parser = argparse.ArgumentParser(description=""Utility for preserving and loading Docker cache"", epilog="""")
    parser.add_argument(""--docker-registry"",
                        help=""Docker hub registry name"",
                        type=str,
                        required=True)

    args = parser.parse_args()

    platforms = build_util.get_platforms()
    try:
        _login_dockerhub()
        return build_save_containers(platforms=platforms, registry=args.docker_registry, load_cache=True)
    finally:
        _logout_dockerhub()","Utility to create and publish the Docker cache to Docker Hub
    :return:"
"def candidate(cls):
        """"""The ``Candidate``.""""""
        return relationship(
            ""Candidate"",
            backref=backref(
                camel_to_under(cls.__name__) + ""s"",
                cascade=""all, delete-orphan"",
                cascade_backrefs=False,
            ),
            cascade_backrefs=False,
        )",The ``Candidate``.
"def intersect(self, other):
        """""" Return a new :class:`DataFrame` containing rows only in
        both this frame and another frame.

        This is equivalent to `INTERSECT` in SQL.
        """"""
        return DataFrame(self._jdf.intersect(other._jdf), self.sql_ctx)","Return a new :class:`DataFrame` containing rows only in
        both this frame and another frame.

        This is equivalent to `INTERSECT` in SQL."
"def groupby_freq(items, times, freq, wkst='SU'):
    """"""
    Group timeseries by frequency. The frequency must be a string in the
    following list: YEARLY, MONTHLY, WEEKLY, DAILY, HOURLY, MINUTELY or
    SECONDLY. The optional weekstart must be a string in the following list:
    MO, TU, WE, TH, FR, SA and SU.

    :param items: items in timeseries
    :param times: times corresponding to items
    :param freq: One of the ``dateutil.rrule`` frequency constants
    :type freq: str
    :param wkst: One of the ``dateutil.rrule`` weekday constants
    :type wkst: str
    :return: generator
    """"""
    timeseries = zip(times, items)  # timeseries map of items
    # create a key lambda to group timeseries by
    if freq.upper() == 'DAILY':
        def key(ts_): return ts_[0].day
    elif freq.upper() == 'WEEKLY':
        weekday = getattr(rrule, wkst.upper())  # weekday start
        # generator that searches times for weekday start
        days = (day for day in times if day.weekday() == weekday.weekday)
        day0 = days.next()  # first weekday start of all times

        def key(ts_): return (ts_[0] - day0).days // 7
    else:
        def key(ts_): return getattr(ts_[0], freq.lower()[:-2])
    for k, ts in itertools.groupby(timeseries, key):
        yield k, ts","Group timeseries by frequency. The frequency must be a string in the
    following list: YEARLY, MONTHLY, WEEKLY, DAILY, HOURLY, MINUTELY or
    SECONDLY. The optional weekstart must be a string in the following list:
    MO, TU, WE, TH, FR, SA and SU.

    :param items: items in timeseries
    :param times: times corresponding to items
    :param freq: One of the ``dateutil.rrule`` frequency constants
    :type freq: str
    :param wkst: One of the ``dateutil.rrule`` weekday constants
    :type wkst: str
    :return: generator"
"def _convert_units(devices):
    '''
    Updates the size and unit dictionary values with the new unit values

    devices
        List of device data objects
    '''
    if devices:
        for device in devices:
            if 'unit' in device and 'size' in device:
                device.update(
                    salt.utils.vmware.convert_to_kb(device['unit'], device['size']))
    else:
        return False
    return True","Updates the size and unit dictionary values with the new unit values

    devices
        List of device data objects"
"def modify_process_summary(self, pid=None, text='', append=False):
        '''
        modify_process_summary(self, pid=None, text='')

        Modifies the summary text of the process execution

        :Parameters:
        * *key* (`pid`) -- Identifier of an existing process
        * *key* (`text`) -- summary text
        * *append* (`boolean`) -- True to append to summary. False to override it.

        '''
        pid = self._get_pid(pid)

        if append:
            current_summary =  self.get_process_info(pid).get('summary') or ''
            modified_text = current_summary + '\n' + text
            text = modified_text

        request_data = {""id"": pid, ""data"": str(text)}
        return self._call_rest_api('post', '/processes/'+pid+'/summary', data=request_data, error='Failed to update process summary')","modify_process_summary(self, pid=None, text='')

        Modifies the summary text of the process execution

        :Parameters:
        * *key* (`pid`) -- Identifier of an existing process
        * *key* (`text`) -- summary text
        * *append* (`boolean`) -- True to append to summary. False to override it."
"def _parse_current_network_settings():
    '''
    Parse /etc/default/networking and return current configuration
    '''
    opts = salt.utils.odict.OrderedDict()
    opts['networking'] = ''

    if os.path.isfile(_DEB_NETWORKING_FILE):
        with salt.utils.files.fopen(_DEB_NETWORKING_FILE) as contents:
            for line in contents:
                salt.utils.stringutils.to_unicode(line)
                if line.startswith('#'):
                    continue
                elif line.startswith('CONFIGURE_INTERFACES'):
                    opts['networking'] = line.split('=', 1)[1].strip()

    hostname = _parse_hostname()
    domainname = _parse_domainname()
    searchdomain = _parse_searchdomain()

    opts['hostname'] = hostname
    opts['domainname'] = domainname
    opts['searchdomain'] = searchdomain
    return opts",Parse /etc/default/networking and return current configuration
"def all_connected_components(i,j):
    '''Associate each label in i with a component #
    
    This function finds all connected components given an array of
    associations between labels i and j using a depth-first search.
    
    i & j give the edges of the graph. The first step of the algorithm makes
    bidirectional edges, (i->j and j<-i), so it's best to only send the
    edges in one direction (although the algorithm can withstand duplicates).
    
    returns a label for each vertex up to the maximum named vertex in i.
    '''
    if len(i) == 0:
        return i
    i1 = np.hstack((i,j))
    j1 = np.hstack((j,i))
    order = np.lexsort((j1,i1))
    i=np.ascontiguousarray(i1[order],np.uint32)
    j=np.ascontiguousarray(j1[order],np.uint32)
    #
    # Get indexes and counts of edges per vertex
    #
    counts = np.ascontiguousarray(np.bincount(i.astype(int)),np.uint32)
    indexes = np.ascontiguousarray(np.cumsum(counts)-counts,np.uint32)
    #
    # This stores the lowest index # during the algorithm - the first
    # vertex to be labeled in a connected component.
    #
    labels = np.zeros(len(counts), np.uint32)
    _all_connected_components(i,j,indexes,counts,labels)
    return labels","Associate each label in i with a component #
    
    This function finds all connected components given an array of
    associations between labels i and j using a depth-first search.
    
    i & j give the edges of the graph. The first step of the algorithm makes
    bidirectional edges, (i->j and j<-i), so it's best to only send the
    edges in one direction (although the algorithm can withstand duplicates).
    
    returns a label for each vertex up to the maximum named vertex in i."
"def expect_response_body(self, schema):
        """"""*Updates the schema to validate the response body properties.*

        Expectations are effective for following requests in the test suite,
        or until they are reset or updated by using expectation keywords again.
        On the test suite level (suite setup), they are best used for expecting
        the endpoint wide properties that are common regardless of the tested
        HTTP method, and on the test case level (test setup) to merge in
        the HTTP method specific properties.

        `Expect Response Body` is intented to be used on the test case level,
        to validate that the response body has the expected properties for
        the particular HTTP method. Note that if something about response body
        has been already expected with `Expected Response`, using this keyword
        updates the expectations in terms of given response body properties.

        If the keyword is used, following HTTP keywords will fail if
        their response body is not valid against the expected schema.

        If the keyword is not used, and no schema is already expected with
        `Expect Response` for response ``body``, a new schema is inferred for it.
        Use `Output Schema` to output it and use it as an input to this keyword.

        *Tips*

        Regardless whether the HTTP method returns one (an object) or many
        (an array of objects), the validation of the object property types and features can usually be done to some extent on the test suite level
        with `Expect Response`, then extended on the test case level using this
        keyword. This helps in ensuring that the data model is unified between
        the different HTTP methods.

        *Examples*

        | `Expect Response Body` | ${CURDIR}/user_properties.json | # See `Output Schema` |
        | `Expect Response Body` | { ""required"": [""id"", ""token""] } | # Only these are required from this method |
        | `Expect Response Body` | { ""additionalProperties"": false } | # Nothing extra should be responded by this method |
        """"""
        response_properties = self.schema[""properties""][""response""][
            ""properties""
        ]
        if ""body"" in response_properties:
            response_properties[""body""].update(self._input_object(schema))
        else:
            response_properties[""body""] = self._input_object(schema)
        return response_properties[""body""]","*Updates the schema to validate the response body properties.*

        Expectations are effective for following requests in the test suite,
        or until they are reset or updated by using expectation keywords again.
        On the test suite level (suite setup), they are best used for expecting
        the endpoint wide properties that are common regardless of the tested
        HTTP method, and on the test case level (test setup) to merge in
        the HTTP method specific properties.

        `Expect Response Body` is intented to be used on the test case level,
        to validate that the response body has the expected properties for
        the particular HTTP method. Note that if something about response body
        has been already expected with `Expected Response`, using this keyword
        updates the expectations in terms of given response body properties.

        If the keyword is used, following HTTP keywords will fail if
        their response body is not valid against the expected schema.

        If the keyword is not used, and no schema is already expected with
        `Expect Response` for response ``body``, a new schema is inferred for it.
        Use `Output Schema` to output it and use it as an input to this keyword.

        *Tips*

        Regardless whether the HTTP method returns one (an object) or many
        (an array of objects), the validation of the object property types and features can usually be done to some extent on the test suite level
        with `Expect Response`, then extended on the test case level using this
        keyword. This helps in ensuring that the data model is unified between
        the different HTTP methods.

        *Examples*

        | `Expect Response Body` | ${CURDIR}/user_properties.json | # See `Output Schema` |
        | `Expect Response Body` | { ""required"": [""id"", ""token""] } | # Only these are required from this method |
        | `Expect Response Body` | { ""additionalProperties"": false } | # Nothing extra should be responded by this method |"
"def release():
    """"""Bump version, tag, build, gen docs.""""""
    if check_staged():
        raise EnvironmentError('There are staged changes, abort.')
    if check_unstaged():
        raise EnvironmentError('There are unstaged changes, abort.')
    bump()
    tag()
    build()
    doc_gen()
    puts(colored.yellow(""Remember to upload documentation and package:""))
    with indent(2):
        puts(colored.cyan(""shovel doc.upload""))
        puts(colored.cyan(""shovel version.upload""))","Bump version, tag, build, gen docs."
"def _add_sql_injection_strings(self, lib):
        '''
        .. note::

            The following mutations will probably not be detected
            unless some entity will check the response from the target
            on the validity of the DB.
        '''
        entries = [
            (""' or 1=1 --"", 'mssql - bypass check'),
            (""\'; desc users; --"", 'mysql - mssql - get desc users'),
            (""' or username is not NULL or username = '"", 'mysql - mssql - bypass check'),
            (""1 union all select 1,2,3,4,5,6,name from sysobjects where xtype = 'u' --"", 'mysql - mssql - get sysobjects'),
            (""` or `1`=`1"", 'oracle - bypass check'),
            (""' or '1'='1"", 'oracle - bypass check'),
        ]
        for (s, desc) in entries:
            lib.append((s, 'sqli - %s' % desc))",".. note::

            The following mutations will probably not be detected
            unless some entity will check the response from the target
            on the validity of the DB."
"def file_writelines_flush_sync(path, lines):
    """"""
    Fill file at @path with @lines then flush all buffers
    (Python and system buffers)
    """"""
    fp = open(path, 'w')
    try:
        fp.writelines(lines)
        flush_sync_file_object(fp)
    finally:
        fp.close()","Fill file at @path with @lines then flush all buffers
    (Python and system buffers)"
"def on_m_open_file(self,event):
        '''
        open orient.txt
        read the data
        display the data from the file in a new grid
        '''
        dlg = wx.FileDialog(
            self, message=""choose orient file"",
            defaultDir=self.WD,
            defaultFile="""",
            style=wx.FD_OPEN | wx.FD_CHANGE_DIR
            )
        if dlg.ShowModal() == wx.ID_OK:
            orient_file = dlg.GetPath()
            dlg.Destroy()
            new_data, dtype, keys = pmag.magic_read_dict(orient_file,
                                                         sort_by_this_name=""sample_name"",
                                                         return_keys=True)

            if len(new_data) > 0:
                self.orient_data={}
                self.orient_data=new_data
            #self.create_sheet()
            self.update_sheet()
            print(""-I- If you don't see a change in the spreadsheet, you may need to manually re-size the window"")","open orient.txt
        read the data
        display the data from the file in a new grid"
"def get_function_host(fn):
    """"""Destructure a given function into its host and its name.

    The 'host' of a function is a module, for methods it is usually its
    instance or its class. This is safe only for methods, for module wide,
    globally declared names it must be considered experimental.

    For all reasonable fn: ``getattr(*get_function_host(fn)) == fn``

    Returns tuple (host, fn-name)
    Otherwise should raise TypeError
    """"""

    obj = None
    try:
        name = fn.__name__
        obj = fn.__self__
    except AttributeError:
        pass

    if obj is None:
        # Due to how python imports work, everything that is global on a module
        # level must be regarded as not safe here. For now, we go for the extra
        # mile, TBC, because just specifying `os.path.exists` would be 'cool'.
        #
        # TLDR;:
        # E.g. `inspect.getmodule(os.path.exists)` returns `genericpath` bc
        # that's where `exists` is defined and comes from. But from the point
        # of view of the user `exists` always comes and is used from `os.path`
        # which points e.g. to `ntpath`. We thus must patch `ntpath`.
        # But that's the same for most imports::
        #
        #     # b.py
        #     from a import foo
        #
        # Now asking `getmodule(b.foo)` it tells you `a`, but we access and use
        # `b.foo` and we therefore must patch `b`.

        obj, name = find_invoking_frame_and_try_parse()
        # safety check!
        assert getattr(obj, name) == fn


    return obj, name","Destructure a given function into its host and its name.

    The 'host' of a function is a module, for methods it is usually its
    instance or its class. This is safe only for methods, for module wide,
    globally declared names it must be considered experimental.

    For all reasonable fn: ``getattr(*get_function_host(fn)) == fn``

    Returns tuple (host, fn-name)
    Otherwise should raise TypeError"
"def includeme(config):
    """"""
    Include `crabpy_pyramid` in this `Pyramid` application.

    :param pyramid.config.Configurator config: A Pyramid configurator.
    """"""

    settings = _parse_settings(config.registry.settings)
    base_settings = _get_proxy_settings(settings)

    # http caching tween
    if not settings.get('etag_tween_disabled', False):
        config.add_tween('crabpy_pyramid.conditional_http_tween_factory')

    # create cache
    root = settings.get('cache.file.root', '/tmp/dogpile_data')
    if not os.path.exists(root):
        os.makedirs(root)

    capakey_settings = dict(_filter_settings(settings, 'capakey.'),
                            **base_settings)
    if 'include' in capakey_settings:
        log.info(""The 'capakey.include' setting is deprecated. Capakey will ""
                 ""always be included."")
    log.info('Adding CAPAKEY Gateway.')
    config.add_renderer('capakey_listjson', capakey_json_list_renderer)
    config.add_renderer('capakey_itemjson', capakey_json_item_renderer)
    _build_capakey(config.registry, capakey_settings)
    config.add_request_method(get_capakey, 'capakey_gateway')
    config.add_directive('get_capakey', get_capakey)
    config.include('crabpy_pyramid.routes.capakey')
    config.scan('crabpy_pyramid.views.capakey')

    crab_settings = dict(_filter_settings(settings, 'crab.'), **base_settings)
    if crab_settings['include']:
        log.info('Adding CRAB Gateway.')
        del crab_settings['include']
        config.add_renderer('crab_listjson', crab_json_list_renderer)
        config.add_renderer('crab_itemjson', crab_json_item_renderer)
        _build_crab(config.registry, crab_settings)
        config.add_directive('get_crab', get_crab)
        config.add_request_method(get_crab, 'crab_gateway')
        config.include('crabpy_pyramid.routes.crab')
        config.scan('crabpy_pyramid.views.crab')","Include `crabpy_pyramid` in this `Pyramid` application.

    :param pyramid.config.Configurator config: A Pyramid configurator."
"def GetHashersInformation(cls):
    """"""Retrieves the hashers information.

    Returns:
      list[tuple]: containing:

          str: hasher name.
          str: hasher description.
    """"""
    hashers_information = []
    for _, hasher_class in cls.GetHasherClasses():
      description = getattr(hasher_class, 'DESCRIPTION', '')
      hashers_information.append((hasher_class.NAME, description))

    return hashers_information","Retrieves the hashers information.

    Returns:
      list[tuple]: containing:

          str: hasher name.
          str: hasher description."
"def rc_channels_override_encode(self, target_system, target_component, chan1_raw, chan2_raw, chan3_raw, chan4_raw, chan5_raw, chan6_raw, chan7_raw, chan8_raw):
                '''
                The RAW values of the RC channels sent to the MAV to override info
                received from the RC radio. A value of UINT16_MAX
                means no change to that channel. A value of 0 means
                control of that channel should be released back to the
                RC radio. The standard PPM modulation is as follows:
                1000 microseconds: 0%, 2000 microseconds: 100%.
                Individual receivers/transmitters might violate this
                specification.

                target_system             : System ID (uint8_t)
                target_component          : Component ID (uint8_t)
                chan1_raw                 : RC channel 1 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan2_raw                 : RC channel 2 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan3_raw                 : RC channel 3 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan4_raw                 : RC channel 4 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan5_raw                 : RC channel 5 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan6_raw                 : RC channel 6 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan7_raw                 : RC channel 7 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan8_raw                 : RC channel 8 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)

                '''
                return MAVLink_rc_channels_override_message(target_system, target_component, chan1_raw, chan2_raw, chan3_raw, chan4_raw, chan5_raw, chan6_raw, chan7_raw, chan8_raw)","The RAW values of the RC channels sent to the MAV to override info
                received from the RC radio. A value of UINT16_MAX
                means no change to that channel. A value of 0 means
                control of that channel should be released back to the
                RC radio. The standard PPM modulation is as follows:
                1000 microseconds: 0%, 2000 microseconds: 100%.
                Individual receivers/transmitters might violate this
                specification.

                target_system             : System ID (uint8_t)
                target_component          : Component ID (uint8_t)
                chan1_raw                 : RC channel 1 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan2_raw                 : RC channel 2 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan3_raw                 : RC channel 3 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan4_raw                 : RC channel 4 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan5_raw                 : RC channel 5 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan6_raw                 : RC channel 6 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan7_raw                 : RC channel 7 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)
                chan8_raw                 : RC channel 8 value, in microseconds. A value of UINT16_MAX means to ignore this field. (uint16_t)"
"def add_double_proxy_for(self, label: str, shape: Collection[int] = None) -> Vertex:
        """"""
        Creates a proxy vertex for the given label and adds to the sequence item
        """"""
        if shape is None:
            return Vertex._from_java_vertex(self.unwrap().addDoubleProxyFor(_VertexLabel(label).unwrap()))
        else:
            return Vertex._from_java_vertex(self.unwrap().addDoubleProxyFor(_VertexLabel(label).unwrap(), shape))",Creates a proxy vertex for the given label and adds to the sequence item
"def pattern(self, value):
        """"""
        Setter for **self.__pattern** attribute.

        :param value: Attribute value.
        :type value: unicode
        """"""

        if value is not None:
            assert type(value) in (unicode, QString), \
                ""'{0}' attribute: '{1}' type is not 'unicode' or 'QString'!"".format(""pattern"", value)
        self.__pattern = value","Setter for **self.__pattern** attribute.

        :param value: Attribute value.
        :type value: unicode"
"def split_size(size):
    '''Split the file size into several chunks.'''
    rem = size % CHUNK_SIZE
    if rem == 0:
        cnt = size // CHUNK_SIZE
    else:
        cnt = size // CHUNK_SIZE + 1

    chunks = []
    for i in range(cnt):
        pos = i * CHUNK_SIZE
        if i == cnt - 1:
            disp = size - pos
        else:
            disp = CHUNK_SIZE
        chunks.append((pos, disp))
    return chunks",Split the file size into several chunks.
"def read(self, size=-1):
        """"""
        Read and return up to size bytes,
        with at most one call to the underlying raw streams.

        Use at most one call to the underlying raw streams read method.

        Args:
            size (int): Number of bytes to read. -1 to read the
                stream until end.

        Returns:
            bytes: Object content
        """"""
        if not self._readable:
            raise UnsupportedOperation('read')

        # Checks if EOF
        if self._seek == self._size:
            return b''

        # Returns existing buffer with no copy
        if size == self._buffer_size:
            queue_index = self._seek

            # Starts initial preloading on first call
            if queue_index == 0:
                self._preload_range()

            # Get buffer from future
            with handle_os_exceptions():
                buffer = self._read_queue.pop(queue_index).result()

            # Append another buffer preload at end of queue
            buffer_size = self._buffer_size
            index = queue_index + buffer_size * self._max_buffers
            if index < self._size:
                self._read_queue[index] = self._workers.submit(
                    self._read_range, index, index + buffer_size)

            # Update seek
                self._seek += buffer_size
            else:
                self._seek = self._size

            return buffer

        # Uses a prealocated buffer
        if size != -1:
            buffer = bytearray(size)

        # Uses a mutable buffer
        else:
            buffer = bytearray()

        read_size = self.readinto(buffer)
        return memoryview(buffer)[:read_size].tobytes()","Read and return up to size bytes,
        with at most one call to the underlying raw streams.

        Use at most one call to the underlying raw streams read method.

        Args:
            size (int): Number of bytes to read. -1 to read the
                stream until end.

        Returns:
            bytes: Object content"
"def create(cls, name, address, inspected_service, secondary=None,
               balancing_mode='ha', proxy_service='generic', location=None,
               comment=None, add_x_forwarded_for=False, trust_host_header=False,
               **kw):
        """"""
        Create a Proxy Server element
        
        :param str name: name of proxy server element
        :param str address: address of element. Can be a single FQDN or comma separated
            list of IP addresses
        :param list secondary: list of secondary IP addresses
        :param str balancing_mode: how to balance traffic, valid options are
            ha (first available server), src, dst, srcdst (default: ha)
        :param str proxy_service: which proxy service to use for next hop, options
            are generic or forcepoint_ap-web_cloud
        :param str,Element location: location for this proxy server
        :param bool add_x_forwarded_for: add X-Forwarded-For header when using the
            Generic Proxy forwarding method (default: False)
        :param bool trust_host_header: trust the host header when using the Generic
            Proxy forwarding method (default: False)
        :param dict inspected_service: inspection services dict. Valid keys are
            service_type and port. Service type valid values are HTTP, HTTPS, FTP or SMTP
            and are case sensitive
        :param str comment: optional comment
        :param kw: keyword arguments are used to collect settings when the proxy_service
            value is forcepoint_ap-web_cloud. Valid keys are `fp_proxy_key`, 
            `fp_proxy_key_id`, `fp_proxy_user_id`. The fp_proxy_key is the password value.
            All other values are of type int
        """"""     
        json = {'name': name,
                'comment': comment,
                'secondary': secondary or [],
                'http_proxy': proxy_service,
                'balancing_mode': balancing_mode,
                'inspected_service': inspected_service,
                'trust_host_header': trust_host_header,
                'add_x_forwarded_for': add_x_forwarded_for,
                'location_ref': element_resolver(location)
            }
        addresses = address.split(',')
        json.update(address=addresses.pop(0))
        json.update(ip_address=addresses if 'ip_address' not in kw else kw['ip_address'])
        
        if proxy_service == 'forcepoint_ap-web_cloud':
            for key in ('fp_proxy_key', 'fp_proxy_key_id', 'fp_proxy_user_id'):
                if key not in kw:
                    raise CreateElementFailed('Missing required fp key when adding a '
                        'proxy server to forward to forcepoint. Missing key: %s' % key)
                json[key] = kw.get(key)
        
        return ElementCreator(cls, json)","Create a Proxy Server element
        
        :param str name: name of proxy server element
        :param str address: address of element. Can be a single FQDN or comma separated
            list of IP addresses
        :param list secondary: list of secondary IP addresses
        :param str balancing_mode: how to balance traffic, valid options are
            ha (first available server), src, dst, srcdst (default: ha)
        :param str proxy_service: which proxy service to use for next hop, options
            are generic or forcepoint_ap-web_cloud
        :param str,Element location: location for this proxy server
        :param bool add_x_forwarded_for: add X-Forwarded-For header when using the
            Generic Proxy forwarding method (default: False)
        :param bool trust_host_header: trust the host header when using the Generic
            Proxy forwarding method (default: False)
        :param dict inspected_service: inspection services dict. Valid keys are
            service_type and port. Service type valid values are HTTP, HTTPS, FTP or SMTP
            and are case sensitive
        :param str comment: optional comment
        :param kw: keyword arguments are used to collect settings when the proxy_service
            value is forcepoint_ap-web_cloud. Valid keys are `fp_proxy_key`, 
            `fp_proxy_key_id`, `fp_proxy_user_id`. The fp_proxy_key is the password value.
            All other values are of type int"
"def parse_formula_name(self, attribute_name):
        """"""
        Returns the starting date of a formula based on its name.

        Valid dated name formats are : 'formula', 'formula_YYYY', 'formula_YYYY_MM' and 'formula_YYYY_MM_DD' where YYYY, MM and DD are a year, month and day.

        By convention, the starting date of:
            - `formula` is `0001-01-01` (minimal date in Python)
            - `formula_YYYY` is `YYYY-01-01`
            - `formula_YYYY_MM` is `YYYY-MM-01`
        """"""

        def raise_error():
            raise ValueError(
                'Unrecognized formula name in variable ""{}"". Expecting ""formula_YYYY"" or ""formula_YYYY_MM"" or ""formula_YYYY_MM_DD where YYYY, MM and DD are year, month and day. Found: ""{}"".'
                .format(self.name, attribute_name))

        if attribute_name == FORMULA_NAME_PREFIX:
            return date.min

        FORMULA_REGEX = r'formula_(\d{4})(?:_(\d{2}))?(?:_(\d{2}))?$'  # YYYY or YYYY_MM or YYYY_MM_DD

        match = re.match(FORMULA_REGEX, attribute_name)
        if not match:
            raise_error()
        date_str = '-'.join([match.group(1), match.group(2) or '01', match.group(3) or '01'])

        try:
            return datetime.datetime.strptime(date_str, '%Y-%m-%d').date()
        except ValueError:  # formula_2005_99_99 for instance
            raise_error()","Returns the starting date of a formula based on its name.

        Valid dated name formats are : 'formula', 'formula_YYYY', 'formula_YYYY_MM' and 'formula_YYYY_MM_DD' where YYYY, MM and DD are a year, month and day.

        By convention, the starting date of:
            - `formula` is `0001-01-01` (minimal date in Python)
            - `formula_YYYY` is `YYYY-01-01`
            - `formula_YYYY_MM` is `YYYY-MM-01`"
"def max_temperature(self,  unit='kelvin'):
        """"""Returns a tuple containing the max value in the temperature
        series preceeded by its timestamp
        
        :param unit: the unit of measure for the temperature values. May be
            among: '*kelvin*' (default), '*celsius*' or '*fahrenheit*'
        :type unit: str
        :returns: a tuple
        :raises: ValueError when invalid values are provided for the unit of
            measure or the measurement series is empty
        """"""
        if unit not in ('kelvin', 'celsius', 'fahrenheit'):
            raise ValueError(""Invalid value for parameter 'unit'"")
        maximum = max(self._purge_none_samples(self.temperature_series()),
                   key=itemgetter(1))
        if unit == 'kelvin':
            result = maximum
        if unit == 'celsius':
            result = (maximum[0], temputils.kelvin_to_celsius(maximum[1]))
        if unit == 'fahrenheit':
            result = (maximum[0], temputils.kelvin_to_fahrenheit(maximum[1]))
        return result","Returns a tuple containing the max value in the temperature
        series preceeded by its timestamp
        
        :param unit: the unit of measure for the temperature values. May be
            among: '*kelvin*' (default), '*celsius*' or '*fahrenheit*'
        :type unit: str
        :returns: a tuple
        :raises: ValueError when invalid values are provided for the unit of
            measure or the measurement series is empty"
"def parse_graminit_h(self, filename):
        """"""Parse the .h file written by pgen.  (Internal)

        This file is a sequence of #define statements defining the
        nonterminals of the grammar as numbers.  We build two tables
        mapping the numbers to names and back.

        """"""
        try:
            f = open(filename)
        except IOError, err:
            print ""Can't open %s: %s"" % (filename, err)
            return False
        self.symbol2number = {}
        self.number2symbol = {}
        lineno = 0
        for line in f:
            lineno += 1
            mo = re.match(r""^#define\s+(\w+)\s+(\d+)$"", line)
            if not mo and line.strip():
                print ""%s(%s): can't parse %s"" % (filename, lineno,
                                                  line.strip())
            else:
                symbol, number = mo.groups()
                number = int(number)
                assert symbol not in self.symbol2number
                assert number not in self.number2symbol
                self.symbol2number[symbol] = number
                self.number2symbol[number] = symbol
        return True","Parse the .h file written by pgen.  (Internal)

        This file is a sequence of #define statements defining the
        nonterminals of the grammar as numbers.  We build two tables
        mapping the numbers to names and back."
"def bandnames(self, names):
        """"""
        set the names of the raster bands

        Parameters
        ----------
        names: list of str
            the names to be set; must be of same length as the number of bands

        Returns
        -------

        """"""
        if not isinstance(names, list):
            raise TypeError('the names to be set must be of type list')
        if len(names) != self.bands:
            raise ValueError(
                'length mismatch of names to be set ({}) and number of bands ({})'.format(len(names), self.bands))
        self.__bandnames = names","set the names of the raster bands

        Parameters
        ----------
        names: list of str
            the names to be set; must be of same length as the number of bands

        Returns
        -------"
"def format(self, options=None):
        """"""
        Format this diagnostic for display. The options argument takes
        Diagnostic.Display* flags, which can be combined using bitwise OR. If
        the options argument is not provided, the default display options will
        be used.
        """"""
        if options is None:
            options = conf.lib.clang_defaultDiagnosticDisplayOptions()
        if options & ~Diagnostic._FormatOptionsMask:
            raise ValueError('Invalid format options')
        return conf.lib.clang_formatDiagnostic(self, options)","Format this diagnostic for display. The options argument takes
        Diagnostic.Display* flags, which can be combined using bitwise OR. If
        the options argument is not provided, the default display options will
        be used."
"def timedelta_range(start=None, end=None, periods=None, freq=None,
                    name=None, closed=None):
    """"""
    Return a fixed frequency TimedeltaIndex, with day as the default
    frequency

    Parameters
    ----------
    start : string or timedelta-like, default None
        Left bound for generating timedeltas
    end : string or timedelta-like, default None
        Right bound for generating timedeltas
    periods : integer, default None
        Number of periods to generate
    freq : string or DateOffset, default 'D'
        Frequency strings can have multiples, e.g. '5H'
    name : string, default None
        Name of the resulting TimedeltaIndex
    closed : string, default None
        Make the interval closed with respect to the given frequency to
        the 'left', 'right', or both sides (None)

    Returns
    -------
    rng : TimedeltaIndex

    Notes
    -----
    Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
    exactly three must be specified. If ``freq`` is omitted, the resulting
    ``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
    ``start`` and ``end`` (closed on both sides).

    To learn more about the frequency strings, please see `this link
    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.

    Examples
    --------

    >>> pd.timedelta_range(start='1 day', periods=4)
    TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],
                   dtype='timedelta64[ns]', freq='D')

    The ``closed`` parameter specifies which endpoint is included.  The default
    behavior is to include both endpoints.

    >>> pd.timedelta_range(start='1 day', periods=4, closed='right')
    TimedeltaIndex(['2 days', '3 days', '4 days'],
                   dtype='timedelta64[ns]', freq='D')

    The ``freq`` parameter specifies the frequency of the TimedeltaIndex.
    Only fixed frequencies can be passed, non-fixed frequencies such as
    'M' (month end) will raise.

    >>> pd.timedelta_range(start='1 day', end='2 days', freq='6H')
    TimedeltaIndex(['1 days 00:00:00', '1 days 06:00:00', '1 days 12:00:00',
                    '1 days 18:00:00', '2 days 00:00:00'],
                   dtype='timedelta64[ns]', freq='6H')

    Specify ``start``, ``end``, and ``periods``; the frequency is generated
    automatically (linearly spaced).

    >>> pd.timedelta_range(start='1 day', end='5 days', periods=4)
    TimedeltaIndex(['1 days 00:00:00', '2 days 08:00:00', '3 days 16:00:00',
                '5 days 00:00:00'],
               dtype='timedelta64[ns]', freq=None)
    """"""
    if freq is None and com._any_none(periods, start, end):
        freq = 'D'

    freq, freq_infer = dtl.maybe_infer_freq(freq)
    tdarr = TimedeltaArray._generate_range(start, end, periods, freq,
                                           closed=closed)
    return TimedeltaIndex._simple_new(tdarr._data, freq=tdarr.freq, name=name)","Return a fixed frequency TimedeltaIndex, with day as the default
    frequency

    Parameters
    ----------
    start : string or timedelta-like, default None
        Left bound for generating timedeltas
    end : string or timedelta-like, default None
        Right bound for generating timedeltas
    periods : integer, default None
        Number of periods to generate
    freq : string or DateOffset, default 'D'
        Frequency strings can have multiples, e.g. '5H'
    name : string, default None
        Name of the resulting TimedeltaIndex
    closed : string, default None
        Make the interval closed with respect to the given frequency to
        the 'left', 'right', or both sides (None)

    Returns
    -------
    rng : TimedeltaIndex

    Notes
    -----
    Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
    exactly three must be specified. If ``freq`` is omitted, the resulting
    ``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
    ``start`` and ``end`` (closed on both sides).

    To learn more about the frequency strings, please see `this link
    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.

    Examples
    --------

    >>> pd.timedelta_range(start='1 day', periods=4)
    TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],
                   dtype='timedelta64[ns]', freq='D')

    The ``closed`` parameter specifies which endpoint is included.  The default
    behavior is to include both endpoints.

    >>> pd.timedelta_range(start='1 day', periods=4, closed='right')
    TimedeltaIndex(['2 days', '3 days', '4 days'],
                   dtype='timedelta64[ns]', freq='D')

    The ``freq`` parameter specifies the frequency of the TimedeltaIndex.
    Only fixed frequencies can be passed, non-fixed frequencies such as
    'M' (month end) will raise.

    >>> pd.timedelta_range(start='1 day', end='2 days', freq='6H')
    TimedeltaIndex(['1 days 00:00:00', '1 days 06:00:00', '1 days 12:00:00',
                    '1 days 18:00:00', '2 days 00:00:00'],
                   dtype='timedelta64[ns]', freq='6H')

    Specify ``start``, ``end``, and ``periods``; the frequency is generated
    automatically (linearly spaced).

    >>> pd.timedelta_range(start='1 day', end='5 days', periods=4)
    TimedeltaIndex(['1 days 00:00:00', '2 days 08:00:00', '3 days 16:00:00',
                '5 days 00:00:00'],
               dtype='timedelta64[ns]', freq=None)"
"def qteKillWindow(self, windowObj: QtmacsWindow=None):
        """"""
        Kill the specified window (applets inside the window are not
        deleted).

        .. note:: The method does nothing if ``windowObj`` is the only
           window left.

        |Args|

        * *windowObj* (**QtmacsWindow**): window object to delete.

        |Returns|

        * **None**

        |Raises|

        * **QtmacsArgumentError** if at least one argument has an invalid type.
        """"""
        # Use the currently active window if none was specified.
        if windowObj is None:
            windowObj = self.qteActiveWindow()
            if windowObj is None:
                msg = ('Cannot kill the currently active window because'
                       ' it does not exist')
                self.qteLogger.error(msg, stack_info=True)
                return

        # If Qtmacs shows only one window then do nothing, because
        # deleting this window is akin to quitting Qtmacs without
        # going through the proper shutdown process.
        if len(self._qteWindowList) <= 1:
            msg = 'The last available window cannot be deleted.'
            self.qteLogger.error(msg, stack_info=True)
            return

        # Remove the window from the list.
        try:
            self._qteWindowList.remove(windowObj)
        except ValueError:
            msg = 'Cannot delete window with ID <b>{}</b> because it'
            msg += ' does not exist.'.format(windowObj._qteWindowID)
            self.qteLogger.error(msg, stack_info=True)
            return

        # Shorthand
        activeWindow = self.qteActiveWindow()

        # If the window to delete is currently active then switch the
        # focus to the first window in the list (not particularly
        # smart, but will do for now).
        if windowObj is activeWindow:
            activeWindow = self._qteWindowList[0]
            self.activeWindow.activateWindow()

            # Since the window with the active applet is deleted,
            # try to find a new applet to activate instead. It is
            # ok if there is no such applet.
            self._qteActiveApplet = self.qteNextApplet(
                windowObj=self._qteWindowList[0])

        # Move the mini applet if it is currently in the doomed window.
        if self._qteMiniApplet is not None:
            if self._qteMiniApplet.qteParentWindow() is windowObj:
                # Re-parent the mini applet to the splitter in the new
                # window.
                activeWindow.qteLayoutSplitter.addWidget(self._qteMiniApplet)
                self._qteMiniApplet.show(True)

                # Give the focus to the first focusable widget (if any).
                wid = self._qteMiniApplet.qteNextWidget(numSkip=0)
                self._qteMiniApplet.qteMakeWidgetActive(wid)
                self.qteMakeAppletActive(self._qteMiniApplet)

        # Compile the list of visible applets in the doomed window
        # (use ``qteParentWindow`` to find all applets that have the
        # doomed window as parent).
        app_list = [_ for _ in self._qteAppletList
                    if _.qteParentWindow() == windowObj]

        # Hide all visible applets in the doomed window. Do not use
        # ``qteRemoveFromLayout`` for this since that method attempts
        # to replace the removed applet with another one, ie. the
        # doomed window would once again contain an applet.
        for app_obj in app_list:
            if not self.qteIsMiniApplet(app_obj):
                app_obj.hide(True)

        # Ensure the focus manager is triggered and the window deleted
        # once the event loop has regained control.
        windowObj.deleteLater()","Kill the specified window (applets inside the window are not
        deleted).

        .. note:: The method does nothing if ``windowObj`` is the only
           window left.

        |Args|

        * *windowObj* (**QtmacsWindow**): window object to delete.

        |Returns|

        * **None**

        |Raises|

        * **QtmacsArgumentError** if at least one argument has an invalid type."
"def render_authenticateLinks(self, ctx, data):
        """"""
        For unauthenticated users, add login and signup links to the given tag.
        For authenticated users, remove the given tag from the output.

        When necessary, the I{signup-link} pattern will be loaded from the tag.
        Each copy of it will have I{prompt} and I{url} slots filled.  The list
        of copies will be added as children of the tag.
        """"""
        if self.username is not None:
            return ''
        # there is a circular import here which should probably be avoidable,
        # since we don't actually need signup links on the signup page.  on the
        # other hand, maybe we want to eventually put those there for
        # consistency.  for now, this import is easiest, and although it's a
        # ""friend"" API, which I dislike, it doesn't seem to cause any real
        # problems...  -glyph
        from xmantissa.signup import _getPublicSignupInfo

        IQ = inevow.IQ(ctx.tag)
        signupPattern = IQ.patternGenerator('signup-link')

        signups = []
        for (prompt, url) in _getPublicSignupInfo(self.store):
            signups.append(signupPattern.fillSlots(
                    'prompt', prompt).fillSlots(
                    'url', url))

        return ctx.tag[signups]","For unauthenticated users, add login and signup links to the given tag.
        For authenticated users, remove the given tag from the output.

        When necessary, the I{signup-link} pattern will be loaded from the tag.
        Each copy of it will have I{prompt} and I{url} slots filled.  The list
        of copies will be added as children of the tag."
"def _connect(self):
        """"""Connect to Squid Proxy Manager interface.""""""
        if sys.version_info[:2] < (2,6):
            self._conn = httplib.HTTPConnection(self._host, self._port)
        else:
            self._conn = httplib.HTTPConnection(self._host, self._port, 
                                                False, defaultTimeout)",Connect to Squid Proxy Manager interface.
"def get_group(self, name, user_name=None):
        """"""
        Get information on the given group or whether or not a user is a member
        of the group.

        Args:
            name (string): Name of group to query.
            user_name (optional[string]): Supply None if not interested in
            determining if user is a member of the given group.

        Returns:
            (mixed): Dictionary if getting group information or bool if a user
                name is supplied.

        Raises:
            requests.HTTPError on failure.
        """"""
        self.project_service.set_auth(self._token_project)
        return self.project_service.get_group(name, user_name)","Get information on the given group or whether or not a user is a member
        of the group.

        Args:
            name (string): Name of group to query.
            user_name (optional[string]): Supply None if not interested in
            determining if user is a member of the given group.

        Returns:
            (mixed): Dictionary if getting group information or bool if a user
                name is supplied.

        Raises:
            requests.HTTPError on failure."
"def create_table(self, names=None):
        """"""Create an astropy Table object with the contents of the ROI model.
        """"""

        scan_shape = (1,)
        for src in self._srcs:
            scan_shape = max(scan_shape, src['dloglike_scan'].shape)

        tab = create_source_table(scan_shape)
        for s in self._srcs:
            if names is not None and s.name not in names:
                continue
            s.add_to_table(tab)

        return tab",Create an astropy Table object with the contents of the ROI model.
"def load_credentials(self, profile):
        """"""
        Loads crentials for a given profile. Profiles are stored in
        ~/.db.py_s3_{profile_name} and are a base64 encoded JSON file. This is
        not to say this a secure way to store sensitive data, but it will
        probably stop your little sister from spinning up EC2 instances.

        Parameters
        ----------
        profile: str
            identifier/name for your database (i.e. ""dev"", ""prod"")
        """"""
        f = profile_path(S3_PROFILE_ID, profile)
        if os.path.exists(f):
            creds = load_profile(f)
            if 'access_key' not in creds:
                raise Exception(""`access_key` not found in s3 profile '{0}'"".format(profile))
            self.access_key = creds['access_key']
            if 'access_key' not in creds:
                raise Exception(""`secret_key` not found in s3 profile '{0}'"".format(profile))
            self.secret_key = creds['secret_key']","Loads crentials for a given profile. Profiles are stored in
        ~/.db.py_s3_{profile_name} and are a base64 encoded JSON file. This is
        not to say this a secure way to store sensitive data, but it will
        probably stop your little sister from spinning up EC2 instances.

        Parameters
        ----------
        profile: str
            identifier/name for your database (i.e. ""dev"", ""prod"")"
"def launch_notebook(request, username, notebook_context):
    """"""Renders a IPython Notebook frame wrapper.

    Starts or reattachs ot an existing Notebook session.
    """"""
    # The notebook manage now tries too hard to get the port allocated for the notebook user, making it slow
    # TODO: Manage a proper state e.g. using Redis
    notebook_info = launch_on_demand(request, username, notebook_context)

    # Jump to the detault notebook
    proxy_route = request.route_url(""notebook_proxy"", remainder=""notebooks/{}"".format(notebook_info[""notebook_name""]))
    proxy_route = route_to_alt_domain(request, proxy_route)

    return HTTPFound(proxy_route)","Renders a IPython Notebook frame wrapper.

    Starts or reattachs ot an existing Notebook session."
"def main(argv=sys.argv):
    # type: (List[str]) -> int
    """"""Parse and check the command line arguments.""""""
    parser = optparse.OptionParser(
        usage=""""""\
usage: %prog [options] -o <output_path> <module_path> [exclude_pattern, ...]

Look recursively in <module_path> for Python modules and packages and create
one reST file with automodule directives per package in the <output_path>.

The <exclude_pattern>s can be file and/or directory patterns that will be
excluded from generation.

Note: By default this script will not overwrite already created files."""""")

    parser.add_option('-o', '--output-dir', action='store', dest='destdir',
                      help='Directory to place all output', default='api')
    parser.add_option('-s', '--source-dir', action='store', dest='srcdir',
                      help='Documentation source directory', default=BASEDIR)
    parser.add_option('-n', '--docname', action='store', dest='docname',
                      help='Index document name', default='api')
    parser.add_option('-l', '--follow-links', action='store_true',
                      dest='followlinks', default=False,
                      help='Follow symbolic links. Powerful when combined '
                      'with collective.recipe.omelette.')
    parser.add_option('-P', '--private', action='store_true',
                      dest='includeprivate',
                      help='Include ""_private"" modules')
    parser.add_option('--implicit-namespaces', action='store_true',
                      dest='implicit_namespaces',
                      help='Interpret module paths according to PEP-0420 '
                           'implicit namespaces specification')
    parser.add_option('--version', action='store_true', dest='show_version',
                      help='Show version information and exit')
    parser.add_option('--clean', action='store_true', dest='cleanup',
                      help='Clean up generated files and exit')
    group = parser.add_option_group('Extension options')
    for ext in EXTENSIONS:
        group.add_option('--ext-' + ext, action='store_true',
                         dest='ext_' + ext, default=False,
                         help='enable %s extension' % ext)

    (opts, args) = parser.parse_args(argv[1:])

    # Make this more explicitly the current directory.
    if not opts.srcdir:
        opts.srcdir = '.'

    if opts.show_version:
        print('Sphinx (sphinx-apidoc) %s' % __display_version__)
        return 0

    if opts.cleanup:
        print(""Removing generated API docs from '{}'..."".format(opts.srcdir))
        return cleanup_api_docs(opts)

    if not args:
        parser.error('A package path is required.')

    opts.rootpath, opts.excludes = args[0], args[1:]
    return generate_api_docs(opts)",Parse and check the command line arguments.
"def Delete(self, n = 1, dl = 0):
        """"""n
        """"""
        self.Delay(dl)
        self.keyboard.tap_key(self.keyboard.delete_key, n)",n
"def _parse_hwtype(self):
        """"""Convert the numerical hardware id to a chip name.""""""

        self.chip_name = KNOWN_HARDWARE_TYPES.get(self.hw_type, ""Unknown Chip (type=%d)"" % self.hw_type)",Convert the numerical hardware id to a chip name.
"def _cast(self, _input, _output):
        """"""
        Transforms a pair of input/output into the real slim shoutput.

        :param _input: Bag
        :param _output: mixed
        :return: Bag
        """"""

        if isenvelope(_output):
            _output, _flags, _options = _output.unfold()
        else:
            _flags, _options = [], {}

        if len(_flags):
            # TODO: parse flags to check constraints are respected (like not modified alone, etc.)

            if F_NOT_MODIFIED in _flags:
                return _input

            if F_INHERIT in _flags:
                if self._output_type is None:
                    self._output_type = concat_types(
                        self._input_type, self._input_length, self._output_type, len(_output)
                    )
                _output = _input + ensure_tuple(_output)

        if not self._output_type:
            if issubclass(type(_output), tuple):
                self._output_type = type(_output)

        return ensure_tuple(_output, cls=self._output_type)","Transforms a pair of input/output into the real slim shoutput.

        :param _input: Bag
        :param _output: mixed
        :return: Bag"
"def remove_repo(self, repo, team):
        """"""Remove ``repo`` from ``team``.

        :param str repo: (required), form: 'user/repo'
        :param str team: (required)
        :returns: bool
        """"""
        for t in self.iter_teams():
            if team == t.name:
                return t.remove_repo(repo)
        return False","Remove ``repo`` from ``team``.

        :param str repo: (required), form: 'user/repo'
        :param str team: (required)
        :returns: bool"
"def _merge_map(key, values, partial):
  """"""A map function used in merge phase.

  Stores (key, values) into KeyValues proto and yields its serialization.

  Args:
    key: values key.
    values: values themselves.
    partial: True if more values for this key will follow. False otherwise.

  Yields:
    The proto.
  """"""
  proto = kv_pb.KeyValues()
  proto.set_key(key)
  proto.value_list().extend(values)
  yield proto.Encode()","A map function used in merge phase.

  Stores (key, values) into KeyValues proto and yields its serialization.

  Args:
    key: values key.
    values: values themselves.
    partial: True if more values for this key will follow. False otherwise.

  Yields:
    The proto."
"def plot_intrusion_curve(self, fig=None):
        r""""""
        Plot the percolation curve as the invader volume or number fraction vs
        the applied capillary pressure.

        """"""
        # Begin creating nicely formatted plot
        x, y = self.get_intrusion_data()
        if fig is None:
            fig = plt.figure()
        plt.semilogx(x, y, 'ko-')
        plt.ylabel('Invading Phase Saturation')
        plt.xlabel('Capillary Pressure')
        plt.grid(True)
        return fig","r""""""
        Plot the percolation curve as the invader volume or number fraction vs
        the applied capillary pressure."
"def see(obj=DEFAULT_ARG, *args, **kwargs):
    """"""
    see(obj=anything)

    Show the features and attributes of an object.

    This function takes a single argument, ``obj``, which can be of any type.
    A summary of the object is printed immediately in the Python interpreter.
    For example::

        >>> see([])
            []            in            +             +=            *
            *=            <             <=            ==            !=
            >             >=            dir()         hash()
            help()        iter()        len()         repr()
            reversed()    str()         .append()     .clear()
            .copy()       .count()      .extend()     .index()
            .insert()     .pop()        .remove()     .reverse()
            .sort()

    If this function is run without arguments, it will instead list the objects
    that are available in the current scope. ::

        >>> see()
            os        random    see()     sys

    The return value is an instance of :class:`SeeResult`.
    """"""
    use_locals = obj is DEFAULT_ARG

    if use_locals:
        # Get the local scope from the caller's stack frame.
        # Typically this is the scope of an interactive Python session.
        obj = Namespace(inspect.currentframe().f_back.f_locals)

    tokens = []
    attrs = dir(obj)

    if not use_locals:

        for name, func in INSPECT_FUNCS:
            if func(obj):
                tokens.append(name)

        for feature in FEATURES:
            if feature.match(obj, attrs):
                tokens.append(feature.symbol)

    for attr in filter(lambda a: not a.startswith('_'), attrs):
        try:
            prop = getattr(obj, attr)
        except (AttributeError, Exception):  # pylint: disable=broad-except
            prop = SeeError()
        action = output.display_name(name=attr, obj=prop, local=use_locals)
        tokens.append(action)

    if args or kwargs:
        tokens = handle_deprecated_args(tokens, args, kwargs)

    return output.SeeResult(tokens)","see(obj=anything)

    Show the features and attributes of an object.

    This function takes a single argument, ``obj``, which can be of any type.
    A summary of the object is printed immediately in the Python interpreter.
    For example::

        >>> see([])
            []            in            +             +=            *
            *=            <             <=            ==            !=
            >             >=            dir()         hash()
            help()        iter()        len()         repr()
            reversed()    str()         .append()     .clear()
            .copy()       .count()      .extend()     .index()
            .insert()     .pop()        .remove()     .reverse()
            .sort()

    If this function is run without arguments, it will instead list the objects
    that are available in the current scope. ::

        >>> see()
            os        random    see()     sys

    The return value is an instance of :class:`SeeResult`."
"def pop_many(a_dict: Dict[str, Any], *args: str,  **kwargs) -> Dict[str, Any]:
    """"""Pop multiple items from a dictionary.
    
    Parameters
    ----------
    a_dict : Dictionary from which the items will popped
    args: Keys which will be popped (and not included if not present)
    kwargs: Keys + default value pairs (if key not found, this default is included)

    Returns
    -------
    A dictionary of collected items.
    """"""
    result = {}
    for arg in args:
        if arg in a_dict:
            result[arg] = a_dict.pop(arg)
    for key, value in kwargs.items():
        result[key] = a_dict.pop(key, value)
    return result","Pop multiple items from a dictionary.
    
    Parameters
    ----------
    a_dict : Dictionary from which the items will popped
    args: Keys which will be popped (and not included if not present)
    kwargs: Keys + default value pairs (if key not found, this default is included)

    Returns
    -------
    A dictionary of collected items."
"def log_path(self, new_path):
        """"""Setter for `log_path`, use with caution.""""""
        if self.has_active_service:
            raise DeviceError(
                self,
                'Cannot change `log_path` when there is service running.')
        old_path = self._log_path
        if new_path == old_path:
            return
        if os.listdir(new_path):
            raise DeviceError(
                self, 'Logs already exist at %s, cannot override.' % new_path)
        if os.path.exists(old_path):
            # Remove new path so copytree doesn't complain.
            shutil.rmtree(new_path, ignore_errors=True)
            shutil.copytree(old_path, new_path)
            shutil.rmtree(old_path, ignore_errors=True)
        self._log_path = new_path","Setter for `log_path`, use with caution."
"def execute(self, i, o):
        """"""
        Executes the command.

        :type i: cleo.inputs.input.Input
        :type o: cleo.outputs.output.Output
        """"""
        super(StatusCommand, self).execute(i, o)

        database = i.get_option('database')
        repository = DatabaseMigrationRepository(self._resolver, 'migrations')

        migrator = Migrator(repository, self._resolver)

        if not migrator.repository_exists():
            return o.writeln('<error>No migrations found</error>')

        self._prepare_database(migrator, database, i, o)

        path = i.get_option('path')

        if path is None:
            path = self._get_migration_path()

        ran = migrator.get_repository().get_ran()

        migrations = []
        for migration in migrator._get_migration_files(path):
            if migration in ran:
                migrations.append(['<fg=cyan>%s</>' % migration, '<info>Yes</info>'])
            else:
                migrations.append(['<fg=cyan>%s</>' % migration, '<fg=red>No</>'])

        if migrations:
            table = self.get_helper('table')
            table.set_headers(['Migration', 'Ran?'])
            table.set_rows(migrations)
            table.render(o)
        else:
            return o.writeln('<error>No migrations found</error>')

        for note in migrator.get_notes():
            o.writeln(note)","Executes the command.

        :type i: cleo.inputs.input.Input
        :type o: cleo.outputs.output.Output"
"def get_audits():
    """"""Get OS hardening apt audits.

    :returns:  dictionary of audits
    """"""
    audits = [AptConfig([{'key': 'APT::Get::AllowUnauthenticated',
                          'expected': 'false'}])]

    settings = get_settings('os')
    clean_packages = settings['security']['packages_clean']
    if clean_packages:
        security_packages = settings['security']['packages_list']
        if security_packages:
            audits.append(RestrictedPackages(security_packages))

    return audits","Get OS hardening apt audits.

    :returns:  dictionary of audits"
"def create_network_configuration_files(self, file_path, guest_networks,
                                           first, active=False):
        """"""Generate network configuration files for guest vm
        :param list guest_networks:  a list of network info for the guest.
               It has one dictionary that contain some of the below keys for
               each network, the format is:
               {'ip_addr': (str) IP address,
               'dns_addr': (list) dns addresses,
               'gateway_addr': (str) gateway address,
               'cidr': (str) cidr format
               'nic_vdev': (str) VDEV of the nic}

               Example for guest_networks:
               [{'ip_addr': '192.168.95.10',
               'dns_addr': ['9.0.2.1', '9.0.3.1'],
               'gateway_addr': '192.168.95.1',
               'cidr': ""192.168.95.0/24"",
               'nic_vdev': '1000'},
               {'ip_addr': '192.168.96.10',
               'dns_addr': ['9.0.2.1', '9.0.3.1'],
               'gateway_addr': '192.168.96.1',
               'cidr': ""192.168.96.0/24"",
               'nic_vdev': '1003}]
        """"""
        cfg_files = []
        cmd_strings = ''
        network_config_file_name = self._get_network_file()
        network_cfg_str = 'auto lo\n'
        network_cfg_str += 'iface lo inet loopback\n'
        net_enable_cmd = ''
        if first:
            clean_cmd = self._get_clean_command()
        else:
            clean_cmd = ''
            network_cfg_str = ''

        for network in guest_networks:
            base_vdev = network['nic_vdev'].lower()
            network_hw_config_fname = self._get_device_filename(base_vdev)
            network_hw_config_str = self._get_network_hw_config_str(base_vdev)
            cfg_files.append((network_hw_config_fname, network_hw_config_str))
            (cfg_str, dns_str) = self._generate_network_configuration(network,
                                    base_vdev)
            LOG.debug('Network configure file content is: %s', cfg_str)
            network_cfg_str += cfg_str
            if len(dns_str) > 0:
                network_cfg_str += dns_str
        if first:
            cfg_files.append((network_config_file_name, network_cfg_str))
        else:
            cmd_strings = ('echo ""%s"" >>%s\n' % (network_cfg_str,
                                                 network_config_file_name))
        return cfg_files, cmd_strings, clean_cmd, net_enable_cmd","Generate network configuration files for guest vm
        :param list guest_networks:  a list of network info for the guest.
               It has one dictionary that contain some of the below keys for
               each network, the format is:
               {'ip_addr': (str) IP address,
               'dns_addr': (list) dns addresses,
               'gateway_addr': (str) gateway address,
               'cidr': (str) cidr format
               'nic_vdev': (str) VDEV of the nic}

               Example for guest_networks:
               [{'ip_addr': '192.168.95.10',
               'dns_addr': ['9.0.2.1', '9.0.3.1'],
               'gateway_addr': '192.168.95.1',
               'cidr': ""192.168.95.0/24"",
               'nic_vdev': '1000'},
               {'ip_addr': '192.168.96.10',
               'dns_addr': ['9.0.2.1', '9.0.3.1'],
               'gateway_addr': '192.168.96.1',
               'cidr': ""192.168.96.0/24"",
               'nic_vdev': '1003}]"
"def read_elastic_tensor(self):
        """"""
        Parse the elastic tensor data.

        Returns:
            6x6 array corresponding to the elastic tensor from the OUTCAR.
        """"""
        header_pattern = r""TOTAL ELASTIC MODULI \(kBar\)\s+"" \
                         r""Direction\s+([X-Z][X-Z]\s+)+"" \
                         r""\-+""
        row_pattern = r""[X-Z][X-Z]\s+"" + r""\s+"".join([r""(\-*[\.\d]+)""] * 6)
        footer_pattern = r""\-+""
        et_table = self.read_table_pattern(header_pattern, row_pattern,
                                           footer_pattern, postprocess=float)
        self.data[""elastic_tensor""] = et_table","Parse the elastic tensor data.

        Returns:
            6x6 array corresponding to the elastic tensor from the OUTCAR."
"def get_one(self, context, name):
        """"""
        Returns a function if it is registered, the context is ignored.
        """"""
        try:
            return self.functions[name]
        except KeyError:
            raise FunctionNotFound(name)","Returns a function if it is registered, the context is ignored."
"def country_list_maker():
    """"""
    Helper function to return dictionary of countries in {""country"" : ""iso""} form.
    """"""
    cts = {""Afghanistan"":""AFG"", ""land Islands"":""ALA"", ""Albania"":""ALB"", ""Algeria"":""DZA"",
    ""American Samoa"":""ASM"", ""Andorra"":""AND"", ""Angola"":""AGO"", ""Anguilla"":""AIA"",
    ""Antarctica"":""ATA"", ""Antigua Barbuda"":""ATG"", ""Argentina"":""ARG"",
    ""Armenia"":""ARM"", ""Aruba"":""ABW"", ""Ascension Island"":""NA"", ""Australia"":""AUS"",
    ""Austria"":""AUT"", ""Azerbaijan"":""AZE"", ""Bahamas"":""BHS"", ""Bahrain"":""BHR"",
    ""Bangladesh"":""BGD"", ""Barbados"":""BRB"", ""Belarus"":""BLR"", ""Belgium"":""BEL"",
    ""Belize"":""BLZ"", ""Benin"":""BEN"", ""Bermuda"":""BMU"", ""Bhutan"":""BTN"",
    ""Bolivia"":""BOL"", ""Bosnia Herzegovina"":""BIH"",
    ""Botswana"":""BWA"", ""Bouvet Island"":""BVT"", ""Brazil"":""BRA"",
    ""Britain"":""GBR"", ""Great Britain"":""GBR"",
    ""British Virgin Islands"":""VGB"", ""Brunei"":""BRN"", ""Bulgaria"":""BGR"", ""Burkina Faso"":""BFA"",
    ""Burundi"":""BDI"", ""Cambodia"":""KHM"", ""Cameroon"":""CMR"",
    ""Canada"":""CAN"",""Cape Verde"":""CPV"", ""Cayman Islands"":""CYM"",
    ""Central African Republic"":""CAF"", ""Chad"":""TCD"", ""Chile"":""CHL"", ""China"":""CHN"",
    ""Cocos Islands"":""CCK"", ""Colombia"":""COL"",
    ""Comoros"":""COM"",     ""Republic of Congo"":""COG"", ""Cook Islands"":""COK"",
    ""Costa Rica"":""CRI"", ""Cote Ivoire"":""CIV"", ""Ivory Coast"":""CIV"",""Croatia"":""HRV"", ""Cuba"":""CUB"",
    ""Curaao"":""CUW"", ""Cyprus"":""CYP"", ""Czech Republic"":""CZE"", ""Denmark"":""DNK"",
    ""Djibouti"":""DJI"", ""Dominica"":""DMA"", ""Dominican Republic"":""DOM"", ""Democratic Republic of Congo"" : ""COD"",
    ""Ecuador"":""ECU"", ""Egypt"":""EGY"", ""El Salvador"":""SLV"", ""England"" : ""GBR"",
    ""Equatorial Guinea"":""GNQ"", ""Eritrea"":""ERI"", ""Estonia"":""EST"", ""Ethiopia"":""ETH"",
    ""Falkland Islands"":""FLK"", ""Faroe Islands"":""FRO"",
    ""Fiji"":""FJI"", ""Finland"":""FIN"", ""France"":""FRA"", ""French Guiana"":""GUF"",
    ""French Polynesia"":""PYF"",""Gabon"":""GAB"",
    ""Gambia"":""GMB"", ""Georgia"":""GEO"", ""Germany"":""DEU"", ""Ghana"":""GHA"",
    ""Gibraltar"":""GIB"", ""Greece"":""GRC"", ""Greenland"":""GRL"", ""Grenada"":""GRD"",
    ""Guadeloupe"":""GLP"", ""Guam"":""GUM"", ""Guatemala"":""GTM"", ""Guernsey"":""GGY"",
    ""Guinea"":""GIN"", ""Guinea Bissau"":""GNB"", ""Guyana"":""GUY"", ""Haiti"":""HTI"",""Honduras"":""HND"",
    ""Hong Kong"":""HKG"",  ""Hungary"":""HUN"", ""Iceland"":""ISL"",
    ""India"":""IND"", ""Indonesia"":""IDN"", ""Iran"":""IRN"", ""Iraq"":""IRQ"", ""Ireland"":""IRL"",
    ""Israel"":""ISR"", ""Italy"":""ITA"", ""Jamaica"":""JAM"", ""Japan"":""JPN"",
    ""Jordan"":""JOR"", ""Kazakhstan"":""KAZ"", ""Kenya"":""KEN"",
    ""Kiribati"":""KIR"", ""Kosovo"": ""XKX"", ""Kuwait"":""KWT"", ""Kyrgyzstan"":""KGZ"", ""Laos"":""LAO"",
    ""Latvia"":""LVA"", ""Lebanon"":""LBN"", ""Lesotho"":""LSO"", ""Liberia"":""LBR"",
    ""Libya"":""LBY"", ""Liechtenstein"":""LIE"", ""Lithuania"":""LTU"", ""Luxembourg"":""LUX"",
    ""Macau"":""MAC"", ""Macedonia"":""MKD"", ""Madagascar"":""MDG"", ""Malawi"":""MWI"",
    ""Malaysia"":""MYS"", ""Maldives"":""MDV"", ""Mali"":""MLI"", ""Malta"":""MLT"", ""Marshall Islands"":""MHL"",
    ""Martinique"":""MTQ"", ""Mauritania"":""MRT"", ""Mauritius"":""MUS"",
    ""Mayotte"":""MYT"", ""Mexico"":""MEX"", ""Micronesia"":""FSM"", ""Moldova"":""MDA"",
    ""Monaco"":""MCO"", ""Mongolia"":""MNG"", ""Montenegro"":""MNE"", ""Montserrat"":""MSR"",
    ""Morocco"":""MAR"", ""Mozambique"":""MOZ"", ""Myanmar"":""MMR"", ""Burma"":""MMR"", ""Namibia"":""NAM"",
    ""Nauru"":""NRU"", ""Nepal"":""NPL"", ""Netherlands"":""NLD"", ""Netherlands Antilles"":""ANT"",
    ""New Caledonia"":""NCL"", ""New Zealand"":""NZL"", ""Nicaragua"":""NIC"",
    ""Niger"":""NER"", ""Nigeria"":""NGA"", ""Niue"":""NIU"", ""North Korea"":""PRK"",
    ""Northern Ireland"":""IRL"", ""Northern Mariana Islands"":""MNP"",
    ""Norway"":""NOR"", ""Oman"":""OMN"", ""Pakistan"":""PAK"",
    ""Palau"":""PLW"", ""Palestine"":""PSE"",""Panama"":""PAN"", ""Papua New Guinea"":""PNG"",
    ""Paraguay"":""PRY"", ""Peru"":""PER"", ""Philippines"":""PHL"", ""Pitcairn Islands"":""PCN"",
    ""Poland"":""POL"", ""Portugal"":""PRT"", ""Puerto Rico"":""PRI"",
    ""Qatar"":""QAT"", ""Runion"":""REU"", ""Romania"":""ROU"", ""Russia"":""RUS"",
    ""Rwanda"":""RWA"", ""Saint Barthlemy"":""BLM"", ""Saint Helena"":""SHN"",
    ""Saint Kitts Nevis"":""KNA"", ""Saint Lucia"":""LCA"",
    ""Saint Pierre Miquelon"":""SPM"", ""Saint Vincent Grenadines"":""VCT"",
    ""Samoa"":""WSM"", ""San Marino"":""SMR"", ""So Tom Prncipe"":""STP"", ""Saudi Arabia"":""SAU"",
    ""Senegal"":""SEN"", ""Serbia"":""SRB"",
    ""Seychelles"":""SYC"", ""Sierra Leone"":""SLE"", ""Singapore"":""SGP"", ""Sint Maarten"":""SXM"",
    ""Slovakia"":""SVK"", ""Slovenia"":""SVN"", ""Solomon Islands"":""SLB"",
    ""Somalia"":""SOM"", ""South Africa"":""ZAF"",
    ""South Korea"":""KOR"", ""South Sudan"":""SSD"", ""Spain"":""ESP"", ""Sri Lanka"":""LKA"", ""Sudan"":""SDN"",
    ""Suriname"":""SUR"", ""Svalbard Jan Mayen"":""SJM"",
    ""Swaziland"":""SWZ"", ""Sweden"":""SWE"", ""Switzerland"":""CHE"", ""Syria"":""SYR"",
    ""Taiwan"":""TWN"", ""Tajikistan"":""TJK"", ""Tanzania"":""TZA"", ""Thailand"":""THA"",
    ""Timor Leste"":""TLS"", ""East Timor"":""TLS"",""Togo"":""TGO"", ""Tokelau"":""TKL"", ""Tonga"":""TON"", ""Trinidad Tobago"":""TTO"",
    ""Tunisia"":""TUN"", ""Turkey"":""TUR"",
    ""Turkmenistan"":""TKM"", ""Turks Caicos Islands"":""TCA"", ""Tuvalu"":""TUV"", ""U.S. Minor Outlying Islands"":""UMI"",
    ""Virgin Islands"":""VIR"", ""Uganda"":""UGA"",
    ""Ukraine"":""UKR"", ""United Arab Emirates"":""ARE"", ""United Kingdom"":""GBR"",
    ""United States"":""USA"",    ""Uruguay"":""URY"", ""Uzbekistan"":""UZB"", ""Vanuatu"":""VUT"", ""Vatican"":""VAT"",
    ""Venezuela"":""VEN"",
    ""Vietnam"":""VNM"", ""Wallis Futuna"":""WLF"",
    ""Western Sahara"":""ESH"", ""Yemen"":""YEM"", ""Zambia"":""ZMB"", ""Zimbabwe"":""ZWE"",
    ""UK"":""GBR"",  ""USA"":""USA"", ""America"":""USA"",  ""Palestinian Territories"":""PSE"",
    ""Congo Brazzaville"":""COG"", ""Congo Kinshasa"":""COD"", ""Wales"" : ""GBR"",
    ""Scotland"" : ""GBR"", ""Britain"" : ""GBR"",}

    return cts","Helper function to return dictionary of countries in {""country"" : ""iso""} form."
"def getEntity(self, name):
        """"""
        Get entity corresponding to the specified name (looks for it in all
        types of entities).

        Args:
            name: Name of the entity.

        Raises:
            TypeError: if the specified entity does not exist.

        Returns:
            The AMPL entity with the specified name.
        """"""
        return lock_and_call(
            lambda: Entity(self._impl.getEntity(name)),
            self._lock
        )","Get entity corresponding to the specified name (looks for it in all
        types of entities).

        Args:
            name: Name of the entity.

        Raises:
            TypeError: if the specified entity does not exist.

        Returns:
            The AMPL entity with the specified name."
"def make_chunks(self,length,overlap=0,play=0,sl=0,excl_play=0,pad_data=0):
    """"""
    Divide each ScienceSegment contained in this object into AnalysisChunks.
    @param length: length of chunk in seconds.
    @param overlap: overlap between segments.
    @param play: if true, only generate chunks that overlap with S2 playground
    data.
    @param sl: slide by sl seconds before determining playground data.
    @param excl_play: exclude the first excl_play second from the start and end
    of the chunk when computing if the chunk overlaps with playground.
    """"""
    for seg in self.__sci_segs:
      seg.make_chunks(length,overlap,play,sl,excl_play,pad_data)","Divide each ScienceSegment contained in this object into AnalysisChunks.
    @param length: length of chunk in seconds.
    @param overlap: overlap between segments.
    @param play: if true, only generate chunks that overlap with S2 playground
    data.
    @param sl: slide by sl seconds before determining playground data.
    @param excl_play: exclude the first excl_play second from the start and end
    of the chunk when computing if the chunk overlaps with playground."
"def _expand(subsequence, sequence, max_l_dist):
    """"""Expand a partial match of a Levenstein search.

    An expansion must begin at the beginning of the sequence, which makes
    this much simpler than a full search, and allows for greater optimization.
    """"""
    # If given a long sub-sequence and relatively small max distance,
    # use a more complex algorithm better optimized for such cases.
    if len(subsequence) > max(max_l_dist * 2, 10):
        return _expand_long(subsequence, sequence, max_l_dist)
    else:
        return _expand_short(subsequence, sequence, max_l_dist)","Expand a partial match of a Levenstein search.

    An expansion must begin at the beginning of the sequence, which makes
    this much simpler than a full search, and allows for greater optimization."
"def _parse_tsplib(f):
    """"""Parses a TSPLIB file descriptor and returns a dict containing the problem definition""""""
    line = ''

    specs = {}

    used_specs = ['NAME', 'COMMENT', 'DIMENSION', 'CAPACITY', 'TYPE', 'EDGE_WEIGHT_TYPE']
    used_data = ['DEMAND_SECTION', 'DEPOT_SECTION']

    # Parse specs part
    for line in f:
        line = strip(line)

        # Arbitrary sort, so we test everything out
        s = None
        for s in used_specs:
            if line.startswith(s):
                specs[s] = line.split('{} :'.format(s))[-1].strip() # get value data part
                break

        if s == 'EDGE_WEIGHT_TYPE' and s in specs and specs[s] == 'EXPLICIT':
            used_specs.append('EDGE_WEIGHT_FORMAT')

        # All specs read
        if len(specs) == len(used_specs):
            break

    if len(specs) != len(used_specs):
        missing_specs = set(used_specs).symmetric_difference(set(specs))
        raise ParseException('Error parsing TSPLIB data: specs {} missing'.format(missing_specs))

    print(specs)

    if specs['EDGE_WEIGHT_TYPE'] == 'EUC_2D':
        used_data.append('NODE_COORD_SECTION')
    elif specs['EDGE_WEIGHT_FORMAT'] == 'FULL_MATRIX':
        used_data.append('EDGE_WEIGHT_SECTION')
    else:
        raise ParseException('EDGE_WEIGHT_TYPE or EDGE_WEIGHT_FORMAT not supported')

    _post_process_specs(specs)

    # Parse data part
    for line in f:
        line = strip(line)

        for d in used_data:
            if line.startswith(d):
                if d == 'DEPOT_SECTION':
                    specs[d] = _parse_depot_section(f)
                elif d in ['NODE_COORD_SECTION', 'DEMAND_SECTION']:
                    specs[d] = _parse_nodes_section(f, d, specs['DIMENSION'])
                elif d == 'EDGE_WEIGHT_SECTION':
                    specs[d] = _parse_edge_weight(f, specs['DIMENSION'])

        if len(specs) == len(used_specs) + len(used_data):
            break

    if len(specs) != len(used_specs) + len(used_data):
        missing_specs = set(specs).symmetric_difference(set(used_specs).union(set(used_data)))
        raise ParseException('Error parsing TSPLIB data: specs {} missing'.format(missing_specs))

    _post_process_data(specs)

    return specs",Parses a TSPLIB file descriptor and returns a dict containing the problem definition
"def set_principal_credit_string(self, credit_string=None):
        """"""Sets the principal credit string.

        :param credit_string: the new credit string
        :type credit_string: ``string``
        :raise: ``InvalidArgument`` -- ``credit_string`` is invalid
        :raise: ``NoAccess`` -- ``Metadata.isReadOnly()`` is ``true``
        :raise: ``NullArgument`` -- ``credit_string`` is ``null``

        *compliance: mandatory -- This method must be implemented.*

        """"""
        if credit_string is None:
            raise NullArgument()
        metadata = Metadata(**settings.METADATA['principal_credit_string'])
        if metadata.is_read_only():
            raise NoAccess()
        if self._is_valid_input(credit_string, metadata, array=False):
            self._my_map['principalCreditString']['text'] = credit_string
        else:
            raise InvalidArgument()","Sets the principal credit string.

        :param credit_string: the new credit string
        :type credit_string: ``string``
        :raise: ``InvalidArgument`` -- ``credit_string`` is invalid
        :raise: ``NoAccess`` -- ``Metadata.isReadOnly()`` is ``true``
        :raise: ``NullArgument`` -- ``credit_string`` is ``null``

        *compliance: mandatory -- This method must be implemented.*"
"def _set_fcoe_enode_fabric_map(self, v, load=False):
    """"""
    Setter method for fcoe_enode_fabric_map, mapped from YANG variable /rbridge_id/fcoe_config/fcoe_enode_fabric_map (list)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_fcoe_enode_fabric_map is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_fcoe_enode_fabric_map() directly.

    YANG Description: List of FCoE fabric map parameters.
    """"""
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=YANGListType(""fcoe_enode_fabric_map_name"",fcoe_enode_fabric_map.fcoe_enode_fabric_map, yang_name=""fcoe-enode-fabric-map"", rest_name=""fabric-map"", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='fcoe-enode-fabric-map-name', extensions={u'tailf-common': {u'info': u'Configure an FCoE Fabric-map parameters', u'alt-name': u'fabric-map', u'cli-full-command': None, u'hidden': u'fcoe-enode-fabric-map', u'callpoint': u'fcoe_enode_cp', u'cli-mode-name': u'config-rbridge-fcoe-fabric-map'}}), is_container='list', yang_name=""fcoe-enode-fabric-map"", rest_name=""fabric-map"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Configure an FCoE Fabric-map parameters', u'alt-name': u'fabric-map', u'cli-full-command': None, u'hidden': u'fcoe-enode-fabric-map', u'callpoint': u'fcoe_enode_cp', u'cli-mode-name': u'config-rbridge-fcoe-fabric-map'}}, namespace='urn:brocade.com:mgmt:brocade-fcoe', defining_module='brocade-fcoe', yang_type='list', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': """"""fcoe_enode_fabric_map must be of a type compatible with list"""""",
          'defined-type': ""list"",
          'generated-type': """"""YANGDynClass(base=YANGListType(""fcoe_enode_fabric_map_name"",fcoe_enode_fabric_map.fcoe_enode_fabric_map, yang_name=""fcoe-enode-fabric-map"", rest_name=""fabric-map"", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='fcoe-enode-fabric-map-name', extensions={u'tailf-common': {u'info': u'Configure an FCoE Fabric-map parameters', u'alt-name': u'fabric-map', u'cli-full-command': None, u'hidden': u'fcoe-enode-fabric-map', u'callpoint': u'fcoe_enode_cp', u'cli-mode-name': u'config-rbridge-fcoe-fabric-map'}}), is_container='list', yang_name=""fcoe-enode-fabric-map"", rest_name=""fabric-map"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Configure an FCoE Fabric-map parameters', u'alt-name': u'fabric-map', u'cli-full-command': None, u'hidden': u'fcoe-enode-fabric-map', u'callpoint': u'fcoe_enode_cp', u'cli-mode-name': u'config-rbridge-fcoe-fabric-map'}}, namespace='urn:brocade.com:mgmt:brocade-fcoe', defining_module='brocade-fcoe', yang_type='list', is_config=True)"""""",
        })

    self.__fcoe_enode_fabric_map = t
    if hasattr(self, '_set'):
      self._set()","Setter method for fcoe_enode_fabric_map, mapped from YANG variable /rbridge_id/fcoe_config/fcoe_enode_fabric_map (list)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_fcoe_enode_fabric_map is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_fcoe_enode_fabric_map() directly.

    YANG Description: List of FCoE fabric map parameters."
"def parse_number(d, key, regex, s):
    """"""Find a number using a given regular expression.
    If the number is found, sets it under the key in the given dictionary.
    
    d - The dictionary that will contain the data.
    key - The key into the dictionary.
    regex - A string containing the regular expression.
    s - The string to search.
    """"""
    result = find_number(regex, s)
    if result is not None:
        d[key] = result","Find a number using a given regular expression.
    If the number is found, sets it under the key in the given dictionary.
    
    d - The dictionary that will contain the data.
    key - The key into the dictionary.
    regex - A string containing the regular expression.
    s - The string to search."
"def publish(self, value):
        """"""
        Accepts: list of tuples in the format (ip, port)
        Returns: unicode
        """"""
        if not isinstance(value, list):
            raise ValueError(value)
        slaves = ['%s:%d' % x for x in value]
        return unicode("", "".join(slaves))","Accepts: list of tuples in the format (ip, port)
        Returns: unicode"
"def label_pattern_option_validator(ctx, param, value):
    """"""Checks that a given string has all the required placeholders. The
    possible placeholders are **{type}**, **{scope}** and **{subject}**.

    ``{type}`` are always required, ``{scope}`` is optional and
    ``{subject}`` are required only when ``label_position`` option is set to
    ``header``.

    If the pattern is invalid, raises :class:`click.UsageError`.
    """"""

    missings = []
    label_position = ctx.params[""label_position""]

    if ""{type}"" not in value:
        missings.append(style(""{type}"", fg=""red"", bold=True))

    if label_position == ""header"" and ""{subject}"" not in value:
        missings.append(style(""{subject}"", fg=""red"", bold=True))

    if missings:
        message = ""\n""

        for placeholder in missings:
            message += f""  {x_mark} Missing {placeholder} placeholder.\n""

        ctx.fail(message)

    return value","Checks that a given string has all the required placeholders. The
    possible placeholders are **{type}**, **{scope}** and **{subject}**.

    ``{type}`` are always required, ``{scope}`` is optional and
    ``{subject}`` are required only when ``label_position`` option is set to
    ``header``.

    If the pattern is invalid, raises :class:`click.UsageError`."
"def licenses_configured(name, licenses=None):
    '''
    Configures licenses on the cluster entity

    Checks if each license exists on the server:
        - if it doesn't, it creates it
    Check if license is assigned to the cluster:
        - if it's not assigned to the cluster:
            - assign it to the cluster if there is space
            - error if there's no space
        - if it's assigned to the cluster nothing needs to be done
    '''
    ret = {'name': name,
           'changes': {},
           'result': None,
           'comment': 'Default'}
    if not licenses:
        raise salt.exceptions.ArgumentValueError('No licenses provided')
    cluster_name, datacenter_name = \
            __salt__['esxcluster.get_details']()['cluster'], \
            __salt__['esxcluster.get_details']()['datacenter']
    display_name = '{0}/{1}'.format(datacenter_name, cluster_name)
    log.info('Running licenses configured for \'%s\'', display_name)
    log.trace('licenses = %s', licenses)
    entity = {'type': 'cluster',
              'datacenter': datacenter_name,
              'cluster': cluster_name}
    log.trace('entity = %s', entity)

    comments = []
    changes = {}
    old_licenses = []
    new_licenses = []
    has_errors = False
    needs_changes = False
    try:
        # Validate licenses
        log.trace('Validating licenses')
        schema = LicenseSchema.serialize()
        try:
            jsonschema.validate({'licenses': licenses}, schema)
        except jsonschema.exceptions.ValidationError as exc:
            raise salt.exceptions.InvalidLicenseError(exc)

        si = __salt__['vsphere.get_service_instance_via_proxy']()
        # Retrieve licenses
        existing_licenses = __salt__['vsphere.list_licenses'](
            service_instance=si)
        remaining_licenses = existing_licenses[:]
        # Cycle through licenses
        for license_name, license in licenses.items():
            # Check if license already exists
            filtered_licenses = [l for l in existing_licenses
                                 if l['key'] == license]
            # TODO Update license description - not of interest right now
            if not filtered_licenses:
                # License doesn't exist - add and assign to cluster
                needs_changes = True
                if __opts__['test']:
                    # If it doesn't exist it clearly needs to be assigned as
                    # well so we can stop the check here
                    comments.append('State {0} will add license \'{1}\', '
                                    'and assign it to cluster \'{2}\'.'
                                    ''.format(name, license_name, display_name))
                    log.info(comments[-1])
                    continue
                else:
                    try:
                        existing_license = __salt__['vsphere.add_license'](
                            key=license, description=license_name,
                            service_instance=si)
                    except salt.exceptions.VMwareApiError as ex:
                        comments.append(ex.err_msg)
                        log.error(comments[-1])
                        has_errors = True
                        continue
                    comments.append('Added license \'{0}\'.'
                                    ''.format(license_name))
                    log.info(comments[-1])
            else:
                # License exists let's check if it's assigned to the cluster
                comments.append('License \'{0}\' already exists. '
                                'Nothing to be done.'.format(license_name))
                log.info(comments[-1])
                existing_license = filtered_licenses[0]

            log.trace('Checking licensed entities...')
            assigned_licenses = __salt__['vsphere.list_assigned_licenses'](
                entity=entity,
                entity_display_name=display_name,
                service_instance=si)

            # Checking if any of the licenses already assigned have the same
            # name as the new license; the already assigned license would be
            # replaced by the new license
            #
            # Licenses with different names but matching features would be
            # replaced as well, but searching for those would be very complex
            #
            # the name check if good enough for now
            already_assigned_license = assigned_licenses[0] if \
                    assigned_licenses else None

            if already_assigned_license and \
               already_assigned_license['key'] == license:

                # License is already assigned to entity
                comments.append('License \'{0}\' already assigned to '
                                'cluster \'{1}\'. Nothing to be done.'
                                ''.format(license_name, display_name))
                log.info(comments[-1])
                continue

            needs_changes = True
            # License needs to be assigned to entity

            if existing_license['capacity'] <= existing_license['used']:
                # License is already fully used
                comments.append('Cannot assign license \'{0}\' to cluster '
                                '\'{1}\'. No free capacity available.'
                                ''.format(license_name, display_name))
                log.error(comments[-1])
                has_errors = True
                continue

            # Assign license
            if __opts__['test']:
                comments.append('State {0} will assign license \'{1}\' '
                                'to cluster \'{2}\'.'.format(
                                    name, license_name, display_name))
                log.info(comments[-1])
            else:
                try:
                    __salt__['vsphere.assign_license'](
                        license_key=license,
                        license_name=license_name,
                        entity=entity,
                        entity_display_name=display_name,
                        service_instance=si)
                except salt.exceptions.VMwareApiError as ex:
                    comments.append(ex.err_msg)
                    log.error(comments[-1])
                    has_errors = True
                    continue
                comments.append('Assigned license \'{0}\' to cluster \'{1}\'.'
                                ''.format(license_name, display_name))
                log.info(comments[-1])
                # Note: Because the already_assigned_license was retrieved
                # from the assignment license manager it doesn't have a used
                # value - that's a limitation from VMware. The license would
                # need to be retrieved again from the license manager to get
                # the value

                # Hide license keys
                assigned_license = __salt__['vsphere.list_assigned_licenses'](
                    entity=entity,
                    entity_display_name=display_name,
                    service_instance=si)[0]
                assigned_license['key'] = '<hidden>'
                if already_assigned_license:
                    already_assigned_license['key'] = '<hidden>'
                if already_assigned_license and \
                   already_assigned_license['capacity'] == sys.maxsize:

                    already_assigned_license['capacity'] = 'Unlimited'

                changes[license_name] = {'new': assigned_license,
                                         'old': already_assigned_license}
            continue
        __salt__['vsphere.disconnect'](si)

        ret.update({'result': True if (not needs_changes) else None if
                        __opts__['test'] else False if has_errors else True,
                    'comment': '\n'.join(comments),
                    'changes': changes if not __opts__['test'] else {}})

        return ret
    except salt.exceptions.CommandExecutionError as exc:
        log.exception('Encountered error')
        if si:
            __salt__['vsphere.disconnect'](si)
        ret.update({
            'result': False,
            'comment': exc.strerror})
        return ret","Configures licenses on the cluster entity

    Checks if each license exists on the server:
        - if it doesn't, it creates it
    Check if license is assigned to the cluster:
        - if it's not assigned to the cluster:
            - assign it to the cluster if there is space
            - error if there's no space
        - if it's assigned to the cluster nothing needs to be done"
"def _to_patches(self, X):
        """"""
        Reshapes input to patches of the size of classifier's receptive field.

        For example:

        input X shape: [n_samples, n_pixels_y, n_pixels_x, n_bands]

        output: [n_samples * n_pixels_y/receptive_field_y * n_pixels_x/receptive_field_x,
                 receptive_field_y, receptive_field_x, n_bands]
        """"""

        window = self.receptive_field
        asteps = self.receptive_field

        if len(X.shape) == 4:
            window += (0,)
            asteps += (1,)

        image_view = rolling_window(X, window, asteps)

        new_shape = image_view.shape

        # this makes a copy of the array? can we do without reshaping?
        image_view = image_view.reshape((new_shape[0] * new_shape[1] * new_shape[2],) + new_shape[3:])

        if len(X.shape) == 4:
            image_view = np.moveaxis(image_view, 1, -1)

        return image_view, new_shape","Reshapes input to patches of the size of classifier's receptive field.

        For example:

        input X shape: [n_samples, n_pixels_y, n_pixels_x, n_bands]

        output: [n_samples * n_pixels_y/receptive_field_y * n_pixels_x/receptive_field_x,
                 receptive_field_y, receptive_field_x, n_bands]"
"def select_projects(self, *args):
        """"""Copy the query and add filtering by monitored projects.

        This is only useful if the target project represents a Stackdriver
        account containing the specified monitored projects.

        Examples::

            query = query.select_projects('project-1')
            query = query.select_projects('project-1', 'project-2')

        :type args: tuple
        :param args: Project IDs limiting the resources to be included
            in the query.

        :rtype: :class:`Query`
        :returns: The new query object.
        """"""
        new_query = copy.deepcopy(self)
        new_query._filter.projects = args
        return new_query","Copy the query and add filtering by monitored projects.

        This is only useful if the target project represents a Stackdriver
        account containing the specified monitored projects.

        Examples::

            query = query.select_projects('project-1')
            query = query.select_projects('project-1', 'project-2')

        :type args: tuple
        :param args: Project IDs limiting the resources to be included
            in the query.

        :rtype: :class:`Query`
        :returns: The new query object."
"def invoke(client, method, **kwargs):
    """"""Invoke a method on the underlying soap service.""""""
    try:
        # Proxy the method to the suds service
        result = getattr(client.service, method)(**kwargs)
    except AttributeError:
        logger.critical(""Unknown method: %s"", method)
        raise
    except URLError as e:
        logger.debug(pprint(e))
        logger.debug(""A URL related error occurred while invoking the '%s' ""
              ""method on the VIM server, this can be caused by ""
              ""name resolution or connection problems."", method)
        logger.debug(""The underlying error is: %s"", e.reason[1])
        raise
    except suds.client.TransportError as e:
        logger.debug(pprint(e))
        logger.debug(""TransportError: %s"", e)
    except suds.WebFault as e:
        # Get the type of fault
        logger.critical(""SUDS Fault: %s"" % e.fault.faultstring)
        if len(e.fault.faultstring) > 0:
            raise

        detail = e.document.childAtPath(""/Envelope/Body/Fault/detail"")
        fault_type = detail.getChildren()[0].name
        fault = create(fault_type)
        if isinstance(e.fault.detail[0], list):
            for attr in e.fault.detail[0]:
                setattr(fault, attr[0], attr[1])
        else:
            fault[""text""] = e.fault.detail[0]

        raise VimFault(fault)

    return result",Invoke a method on the underlying soap service.
"def next(self):
        """"""Request next data container.

        This function call is blocking.

        Returns
        -------
        data : dict
            The data for this train, keyed by source name.
        meta : dict
            The metadata for this train, keyed by source name.

            This dictionary is populated for protocol version 1.0 and 2.2.
            For other protocol versions, metadata information is available in
            `data` dict.

        Raises
        ------
        TimeoutError
            If timeout is reached before receiving data.
        """"""
        if self._pattern == zmq.REQ and not self._recv_ready:
            self._socket.send(b'next')
            self._recv_ready = True
        try:
            msg = self._socket.recv_multipart(copy=False)
        except zmq.error.Again:
            raise TimeoutError(
                'No data received from {} in the last {} ms'.format(
                    self._socket.getsockopt_string(zmq.LAST_ENDPOINT),
                    self._socket.getsockopt(zmq.RCVTIMEO)))
        self._recv_ready = False
        return self._deserialize(msg)","Request next data container.

        This function call is blocking.

        Returns
        -------
        data : dict
            The data for this train, keyed by source name.
        meta : dict
            The metadata for this train, keyed by source name.

            This dictionary is populated for protocol version 1.0 and 2.2.
            For other protocol versions, metadata information is available in
            `data` dict.

        Raises
        ------
        TimeoutError
            If timeout is reached before receiving data."
"def softmax_to_unary(sm, GT_PROB=1):
    """"""Deprecated, use `unary_from_softmax` instead.""""""
    warning(""pydensecrf.softmax_to_unary is deprecated, use unary_from_softmax instead."")
    scale = None if GT_PROB == 1 else GT_PROB
    return unary_from_softmax(sm, scale, clip=None)","Deprecated, use `unary_from_softmax` instead."
"def  dBinaryRochedz(r, D, q, F):
    """"""
    Computes a derivative of the potential with respect to z.

    @param r:      relative radius vector (3 components)
    @param D:      instantaneous separation
    @param q:      mass ratio
    @param F:      synchronicity parameter
    """"""
    return -r[2]*(r[0]*r[0]+r[1]*r[1]+r[2]*r[2])**-1.5 -q*r[2]*((r[0]-D)*(r[0]-D)+r[1]*r[1]+r[2]*r[2])**-1.5","Computes a derivative of the potential with respect to z.

    @param r:      relative radius vector (3 components)
    @param D:      instantaneous separation
    @param q:      mass ratio
    @param F:      synchronicity parameter"
"def file_list(self, page_size: int, fields: str, order_by: str, query: str) -> dict:
        """"""
        Get file list from Google Drive
        :param page_size:
        :param fields:
        :param order_by:
        :param query:
        :return:
        """"""
        service = self.__get_service()
        results = service.files().list(
            pageSize=page_size,
            fields=fields,
            orderBy=order_by,
            q=query
        ).execute()
        return results.get('files', [])","Get file list from Google Drive
        :param page_size:
        :param fields:
        :param order_by:
        :param query:
        :return:"
"def title(self, value=None):
        """"""Get or set the document's title from/in the metadata

           No arguments: Get the document's title from metadata
           Argument: Set the document's title in metadata
        """"""
        if not (value is None):
            if (self.metadatatype == ""native""):
                self.metadata['title'] = value
            else:
                self._title = value
        if (self.metadatatype == ""native""):
            if 'title' in self.metadata:
                return self.metadata['title']
            else:
                return None
        else:
            return self._title","Get or set the document's title from/in the metadata

           No arguments: Get the document's title from metadata
           Argument: Set the document's title in metadata"
"def _argsort_and_resolve_ties(time, random_state):
        """"""Like numpy.argsort, but resolves ties uniformly at random""""""
        n_samples = len(time)
        order = numpy.argsort(time, kind=""mergesort"")

        i = 0
        while i < n_samples - 1:
            inext = i + 1
            while inext < n_samples and time[order[i]] == time[order[inext]]:
                inext += 1

            if i + 1 != inext:
                # resolve ties randomly
                random_state.shuffle(order[i:inext])
            i = inext
        return order","Like numpy.argsort, but resolves ties uniformly at random"
"def _openResources(self):
        """""" Opens the root Dataset.
        """"""
        logger.info(""Opening: {}"".format(self._fileName))
        self._h5Group = h5py.File(self._fileName, 'r')",Opens the root Dataset.
"def get_all_conversion_chains(self, from_type: Type[Any] = JOKER, to_type: Type[Any] = JOKER)\
            -> Tuple[List[Converter], List[Converter], List[Converter]]:
        """"""
        Utility method to find all converters or conversion chains matching the provided query.

        :param from_type: a required type of input object, or JOKER for 'wildcard'(*) .
        WARNING: ""from_type=AnyObject/object/Any"" means
        ""all converters able to source from anything"", which is different from ""from_type=JOKER"" which means ""all
        converters whatever their source type"".
        :param to_type: a required type of output object, or JOKER for 'wildcard'(*) .
        WARNING: ""to_type=AnyObject/object/Any"" means ""all
        converters able to produce any type of object"", which is different from ""to_type=JOKER"" which means ""all
        converters whatever type they are able to produce"".
        :return: a tuple of lists of matching converters, by type of *dest_type* match : generic, approximate, exact
        """"""
        pass","Utility method to find all converters or conversion chains matching the provided query.

        :param from_type: a required type of input object, or JOKER for 'wildcard'(*) .
        WARNING: ""from_type=AnyObject/object/Any"" means
        ""all converters able to source from anything"", which is different from ""from_type=JOKER"" which means ""all
        converters whatever their source type"".
        :param to_type: a required type of output object, or JOKER for 'wildcard'(*) .
        WARNING: ""to_type=AnyObject/object/Any"" means ""all
        converters able to produce any type of object"", which is different from ""to_type=JOKER"" which means ""all
        converters whatever type they are able to produce"".
        :return: a tuple of lists of matching converters, by type of *dest_type* match : generic, approximate, exact"
"def filter_genes_dispersion(data,
                            flavor='seurat',
                            min_disp=None, max_disp=None,
                            min_mean=None, max_mean=None,
                            n_bins=20,
                            n_top_genes=None,
                            log=True,
                            subset=True,
                            copy=False):
    """"""Extract highly variable genes [Satija15]_ [Zheng17]_.

    .. warning::
        .. deprecated:: 1.3.6
            Use :func:`~scanpy.api.pp.highly_variable_genes`
            instead. The new function is equivalent to the present
            function, except that

            * the new function always expects logarithmized data
            * `subset=False` in the new function, it suffices to
              merely annotate the genes, tools like `pp.pca` will
              detect the annotation
            * you can now call: `sc.pl.highly_variable_genes(adata)`
            * `copy` is replaced by `inplace`

    If trying out parameters, pass the data matrix instead of AnnData.

    Depending on `flavor`, this reproduces the R-implementations of Seurat
    [Satija15]_ and Cell Ranger [Zheng17]_.

    The normalized dispersion is obtained by scaling with the mean and standard
    deviation of the dispersions for genes falling into a given bin for mean
    expression of genes. This means that for each bin of mean expression, highly
    variable genes are selected.

    Use `flavor='cell_ranger'` with care and in the same way as in
    :func:`~scanpy.api.pp.recipe_zheng17`.

    Parameters
    ----------
    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`
        The (annotated) data matrix of shape `n_obs`  `n_vars`. Rows correspond
        to cells and columns to genes.
    flavor : {'seurat', 'cell_ranger'}, optional (default: 'seurat')
        Choose the flavor for computing normalized dispersion. If choosing
        'seurat', this expects non-logarithmized data - the logarithm of mean
        and dispersion is taken internally when `log` is at its default value
        `True`. For 'cell_ranger', this is usually called for logarithmized data
        - in this case you should set `log` to `False`. In their default
        workflows, Seurat passes the cutoffs whereas Cell Ranger passes
        `n_top_genes`.
    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=`None` : `float`, optional
        If `n_top_genes` unequals `None`, these cutoffs for the means and the
        normalized dispersions are ignored.
    n_bins : `int` (default: 20)
        Number of bins for binning the mean gene expression. Normalization is
        done with respect to each bin. If just a single gene falls into a bin,
        the normalized dispersion is artificially set to 1. You'll be informed
        about this if you set `settings.verbosity = 4`.
    n_top_genes : `int` or `None` (default: `None`)
        Number of highly-variable genes to keep.
    log : `bool`, optional (default: `True`)
        Use the logarithm of the mean to variance ratio.
    subset : `bool`, optional (default: `True`)
        Keep highly-variable genes only (if True) else write a bool array for h
        ighly-variable genes while keeping all genes
    copy : `bool`, optional (default: `False`)
        If an :class:`~anndata.AnnData` is passed, determines whether a copy
        is returned.

    Returns
    -------
    If an AnnData `adata` is passed, returns or updates `adata` depending on
    `copy`. It filters the `adata` and adds the annotations

    **means** : adata.var
        Means per gene. Logarithmized when `log` is `True`.
    **dispersions** : adata.var
        Dispersions per gene. Logarithmized when `log` is `True`.
    **dispersions_norm** : adata.var
        Normalized dispersions per gene. Logarithmized when `log` is `True`.

    If a data matrix `X` is passed, the annotation is returned as `np.recarray`
    with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.
    """"""
    if n_top_genes is not None and not all([
            min_disp is None, max_disp is None, min_mean is None, max_mean is None]):
        logg.info('If you pass `n_top_genes`, all cutoffs are ignored.')
    if min_disp is None: min_disp = 0.5
    if min_mean is None: min_mean = 0.0125
    if max_mean is None: max_mean = 3
    if isinstance(data, AnnData):
        adata = data.copy() if copy else data
        result = filter_genes_dispersion(adata.X, log=log,
                                         min_disp=min_disp, max_disp=max_disp,
                                         min_mean=min_mean, max_mean=max_mean,
                                         n_top_genes=n_top_genes,
                                         flavor=flavor)
        adata.var['means'] = result['means']
        adata.var['dispersions'] = result['dispersions']
        adata.var['dispersions_norm'] = result['dispersions_norm']
        if subset:
            adata._inplace_subset_var(result['gene_subset'])
        else:
            adata.var['highly_variable'] = result['gene_subset']
        return adata if copy else None
    logg.msg('extracting highly variable genes',
              r=True, v=4)
    X = data  # no copy necessary, X remains unchanged in the following
    mean, var = materialize_as_ndarray(_get_mean_var(X))
    # now actually compute the dispersion
    mean[mean == 0] = 1e-12  # set entries equal to zero to small value
    dispersion = var / mean
    if log:  # logarithmized mean as in Seurat
        dispersion[dispersion == 0] = np.nan
        dispersion = np.log(dispersion)
        mean = np.log1p(mean)
    # all of the following quantities are ""per-gene"" here
    import pandas as pd
    df = pd.DataFrame()
    df['mean'] = mean
    df['dispersion'] = dispersion
    if flavor == 'seurat':
        df['mean_bin'] = pd.cut(df['mean'], bins=n_bins)
        disp_grouped = df.groupby('mean_bin')['dispersion']
        disp_mean_bin = disp_grouped.mean()
        disp_std_bin = disp_grouped.std(ddof=1)
        # retrieve those genes that have nan std, these are the ones where
        # only a single gene fell in the bin and implicitly set them to have
        # a normalized disperion of 1
        one_gene_per_bin = disp_std_bin.isnull()
        gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist()
        if len(gen_indices) > 0:
            logg.msg(
                'Gene indices {} fell into a single bin: their '
                'normalized dispersion was set to 1.\n    '
                'Decreasing `n_bins` will likely avoid this effect.'
                .format(gen_indices), v=4)
        # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,
        # but theres still a dtype error without .value.
        disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin.values].values
        disp_mean_bin[one_gene_per_bin] = 0
        # actually do the normalization
        df['dispersion_norm'] = (df['dispersion'].values  # use values here as index differs
                                 - disp_mean_bin[df['mean_bin'].values].values) \
                                 / disp_std_bin[df['mean_bin'].values].values
    elif flavor == 'cell_ranger':
        from statsmodels import robust
        df['mean_bin'] = pd.cut(df['mean'], np.r_[-np.inf,
            np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])
        disp_grouped = df.groupby('mean_bin')['dispersion']
        disp_median_bin = disp_grouped.median()
        # the next line raises the warning: ""Mean of empty slice""
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            disp_mad_bin = disp_grouped.apply(robust.mad)
        df['dispersion_norm'] = np.abs((df['dispersion'].values
                                 - disp_median_bin[df['mean_bin'].values].values)) \
                                / disp_mad_bin[df['mean_bin'].values].values
    else:
        raise ValueError('`flavor` needs to be ""seurat"" or ""cell_ranger""')
    dispersion_norm = df['dispersion_norm'].values.astype('float32')
    if n_top_genes is not None:
        dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]
        dispersion_norm[::-1].sort()  # interestingly, np.argpartition is slightly slower
        disp_cut_off = dispersion_norm[n_top_genes-1]
        gene_subset = df['dispersion_norm'].values >= disp_cut_off
        logg.msg('the {} top genes correspond to a normalized dispersion cutoff of'
                 .format(n_top_genes, disp_cut_off), v=5)
    else:
        max_disp = np.inf if max_disp is None else max_disp
        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat
        gene_subset = np.logical_and.reduce((mean > min_mean, mean < max_mean,
                                             dispersion_norm > min_disp,
                                             dispersion_norm < max_disp))
    logg.msg('    finished', time=True, v=4)
    return np.rec.fromarrays((gene_subset,
                              df['mean'].values,
                              df['dispersion'].values,
                              df['dispersion_norm'].values.astype('float32', copy=False)),
                              dtype=[('gene_subset', bool),
                                     ('means', 'float32'),
                                     ('dispersions', 'float32'),
                                     ('dispersions_norm', 'float32')])","Extract highly variable genes [Satija15]_ [Zheng17]_.

    .. warning::
        .. deprecated:: 1.3.6
            Use :func:`~scanpy.api.pp.highly_variable_genes`
            instead. The new function is equivalent to the present
            function, except that

            * the new function always expects logarithmized data
            * `subset=False` in the new function, it suffices to
              merely annotate the genes, tools like `pp.pca` will
              detect the annotation
            * you can now call: `sc.pl.highly_variable_genes(adata)`
            * `copy` is replaced by `inplace`

    If trying out parameters, pass the data matrix instead of AnnData.

    Depending on `flavor`, this reproduces the R-implementations of Seurat
    [Satija15]_ and Cell Ranger [Zheng17]_.

    The normalized dispersion is obtained by scaling with the mean and standard
    deviation of the dispersions for genes falling into a given bin for mean
    expression of genes. This means that for each bin of mean expression, highly
    variable genes are selected.

    Use `flavor='cell_ranger'` with care and in the same way as in
    :func:`~scanpy.api.pp.recipe_zheng17`.

    Parameters
    ----------
    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`
        The (annotated) data matrix of shape `n_obs`  `n_vars`. Rows correspond
        to cells and columns to genes.
    flavor : {'seurat', 'cell_ranger'}, optional (default: 'seurat')
        Choose the flavor for computing normalized dispersion. If choosing
        'seurat', this expects non-logarithmized data - the logarithm of mean
        and dispersion is taken internally when `log` is at its default value
        `True`. For 'cell_ranger', this is usually called for logarithmized data
        - in this case you should set `log` to `False`. In their default
        workflows, Seurat passes the cutoffs whereas Cell Ranger passes
        `n_top_genes`.
    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=`None` : `float`, optional
        If `n_top_genes` unequals `None`, these cutoffs for the means and the
        normalized dispersions are ignored.
    n_bins : `int` (default: 20)
        Number of bins for binning the mean gene expression. Normalization is
        done with respect to each bin. If just a single gene falls into a bin,
        the normalized dispersion is artificially set to 1. You'll be informed
        about this if you set `settings.verbosity = 4`.
    n_top_genes : `int` or `None` (default: `None`)
        Number of highly-variable genes to keep.
    log : `bool`, optional (default: `True`)
        Use the logarithm of the mean to variance ratio.
    subset : `bool`, optional (default: `True`)
        Keep highly-variable genes only (if True) else write a bool array for h
        ighly-variable genes while keeping all genes
    copy : `bool`, optional (default: `False`)
        If an :class:`~anndata.AnnData` is passed, determines whether a copy
        is returned.

    Returns
    -------
    If an AnnData `adata` is passed, returns or updates `adata` depending on
    `copy`. It filters the `adata` and adds the annotations

    **means** : adata.var
        Means per gene. Logarithmized when `log` is `True`.
    **dispersions** : adata.var
        Dispersions per gene. Logarithmized when `log` is `True`.
    **dispersions_norm** : adata.var
        Normalized dispersions per gene. Logarithmized when `log` is `True`.

    If a data matrix `X` is passed, the annotation is returned as `np.recarray`
    with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`."
"def rest_put(self, url, params=None, headers=None, auth=None, verify=True, cert=None):
        """"""
        Perform a PUT request to url with optional authentication
        """"""
        res = requests.put(url, params=params, headers=headers, auth=auth, verify=verify,
                           cert=cert)
        return res.text, res.status_code",Perform a PUT request to url with optional authentication
"def get_best_experiment_sets(nets,expvars,num):
  '''
  given the network and the experimental variables, and the bound on the size of an experiment set
  returns the experiments as a``TermSet`` object [instance].
  '''

  netsf    = nets.to_file()
  expvarsf = expvars.to_file()

  best           = -1
  best_solutions = []
  best_found     = False

  i = 0
  while i < num and not best_found :
    i += 1

    num_exp   = String2TermSet('pexperiment('+str(i)+')')
    num_expf  = num_exp.to_file()
    prg       = [ netsf, expvarsf, num_expf, find_best_exp_sets_prg,
                  elem_path_prg ]
    coptions  = '--project --opt-mode=optN --opt-strategy=0 --opt-heuristic'
    solver    = GringoClasp(clasp_options=coptions)
    solutions = solver.run(prg,collapseTerms=True,collapseAtoms=False)

    #print(solutions[0].score[0],solutions[0].score[1],solutions[0].score[2],solutions[0].score[3])

    os.unlink(num_expf)
    if solutions == []: best_found = True
    else:
      opt=(solutions[0].score[0]+solutions[0].score[1]+solutions[0].score[2])
      if best == opt:
        best_found = True
      else:
        best           = opt
        best_solutions = solutions


  os.unlink(netsf)
  os.unlink(expvarsf)

  return best_solutions","given the network and the experimental variables, and the bound on the size of an experiment set
  returns the experiments as a``TermSet`` object [instance]."
"def from_file(fn, **options):
        """"""
        Creates a new TableFu instance from a file or path
        """"""
        if hasattr(fn, 'read'):
            return TableFu(fn, **options)
        with open(fn) as f:
            return TableFu(f, **options)",Creates a new TableFu instance from a file or path
"def unload(self):
        """"""
        Unloads OpenALPR from memory.

        :return: None
        """"""

        if self.loaded:
            self.loaded = False
            self._openalprpy_lib.dispose(self.alpr_pointer)","Unloads OpenALPR from memory.

        :return: None"
"def _parsed_string_to_bounds(date_type, resolution, parsed):
    """"""Generalization of
    pandas.tseries.index.DatetimeIndex._parsed_string_to_bounds
    for use with non-standard calendars and cftime.datetime
    objects.
    """"""
    if resolution == 'year':
        return (date_type(parsed.year, 1, 1),
                date_type(parsed.year + 1, 1, 1) - timedelta(microseconds=1))
    elif resolution == 'month':
        if parsed.month == 12:
            end = date_type(parsed.year + 1, 1, 1) - timedelta(microseconds=1)
        else:
            end = (date_type(parsed.year, parsed.month + 1, 1) -
                   timedelta(microseconds=1))
        return date_type(parsed.year, parsed.month, 1), end
    elif resolution == 'day':
        start = date_type(parsed.year, parsed.month, parsed.day)
        return start, start + timedelta(days=1, microseconds=-1)
    elif resolution == 'hour':
        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour)
        return start, start + timedelta(hours=1, microseconds=-1)
    elif resolution == 'minute':
        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour,
                          parsed.minute)
        return start, start + timedelta(minutes=1, microseconds=-1)
    elif resolution == 'second':
        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour,
                          parsed.minute, parsed.second)
        return start, start + timedelta(seconds=1, microseconds=-1)
    else:
        raise KeyError","Generalization of
    pandas.tseries.index.DatetimeIndex._parsed_string_to_bounds
    for use with non-standard calendars and cftime.datetime
    objects."
"def simplex_connect(self, solution_g):
        '''
        API:
            simplex_connect(self, solution_g)
        Description:
            At this point we assume that the solution does not have a cycle.
            We check if all the nodes are connected, if not we add an arc to
            solution_g that does not create a cycle and return True. Otherwise
            we do nothing and return False.
        Pre:
            (1) We assume there is no cycle in the solution.
        Input:
            solution_g: current spanning tree solution instance.
        Post:
            (1) solution_g is updated. An arc that does not create a cycle is
            added.
            (2) 'component' attribute of nodes are changed.
        Return:
            Returns True if an arc is added, returns False otherwise.
        '''
        nl = solution_g.get_node_list()
        current = nl[0]
        pred = solution_g.simplex_search(current, current)
        separated = list(pred.keys())
        for n in nl:
            if solution_g.get_node(n).get_attr('component') != current:
                # find an arc from n to seperated
                for m in separated:
                    if (n,m) in self.edge_attr:
                        solution_g.add_edge(n,m)
                        return True
                    elif (m,n) in self.edge_attr:
                        solution_g.add_edge(m,n)
                        return True
        return False","API:
            simplex_connect(self, solution_g)
        Description:
            At this point we assume that the solution does not have a cycle.
            We check if all the nodes are connected, if not we add an arc to
            solution_g that does not create a cycle and return True. Otherwise
            we do nothing and return False.
        Pre:
            (1) We assume there is no cycle in the solution.
        Input:
            solution_g: current spanning tree solution instance.
        Post:
            (1) solution_g is updated. An arc that does not create a cycle is
            added.
            (2) 'component' attribute of nodes are changed.
        Return:
            Returns True if an arc is added, returns False otherwise."
"def decodeBinaryData(binaryData, arrayLength, bitEncoding, compression):
    """"""Function to decode a mzML byte array into a numpy array. This is the
    inverse function of :func:`encodeBinaryData`. Concept inherited from
    :func:`pymzml.spec.Spectrum._decode` of the python library `pymzML
    <https://pymzml.github.io/>`_.

    :param binaryData: #TODO: docstring
    :param arrayLength: #TODO: docstring
    :param binEncoding: #TODO: docstring
    :param compression: #TODO: docstring

    :returns: #TODO: docstring
    """"""
    #TODO: should raise an error if a wrong compression is specified
    bitEncodedData = binaryData.encode(""utf-8"")
    bitDecodedData = B64DEC(bitEncodedData)
    floattype, numpyType = interpretBitEncoding(bitEncoding)

    if compression == 'zlib':
        decompressedData = zlib.decompress(bitDecodedData)
    else:
        decompressedData = bitDecodedData

    fmt = '{endian}{arraylength}{floattype}'.format(endian='<',
                                                    arraylength=arrayLength,
                                                    floattype=floattype
                                                    )
    dataArray = numpy.array(UNPACK(fmt, decompressedData), dtype=numpyType)
    return dataArray","Function to decode a mzML byte array into a numpy array. This is the
    inverse function of :func:`encodeBinaryData`. Concept inherited from
    :func:`pymzml.spec.Spectrum._decode` of the python library `pymzML
    <https://pymzml.github.io/>`_.

    :param binaryData: #TODO: docstring
    :param arrayLength: #TODO: docstring
    :param binEncoding: #TODO: docstring
    :param compression: #TODO: docstring

    :returns: #TODO: docstring"
"def set(self, key, val, bucket):
        """""" Set a cached item by key

        WARN: Regardless if the item is already in the cache,
              it will be udpated with the new value.
        """"""

        if bucket not in self._cache:
            self._cache[bucket] = {}

        self._cache[bucket][key] = val","Set a cached item by key

        WARN: Regardless if the item is already in the cache,
              it will be udpated with the new value."
"def list_my_requests_view(request):
    '''
    Show user his/her requests in list form.
    '''
    userProfile = UserProfile.objects.get(user=request.user)
    requests = Request.objects.filter(owner=userProfile)
    return render_to_response('list_requests.html', {
        'page_name': ""Your Requests"",
        'requests': requests,
        }, context_instance=RequestContext(request))",Show user his/her requests in list form.
"def angle(self, vector):
        """"""Return the angle between two vectors in degrees.""""""
        return math.degrees(
            math.acos(
                self.dot(vector) /
                (self.magnitude() * vector.magnitude())
            )
        )",Return the angle between two vectors in degrees.
"def hacking_has_only_comments(physical_line, filename, lines, line_number):
    """"""Check for empty files with only comments

    H104 empty file with only comments
    """"""
    if line_number == 1 and all(map(EMPTY_LINE_RE.match, lines)):
        return (0, ""H104: File contains nothing but comments"")","Check for empty files with only comments

    H104 empty file with only comments"
"def generate_message_definitions(basename, xml):
    '''generate files for one XML file'''

    directory = os.path.join(basename, xml.basename)

    print(""Generating Objective-C implementation in directory %s"" % directory)
    mavparse.mkdir_p(directory)

    xml.basename_camel_case = camel_case_from_underscores(xml.basename)

    # Add some extra field attributes for convenience
    for m in xml.message:
        m.basename = xml.basename
        m.parse_time = xml.parse_time
        m.name_camel_case = camel_case_from_underscores(m.name_lower)
        for f in m.fields:
            f.name_lower_camel_case = lower_camel_case_from_underscores(f.name);
            f.get_message = ""[self %s]"" % f.name_lower_camel_case
            f.return_method_implementation = ''
            f.array_prefix = ''
            f.array_return_arg = ''
            f.get_arg = ''
            f.get_arg_objc = ''
            if f.enum:
                f.return_type = f.enum
                f.arg_type = f.enum
            else:
                f.return_type = f.type
                f.arg_type = f.type
            if f.print_format is None:
                if f.array_length != 0:
                    f.print_format = ""%@""
                elif f.type.startswith('uint64_t'):
                    f.print_format = ""%lld""
                elif f.type.startswith('uint') or f.type.startswith('int'):
                    f.print_format = ""%d""
                elif f.type.startswith('float'):
                    f.print_format = ""%f""
                elif f.type.startswith('char'):
                    f.print_format = ""%c""
                else:
                    print(""print_format unsupported for type %s"" % f.type)
            if f.array_length != 0:
                f.get_message = '@""[array of %s[%d]]""' % (f.type, f.array_length)
                f.array_prefix = ' *'
                f.array_return_arg = '%s, %u, ' % (f.name, f.array_length)
                f.return_type = 'uint16_t'
                f.get_arg = ', %s' % (f.name)
                f.get_arg_objc = ':(%s *)%s' % (f.type, f.name)
                if f.type == 'char':
                    # Special handling for strings (assumes all char arrays are strings)
                    f.return_type = 'NSString *'
                    f.get_arg_objc = ''
                    f.get_message = ""[self %s]"" % f.name_lower_camel_case
                    f.return_method_implementation = \
""""""char string[%(array_length)d];
  mavlink_msg_%(message_name_lower)s_get_%(name)s(&(self->_message), (char *)&string);
  return [[NSString alloc] initWithBytes:string length:%(array_length)d encoding:NSASCIIStringEncoding];"""""" % {'array_length': f.array_length, 'message_name_lower': m.name_lower, 'name': f.name}

            if not f.return_method_implementation:
                f.return_method_implementation = \
""""""return mavlink_msg_%(message_name_lower)s_get_%(name)s(&(self->_message)%(get_arg)s);"""""" % {'message_name_lower': m.name_lower, 'name': f.name, 'get_arg': f.get_arg}

    for m in xml.message:
        m.arg_fields = []
        for f in m.fields:
            if not f.omit_arg:
                m.arg_fields.append(f)
 
    generate_message_definitions_h(directory, xml)
    for m in xml.message:
        generate_message(directory, m)",generate files for one XML file
"def load_csv(path):
    """"""Load data from a CSV file.

    Args:
        path (str): A path to the CSV format file containing data.
        dense (boolean): An optional variable indicating if the return matrix
                         should be dense.  By default, it is false.

    Returns:
        Data matrix X and target vector y
    """"""

    with open(path) as f:
        line = f.readline().strip()

    X = np.loadtxt(path, delimiter=',',
                   skiprows=0 if is_number(line.split(',')[0]) else 1)

    y = np.array(X[:, 0]).flatten()
    X = X[:, 1:]

    return X, y","Load data from a CSV file.

    Args:
        path (str): A path to the CSV format file containing data.
        dense (boolean): An optional variable indicating if the return matrix
                         should be dense.  By default, it is false.

    Returns:
        Data matrix X and target vector y"
"def isinstance(self, instance, class_name):
        """"""Check if a BaseNode is an instance of a registered dynamic class""""""
        if isinstance(instance, BaseNode):
            klass = self.dynamic_node_classes.get(class_name, None)
            if klass:
                return isinstance(instance, klass)
            # Not an instance of a class in the registry
            return False
        else:
            raise TypeError(""This function can only be used for BaseNode objects"")",Check if a BaseNode is an instance of a registered dynamic class
"def verb(**kwargs):
    """"""Create OAI-PMH envelope for response with verb.""""""
    e_tree, e_oaipmh = envelope(**kwargs)
    e_element = SubElement(e_oaipmh, etree.QName(NS_OAIPMH, kwargs['verb']))
    return e_tree, e_element",Create OAI-PMH envelope for response with verb.
"def get_token_network_registry_events(
        chain: BlockChainService,
        token_network_registry_address: PaymentNetworkID,
        contract_manager: ContractManager,
        events: Optional[List[str]] = ALL_EVENTS,
        from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,
        to_block: BlockSpecification = 'latest',
) -> List[Dict]:
    """""" Helper to get all events of the Registry contract at `registry_address`. """"""
    return get_contract_events(
        chain=chain,
        abi=contract_manager.get_contract_abi(CONTRACT_TOKEN_NETWORK_REGISTRY),
        contract_address=Address(token_network_registry_address),
        topics=events,
        from_block=from_block,
        to_block=to_block,
    )",Helper to get all events of the Registry contract at `registry_address`.
"def cds_identifier_validator(record, result):
    """"""Ensure that the two records have the same CDS identifier.

    This is needed because the search is done only for
    ``external_system_identifiers.value``, which might cause false positives in
    case the matched record has an identifier with the same ``value`` but
    ``schema`` different from CDS.

    Args:
        record (dict): the given record we are trying to match with similar ones in INSPIRE.
        result (dict): possible match returned by the ES query that needs to be validated.

    Returns:
        bool: validation decision.

    """"""

    record_external_identifiers = get_value(record, 'external_system_identifiers', [])
    result_external_identifiers = get_value(result, '_source.external_system_identifiers', [])

    record_external_identifiers = {external_id[""value""] for external_id in record_external_identifiers if external_id[""schema""] == 'CDS'}
    result_external_identifiers = {external_id[""value""] for external_id in result_external_identifiers if external_id[""schema""] == 'CDS'}

    return bool(record_external_identifiers & result_external_identifiers)","Ensure that the two records have the same CDS identifier.

    This is needed because the search is done only for
    ``external_system_identifiers.value``, which might cause false positives in
    case the matched record has an identifier with the same ``value`` but
    ``schema`` different from CDS.

    Args:
        record (dict): the given record we are trying to match with similar ones in INSPIRE.
        result (dict): possible match returned by the ES query that needs to be validated.

    Returns:
        bool: validation decision."
"def get_network_events(self):
        """"""
        :calls: `GET /networks/:owner/:repo/events <http://developer.github.com/v3/activity/events>`_
        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Event.Event`
        """"""
        return github.PaginatedList.PaginatedList(
            github.Event.Event,
            self._requester,
            ""/networks/"" + self.owner.login + ""/"" + self.name + ""/events"",
            None
        )",":calls: `GET /networks/:owner/:repo/events <http://developer.github.com/v3/activity/events>`_
        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Event.Event`"
"def ConvertToWireFormat(self, value):
    """"""Convert to the wire format.

    Args:
      value: is of type RepeatedFieldHelper.

    Returns:
      A wire format representation of the value.
    """"""
    output = _SerializeEntries(
        (python_format, wire_format, value.type_descriptor)
        for (python_format, wire_format) in value.wrapped_list)
    return b"""", b"""", output","Convert to the wire format.

    Args:
      value: is of type RepeatedFieldHelper.

    Returns:
      A wire format representation of the value."
"def add_body_part(self, key, data, mime_type, size=None):
    """"""Adds data to the HTTP request body.
   
    If more than one part is added, this is assumed to be a mime-multipart
    request. This method is designed to create MIME 1.0 requests as specified
    in RFC 1341.

    Args:
      data: str or a file-like object containing a part of the request body.
      mime_type: str The MIME type describing the data
      size: int Required if the data is a file like object. If the data is a
            string, the size is calculated so this parameter is ignored.
    """"""
    if isinstance(data, str):
      size = len(data)
    if hasattr(data, ""fileno""):
      size = os.fstat(data.fileno())[stat.ST_SIZE]
    if size is None:
      # TODO: support chunked transfer if some of the body is of unknown size.
      raise UnknownSize('Each part of the body must have a known size.')
    if 'Content-Length' in self.headers:
      content_length = int(self.headers['Content-Length'])
    else:
      content_length = 0
    # If this is the first part added to the body, then this is not a multipart
    # request.
    boundary_string = '\r\n--%s\r\n' % (MIME_BOUNDARY,)
    self._body_parts.append(boundary_string)
    content_length += len(boundary_string) + size
    # Include the mime type of this part.
    cd = 'Content-Disposition: form-data; name=""%s""' % key
    mt = mime_type
    if hasattr(data, ""fileno""):
        cd += '; filename=""%s""' % data.name.split('/')[-1]
        mt = mimetypes.guess_type(data.name)[0] or 'application/octet-stream'
    cd += '\r\n'
    type_string = 'Content-Type: %s\r\n\r\n' % (mt)
    self._body_parts.append(cd)
    self._body_parts.append(type_string)
    content_length += len(type_string) + len(cd)
    self._body_parts.append(data)
    self.headers['Content-Length'] = str(content_length)","Adds data to the HTTP request body.
   
    If more than one part is added, this is assumed to be a mime-multipart
    request. This method is designed to create MIME 1.0 requests as specified
    in RFC 1341.

    Args:
      data: str or a file-like object containing a part of the request body.
      mime_type: str The MIME type describing the data
      size: int Required if the data is a file like object. If the data is a
            string, the size is calculated so this parameter is ignored."
"def wait_for_batches(self, batch_ids, timeout=None):
        """"""Locks until a list of batch ids is committed to the block chain
        or a timeout is exceeded. Returns the statuses of those batches.

        Args:
            batch_ids (list of str): The ids of the batches to wait for
            timeout(int): Maximum time in seconds to wait for

        Returns:
            list of BatchStatus: BatchStatuses to send back to client
        """"""
        self._batch_tracker.watch_statuses(self, batch_ids)
        timeout = timeout or DEFAULT_TIMEOUT
        start_time = time()

        with self._wait_condition:
            while True:
                if self._statuses is not None:
                    return _format_batch_statuses(
                        self._statuses, batch_ids, self._batch_tracker)

                if time() - start_time > timeout:
                    statuses = self._batch_tracker.get_statuses(batch_ids)
                    return _format_batch_statuses(
                        statuses, batch_ids, self._batch_tracker)

                self._wait_condition.wait(timeout - (time() - start_time))","Locks until a list of batch ids is committed to the block chain
        or a timeout is exceeded. Returns the statuses of those batches.

        Args:
            batch_ids (list of str): The ids of the batches to wait for
            timeout(int): Maximum time in seconds to wait for

        Returns:
            list of BatchStatus: BatchStatuses to send back to client"
"def reload(self):
        """"""
        Re-fetches the object from the API, discarding any local changes.
        Returns without doing anything if the object is new.
        """"""

        if not self.id:
            return
        reloaded_object = self.__class__.find(self.id)
        self.set_raw(
            reloaded_object.raw,
            reloaded_object.etag
        )","Re-fetches the object from the API, discarding any local changes.
        Returns without doing anything if the object is new."
"async def main() -> None:
    """"""Create the aiohttp session and run the example.""""""
    logging.basicConfig(level=logging.INFO)

    async with ClientSession() as websession:
        try:
            client = Client(websession)

            await client.profile.login('<EMAIL>', '<PASSWORD>')
            _LOGGER.info('Account ID: %s', client.profile.account_id)

            summary = await client.profile.summary()
            _LOGGER.info('Account Summary: %s', summary)

            packages = await client.profile.packages()
            _LOGGER.info('Package Summary: %s', packages)
        except SeventeenTrackError as err:
            print(err)",Create the aiohttp session and run the example.
"def parse_coach_go(infile):
    """"""Parse a GO output file from COACH and return a rank-ordered list of GO term predictions

    The columns in all files are: GO terms, Confidence score, Name of GO terms. The files are:
        
        - GO_MF.dat - GO terms in 'molecular function'
        - GO_BP.dat - GO terms in 'biological process'
        - GO_CC.dat - GO terms in 'cellular component'

    Args:
        infile (str): Path to any COACH GO prediction file

    Returns:
        Pandas DataFrame: Organized dataframe of results, columns defined below
            
            - ``go_id``: GO term ID
            - ``go_term``: GO term text
            - ``c_score``: confidence score of the GO prediction

    """"""
    go_list = []

    with open(infile) as go_file:
        for line in go_file.readlines():
            go_dict = {}

            go_split = line.split()
            go_dict['go_id'] = go_split[0]
            go_dict['c_score'] = go_split[1]
            go_dict['go_term'] = ' '.join(go_split[2:])

            go_list.append(go_dict)

    return go_list","Parse a GO output file from COACH and return a rank-ordered list of GO term predictions

    The columns in all files are: GO terms, Confidence score, Name of GO terms. The files are:
        
        - GO_MF.dat - GO terms in 'molecular function'
        - GO_BP.dat - GO terms in 'biological process'
        - GO_CC.dat - GO terms in 'cellular component'

    Args:
        infile (str): Path to any COACH GO prediction file

    Returns:
        Pandas DataFrame: Organized dataframe of results, columns defined below
            
            - ``go_id``: GO term ID
            - ``go_term``: GO term text
            - ``c_score``: confidence score of the GO prediction"
"def import_schema(self, definitions, d):
        """"""Import schema as <types/> content.""""""
        if not definitions.types:
            root = Element(""types"", ns=wsdlns)
            definitions.root.insert(root)
            types = Types(root, definitions)
            definitions.types.append(types)
        else:
            types = definitions.types[-1]
        types.root.append(d.root)
        log.debug(""imported (XSD):\n%s"", d.root)",Import schema as <types/> content.
"def p_user_add_link(self):
        '''
        user add link.
        '''
        if self.check_post_role()['ADD']:
            pass
        else:
            return False
        post_data = self.get_post_data()

        post_data['user_name'] = self.get_current_user()

        cur_uid = tools.get_uudd(2)
        while MLink.get_by_uid(cur_uid):
            cur_uid = tools.get_uudd(2)

        if MLink.create_link(cur_uid, post_data):
            output = {
                'addinfo ': 1,
            }
        else:
            output = {
                'addinfo ': 0,
            }
        return json.dump(output, self)",user add link.
"def decrypt_PBEWithSHAAndTwofishCBC(encrypted_data, password, salt, iteration_count):
    """"""
    Decrypts PBEWithSHAAndTwofishCBC, assuming PKCS#12-generated PBE parameters.
    (Not explicitly defined as an algorithm in RFC 7292, but defined here nevertheless because of the assumption of PKCS#12 parameters).
    """"""
    iv  = derive_key(hashlib.sha1, PURPOSE_IV_MATERIAL,  password, salt, iteration_count, 16)
    key = derive_key(hashlib.sha1, PURPOSE_KEY_MATERIAL, password, salt, iteration_count, 256//8)

    encrypted_data = bytearray(encrypted_data)
    encrypted_data_len = len(encrypted_data)
    if encrypted_data_len % 16 != 0:
        raise BadDataLengthException(""encrypted data length is not a multiple of 16 bytes"")

    plaintext = bytearray()

    # slow and dirty CBC decrypt
    from twofish import Twofish
    cipher = Twofish(key)

    last_cipher_block = bytearray(iv)
    for block_offset in range(0, encrypted_data_len, 16):
        cipher_block = encrypted_data[block_offset:block_offset+16]
        plaintext_block = xor_bytearrays(bytearray(cipher.decrypt(bytes(cipher_block))), last_cipher_block)
        plaintext.extend(plaintext_block)
        last_cipher_block = cipher_block

    plaintext = strip_pkcs7_padding(plaintext, 16)
    return bytes(plaintext)","Decrypts PBEWithSHAAndTwofishCBC, assuming PKCS#12-generated PBE parameters.
    (Not explicitly defined as an algorithm in RFC 7292, but defined here nevertheless because of the assumption of PKCS#12 parameters)."
"def click(self):
        """"""
        Pretends to user clicked it, sending the signal and everything.
        """"""
        if self.is_checkable():        
            if self.is_checked(): self.set_checked(False)
            else:                 self.set_checked(True)
            self.signal_clicked.emit(self.is_checked())
        else:
            self.signal_clicked.emit(True)
        return self","Pretends to user clicked it, sending the signal and everything."
"def mb_neg_loglik(self, beta, mini_batch):
        """""" Creates the negative log likelihood of the model

        Parameters
        ----------
        beta : np.array
            Contains untransformed starting values for latent variables

        mini_batch : int
            Size of each mini batch of data

        Returns
        ----------
        The negative log logliklihood of the model
        """"""    

        rand_int =  np.random.randint(low=0, high=self.data.shape[0]-mini_batch-self.max_lag+1)
        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)

        _, _, _, F, v = self._model(self.data[sample],beta)
        loglik = 0.0
        for i in range(0,len(sample)):
            loglik += np.linalg.slogdet(F[:,:,i])[1] + np.dot(v[i],np.dot(np.linalg.pinv(F[:,:,i]),v[i]))
        return -(-((len(sample)/2)*np.log(2*np.pi))-0.5*loglik.T[0].sum())","Creates the negative log likelihood of the model

        Parameters
        ----------
        beta : np.array
            Contains untransformed starting values for latent variables

        mini_batch : int
            Size of each mini batch of data

        Returns
        ----------
        The negative log logliklihood of the model"
"def _data_graph_add_edge(self, src, dst, **edge_labels):
        """"""
        Add an edge in the data dependence graph.

        :param ProgramVariable src: Source node.
        :param ProgramVariable dst: Destination node.
        :param edge_labels: All labels associated with the edge.
        :return: None
        """"""

        if src in self._data_graph and dst in self._data_graph[src]:
            return

        self._data_graph.add_edge(src, dst, **edge_labels)

        self._simplified_data_graph = None","Add an edge in the data dependence graph.

        :param ProgramVariable src: Source node.
        :param ProgramVariable dst: Destination node.
        :param edge_labels: All labels associated with the edge.
        :return: None"
"def _traverse(summary, function, *args):
    """"""Traverse all objects of a summary and call function with each as a
    parameter.

    Using this function, the following objects will be traversed:
    - the summary
    - each row
    - each item of a row
    """"""
    function(summary, *args)
    for row in summary:
        function(row, *args)
        for item in row:
            function(item, *args)","Traverse all objects of a summary and call function with each as a
    parameter.

    Using this function, the following objects will be traversed:
    - the summary
    - each row
    - each item of a row"
"def runfile(fd, argv, fd_name='<unknown>', compile_flags=0, dont_inherit=1,
        filename=None, threads=True, verbose=False):
    """"""
    Run code from given file descriptor with profiling enabled.
    Closes fd before executing contained code.
    """"""
    _run(threads, verbose, 'runfile', filename, fd, argv, fd_name,
        compile_flags, dont_inherit)","Run code from given file descriptor with profiling enabled.
    Closes fd before executing contained code."
"def asyncPipeSubstr(context=None, _INPUT=None, conf=None, **kwargs):
    """"""A string module that asynchronously returns a substring. Loopable.

    Parameters
    ----------
    context : pipe2py.Context object
    _INPUT : twisted Deferred iterable of items or strings
    conf : {
        'from': {'type': 'number', value': <starting position>},
        'length': {'type': 'number', 'value': <count of characters to return>}
    }

    returns
    -------
    _OUTPUT : twisted.internet.defer.Deferred generator of substrings
    """"""
    conf['start'] = conf.pop('from', dict.get(conf, 'start'))
    splits = yield asyncGetSplits(_INPUT, conf, **cdicts(opts, kwargs))
    parsed = yield asyncDispatch(splits, *get_async_dispatch_funcs())
    _OUTPUT = yield asyncStarMap(partial(maybeDeferred, parse_result), parsed)
    returnValue(iter(_OUTPUT))","A string module that asynchronously returns a substring. Loopable.

    Parameters
    ----------
    context : pipe2py.Context object
    _INPUT : twisted Deferred iterable of items or strings
    conf : {
        'from': {'type': 'number', value': <starting position>},
        'length': {'type': 'number', 'value': <count of characters to return>}
    }

    returns
    -------
    _OUTPUT : twisted.internet.defer.Deferred generator of substrings"
"def get_bool_relative(strings: Sequence[str],
                      prefix1: str,
                      delta: int,
                      prefix2: str,
                      ignoreleadingcolon: bool = False) -> Optional[bool]:
    """"""
    Fetches a boolean parameter via :func:`get_string_relative`.
    """"""
    return get_bool_raw(get_string_relative(
        strings, prefix1, delta, prefix2,
        ignoreleadingcolon=ignoreleadingcolon))",Fetches a boolean parameter via :func:`get_string_relative`.
"def parse_result_to_dsl(tokens):
    """"""Convert a ParseResult to a PyBEL DSL object.

    :type tokens: dict or pyparsing.ParseResults
    :rtype: BaseEntity
    """"""
    if MODIFIER in tokens:
        return parse_result_to_dsl(tokens[TARGET])

    elif REACTION == tokens[FUNCTION]:
        return _reaction_po_to_dict(tokens)

    elif VARIANTS in tokens:
        return _variant_po_to_dict(tokens)

    elif MEMBERS in tokens:
        return _list_po_to_dict(tokens)

    elif FUSION in tokens:
        return _fusion_to_dsl(tokens)

    return _simple_po_to_dict(tokens)","Convert a ParseResult to a PyBEL DSL object.

    :type tokens: dict or pyparsing.ParseResults
    :rtype: BaseEntity"
"def page_title(step, title):
    """"""
    Check that the page title matches the given one.
    """"""

    with AssertContextManager(step):
        assert_equals(world.browser.title, title)",Check that the page title matches the given one.
"def humidity(self):
        """"""
        The relative humidity in RH %
        returns None if humidity measurement is disabled
        """"""
        self._read_temperature()
        hum = self._read_register(_BME280_REGISTER_HUMIDDATA, 2)
        #print(""Humidity data: "", hum)
        adc = float(hum[0] << 8 | hum[1])
        #print(""adc:"", adc)

        # Algorithm from the BME280 driver
        # https://github.com/BoschSensortec/BME280_driver/blob/master/bme280.c
        var1 = float(self._t_fine) - 76800.0
        #print(""var1 "", var1)
        var2 = (self._humidity_calib[3] * 64.0 + (self._humidity_calib[4] / 16384.0) * var1)
        #print(""var2 "",var2)
        var3 = adc - var2
        #print(""var3 "",var3)
        var4 = self._humidity_calib[1] / 65536.0
        #print(""var4 "",var4)
        var5 = (1.0 + (self._humidity_calib[2] / 67108864.0) * var1)
        #print(""var5 "",var5)
        var6 = 1.0 + (self._humidity_calib[5] / 67108864.0) * var1 * var5
        #print(""var6 "",var6)
        var6 = var3 * var4 * (var5 * var6)
        humidity = var6 * (1.0 - self._humidity_calib[0] * var6 / 524288.0)

        if humidity > _BME280_HUMIDITY_MAX:
            return _BME280_HUMIDITY_MAX
        if humidity < _BME280_HUMIDITY_MIN:
            return _BME280_HUMIDITY_MIN
        # else...
        return humidity","The relative humidity in RH %
        returns None if humidity measurement is disabled"
"def create_spot_nodes(self):
        """"""Create spot nodes.
        """"""
        self.create_stack(
            self.spot_nodes_name,
            'amazon-spot-nodes.yaml',
            parameters=define_parameters(
                ClusterName=self.cluster_name,
                Subnets=self.subnet_ids,
                NodeInstanceProfile=self.node_instance_profile,
                NodeInstanceRole=self.node_instance_role,
                NodeSecurityGroup=self.node_security_group,

            )
        )",Create spot nodes.
"def attach(self, attached_file, **attrs):
        """"""
        Attach a file to the :class:`Task`

        :param attached_file: file path to attach
        :param attrs: optional attributes for the attached file
        """"""
        return TaskAttachments(self.requester).create(
            self.project, self.id,
            attached_file, **attrs
        )","Attach a file to the :class:`Task`

        :param attached_file: file path to attach
        :param attrs: optional attributes for the attached file"
"def mergeSplitsOnInterfaces(root: LNode):
    """"""
    collect all split/concatenation nodes and group them by target interface
    """"""
    for ch in root.children:
        if ch.children:
            mergeSplitsOnInterfaces(ch)

    ctx = MergeSplitsOnInterfacesCtx()
    for ch in root.children:
        srcPorts = None
        try:
            if ch.name == ""CONCAT"":
                p = single(ch.east, lambda x: True)
                e = single(p.outgoingEdges, lambda x: True)
                srcPorts = e.dsts
            elif ch.name == ""SLICE"":
                p = single(ch.west, lambda x: True)
                e = single(p.incomingEdges, lambda x: True)
                srcPorts = e.srcs
        except (DuplicitValueExc, NoValueExc):
            continue

        if srcPorts is not None:
            for srcPort in srcPorts:
                if isinstance(srcPort.parent, LPort):
                    # only for non primitive ports
                    rootPort = getRootIntfPort(srcPort)
                    ctx.register(rootPort, ch, e)

    # join them if it is possible
    for srcPort, splitsAndConcats in ctx.iterPortSplits():
        if len(splitsAndConcats) <= 1:
            continue

        name = ""SPLIT"" if srcPort.direction == PortType.OUTPUT else ""CONCAT""
        newSplitNode = root.addNode(name)
        copyPort(srcPort, newSplitNode, True, """")
        n = splitsAndConcats[0][0]
        for i in range(max(len(n.west),
                           len(n.east))):
            copyPort(
                srcPort, newSplitNode,
                False, ""[%d]"" % i)

        reconnectPorts(root, srcPort, splitsAndConcats,
                       newSplitNode)",collect all split/concatenation nodes and group them by target interface
"def get_dataset_end_date(self, date_format=None):
        # type: (Optional[str]) -> Optional[str]
        """"""Get dataset date as string in specified format. For range returns start date.
        If no format is supplied, an ISO 8601 string is returned.

        Args:
            date_format (Optional[str]): Date format. None is taken to be ISO 8601. Defaults to None.

        Returns:
            Optional[str]: Dataset date string or None if no date is set
        """"""
        dataset_date = self.get_dataset_end_date_as_datetime()
        return self._get_formatted_date(dataset_date, date_format)","Get dataset date as string in specified format. For range returns start date.
        If no format is supplied, an ISO 8601 string is returned.

        Args:
            date_format (Optional[str]): Date format. None is taken to be ISO 8601. Defaults to None.

        Returns:
            Optional[str]: Dataset date string or None if no date is set"
"def delete(self, url, data=None, params=None):
        """"""
        Low-level DELETE request interface to mite. Takes a URL to request
        (relative), and optionally data to add to the request. Either returns
        the JSON body of the request or raises a HttpException.

        """"""
        return self.request(""delete"", url, data, params)","Low-level DELETE request interface to mite. Takes a URL to request
        (relative), and optionally data to add to the request. Either returns
        the JSON body of the request or raises a HttpException."
"def read_user_yes_no(question, default_value):
    """"""Prompt the user to reply with 'yes' or 'no' (or equivalent values).

    Note:
      Possible choices are 'true', '1', 'yes', 'y' or 'false', '0', 'no', 'n'

    :param str question: Question to the user
    :param default_value: Value that will be returned if no input happens
    """"""
    # Please see http://click.pocoo.org/4/api/#click.prompt
    return click.prompt(
        question,
        default=default_value,
        type=click.BOOL
    )","Prompt the user to reply with 'yes' or 'no' (or equivalent values).

    Note:
      Possible choices are 'true', '1', 'yes', 'y' or 'false', '0', 'no', 'n'

    :param str question: Question to the user
    :param default_value: Value that will be returned if no input happens"
"def close(self):
        """"""Close the connection""""""

        if self.closed:
            return

        # Make sure no one minds the connection to be closed
        # This will help avoid MemoryError in other threads,
        # they will get sqlite3.ProgrammingError instead
        if not self.personal_lock.acquire(timeout=self.lock_timeout):
            raise LockTimeoutError(self)

        try:
            try:
                if self.in_transaction:
                    self.db_state.active_connection = None
                    self.db_state.transaction_lock.release()
            except sqlite3.ProgrammingError:
                pass

            if self._cursor is not None and not self._cursor.closed:
                self._cursor._cursor.close()
                self._cursor.closed = True

            if self.connection is not None:
                self.connection.close()

            self.closed = True
        finally:
            self.personal_lock.release()",Close the connection
"def _look_for_interface(self, network_backend):
        """"""
        Look for an interface with a specific network backend.

        :returns: interface number or -1 if none is found
        """"""

        result = yield from self._execute(""showvminfo"", [self._vmname, ""--machinereadable""])
        interface = -1
        for info in result.splitlines():
            if '=' in info:
                name, value = info.split('=', 1)
                if name.startswith(""nic"") and value.strip('""') == network_backend:
                    try:
                        interface = int(name[3:])
                        break
                    except ValueError:
                        continue
        return interface","Look for an interface with a specific network backend.

        :returns: interface number or -1 if none is found"
"def run_scheduler_jobs(
    scheduler: str,
    jobs: Iterator[Job],
    directory: PathLike = Path.cwd(),
    basename: str = ""experi"",
    dry_run: bool = False,
) -> None:
    """"""Submit a series of commands to a batch scheduler.

    This takes a list of strings which are the contents of the pbs files, writes the
    files to disk and submits the job to the scheduler. Files which match the pattern of
    the resulting files <basename>_<index>.pbs are deleted before writing the new files.

    To ensure that commands run consecutively the aditional requirement to the run
    script `-W depend=afterok:<prev_jobid>` is added. This allows for all the components
    of the experiment to be conducted in a single script.

    Note: Having this function submit jobs requires that the command `qsub` exists,
    implying that a job scheduler is installed.

    """"""
    submit_job = True
    logger.debug(""Creating commands in %s files."", scheduler)

    # Check scheduler submit command exists
    if scheduler == ""pbs"":
        submit_executable = ""qsub""
    elif scheduler == ""slurm"":
        submit_executable = ""sbatch""
    else:
        raise ValueError(""scheduler can only take values ['pbs', 'slurm']"")

    if shutil.which(submit_executable) is None:
        logger.warning(
            ""The `%s` command is not found.""
            ""Skipping job submission and just generating files"",
            submit_executable,
        )
        submit_job = False

    # Ensure directory is a Path
    directory = Path(directory)

    # remove existing files
    for fname in directory.glob(basename + f""*.{scheduler}""):
        print(""Removing {}"".format(fname))
        os.remove(str(fname))

    # Write new files and generate commands
    prev_jobids: List[str] = []
    for index, job in enumerate(jobs):
        # Generate scheduler file
        content = create_scheduler_file(scheduler, job)
        logger.debug(""File contents:\n%s"", content)
        # Write file to disk
        fname = Path(directory / ""{}_{:02d}.{}"".format(basename, index, scheduler))
        with fname.open(""w"") as dst:
            dst.write(content)

        if submit_job or dry_run:
            # Construct command
            submit_cmd = [submit_executable]

            if prev_jobids:
                # Continue to append all previous jobs to submit_cmd so subsequent jobs die along
                # with the first.
                afterok = f""afterok:{':'.join(prev_jobids)}""
                if scheduler == ""pbs"":
                    submit_cmd += [""-W"", f""depend={afterok}""]
                elif scheduler == ""slurm"":
                    submit_cmd += [""--dependency"", afterok]

            # actually run the command
            logger.info(str(submit_cmd))
            try:
                if dry_run:
                    print(f""{submit_cmd} {fname.name}"")
                    prev_jobids.append(""dry_run"")
                else:
                    cmd_res = subprocess.check_output(
                        submit_cmd + [fname.name], cwd=str(directory)
                    )
                    prev_jobids.append(cmd_res.decode().strip())
            except subprocess.CalledProcessError:
                logger.error(""Submitting job to the queue failed."")
                break","Submit a series of commands to a batch scheduler.

    This takes a list of strings which are the contents of the pbs files, writes the
    files to disk and submits the job to the scheduler. Files which match the pattern of
    the resulting files <basename>_<index>.pbs are deleted before writing the new files.

    To ensure that commands run consecutively the aditional requirement to the run
    script `-W depend=afterok:<prev_jobid>` is added. This allows for all the components
    of the experiment to be conducted in a single script.

    Note: Having this function submit jobs requires that the command `qsub` exists,
    implying that a job scheduler is installed."
"async def do_write_aldb(self, args):
        """"""Write device All-Link record.

        WARNING THIS METHOD CAN DAMAGE YOUR DEVICE IF USED INCORRECTLY.
        Please ensure the memory id is appropriate for the device.
        You must load the ALDB of the device before using this method.
        The memory id must be an existing memory id in the ALDB or this
        method will return an error.

        If you are looking to create a new link between two devices,
        use the `link_devices` command or the `start_all_linking` command.

        Usage:
           write_aldb addr memory mode group target [data1 data2 data3]

        Required Parameters:
            addr: Inseon address of the device to write
            memory: record ID of the record to write (i.e. 0fff)
            mode: r | c
                    r = Device is a responder of target
                    c = Device is a controller of target
            group:  All-Link group integer
            target: Insteon address of the link target device

        Optional Parameters:
            data1: int = Device sepcific
            data2: int = Device specific
            data3: int = Device specific
        """"""
        params = args.split()
        addr = None
        mem_bytes = None
        memory = None
        mode = None
        group = None
        target = None
        data1 = 0x00
        data2 = 0x00
        data3 = 0x00

        try:
            addr = Address(params[0])
            mem_bytes = binascii.unhexlify(params[1])
            memory = int.from_bytes(mem_bytes, byteorder='big')
            mode = params[2]
            group = int(params[3])
            target = Address(params[4])

            _LOGGING.info('address: %s', addr)
            _LOGGING.info('memory: %04x', memory)
            _LOGGING.info('mode: %s', mode)
            _LOGGING.info('group: %d', group)
            _LOGGING.info('target: %s', target)

        except IndexError:
            _LOGGING.error('Device address memory mode group and target '
                           'are all required.')
            self.do_help('write_aldb')
        except ValueError:
            _LOGGING.error('Value error - Check parameters')
            self.do_help('write_aldb')

        try:
            data1 = int(params[5])
            data2 = int(params[6])
            data3 = int(params[7])
        except IndexError:
            pass
        except ValueError:
            addr = None
            _LOGGING.error('Value error - Check parameters')
            self.do_help('write_aldb')
            return

        if addr and memory and mode and isinstance(group, int) and target:
            await self.tools.write_aldb(addr, memory, mode, group, target,
                                        data1, data2, data3)","Write device All-Link record.

        WARNING THIS METHOD CAN DAMAGE YOUR DEVICE IF USED INCORRECTLY.
        Please ensure the memory id is appropriate for the device.
        You must load the ALDB of the device before using this method.
        The memory id must be an existing memory id in the ALDB or this
        method will return an error.

        If you are looking to create a new link between two devices,
        use the `link_devices` command or the `start_all_linking` command.

        Usage:
           write_aldb addr memory mode group target [data1 data2 data3]

        Required Parameters:
            addr: Inseon address of the device to write
            memory: record ID of the record to write (i.e. 0fff)
            mode: r | c
                    r = Device is a responder of target
                    c = Device is a controller of target
            group:  All-Link group integer
            target: Insteon address of the link target device

        Optional Parameters:
            data1: int = Device sepcific
            data2: int = Device specific
            data3: int = Device specific"
"def _ipython_display_(self):
        """"""Display Jupyter Notebook widget""""""
        from IPython.display import display

        self.build_widget()
        display(self.widget())",Display Jupyter Notebook widget
"def merge_results(self, other_processor):
        """"""Merge the results of this processor with those of another.""""""
        if not isinstance(other_processor, self.__class__):
            raise ValueError(""Can only extend with another %s instance.""
                             % self.__class__.__name__)
        self.statements.extend(other_processor.statements)
        if other_processor.statements_sample is not None:
            if self.statements_sample is None:
                self.statements_sample = other_processor.statements_sample
            else:
                self.statements_sample.extend(other_processor.statements_sample)

        self._merge_json(other_processor.__statement_jsons,
                         other_processor.__evidence_counts)
        return",Merge the results of this processor with those of another.
"def new_approve_transaction(self, asset: str, b58_send_address: str, b58_recv_address: str, amount: int,
                                b58_payer_address: str, gas_limit: int, gas_price: int) -> Transaction:
        """"""
        This interface is used to generate a Transaction object for approve.

        :param asset: a string which is used to indicate which asset we want to approve.
        :param b58_send_address: a base58 encode address which indicate where the approve from.
        :param b58_recv_address: a base58 encode address which indicate where the approve to.
        :param amount: the amount of asset that will be approved.
        :param b58_payer_address: a base58 encode address which indicate who will pay for the transaction.
        :param gas_limit: an int value that indicate the gas limit.
        :param gas_price: an int value that indicate the gas price.
        :return: a Transaction object which can be used for approve.
        """"""
        if not isinstance(b58_send_address, str) or not isinstance(b58_recv_address, str):
            raise SDKException(ErrorCode.param_err('the data type of base58 encode address should be the string.'))
        if len(b58_send_address) != 34 or len(b58_recv_address) != 34:
            raise SDKException(ErrorCode.param_err('the length of base58 encode address should be 34 bytes.'))
        if amount <= 0:
            raise SDKException(ErrorCode.other_error('the amount should be greater than than zero.'))
        if gas_price < 0:
            raise SDKException(ErrorCode.other_error('the gas price should be equal or greater than zero.'))
        if gas_limit < 0:
            raise SDKException(ErrorCode.other_error('the gas limit should be equal or greater than zero.'))
        contract_address = self.get_asset_address(asset)
        raw_send = Address.b58decode(b58_send_address).to_bytes()
        raw_recv = Address.b58decode(b58_recv_address).to_bytes()
        raw_payer = Address.b58decode(b58_payer_address).to_bytes()
        args = {""from"": raw_send, ""to"": raw_recv, ""amount"": amount}
        invoke_code = build_native_invoke_code(contract_address, b'\x00', 'approve', args)
        return Transaction(0, 0xd1, int(time()), gas_price, gas_limit, raw_payer, invoke_code, bytearray(), list())","This interface is used to generate a Transaction object for approve.

        :param asset: a string which is used to indicate which asset we want to approve.
        :param b58_send_address: a base58 encode address which indicate where the approve from.
        :param b58_recv_address: a base58 encode address which indicate where the approve to.
        :param amount: the amount of asset that will be approved.
        :param b58_payer_address: a base58 encode address which indicate who will pay for the transaction.
        :param gas_limit: an int value that indicate the gas limit.
        :param gas_price: an int value that indicate the gas price.
        :return: a Transaction object which can be used for approve."
"def map(requests, stream=False, size=None, exception_handler=None, gtimeout=None):
    """"""Concurrently converts a list of Requests to Responses.

    :param requests: a collection of Request objects.
    :param stream: If True, the content will not be downloaded immediately.
    :param size: Specifies the number of requests to make at a time. If None, no throttling occurs.
    :param exception_handler: Callback function, called when exception occured. Params: Request, Exception
    :param gtimeout: Gevent joinall timeout in seconds. (Note: unrelated to requests timeout)
    """"""

    requests = list(requests)

    pool = Pool(size) if size else None
    jobs = [send(r, pool, stream=stream) for r in requests]
    gevent.joinall(jobs, timeout=gtimeout)

    ret = []

    for request in requests:
        if request.response is not None:
            ret.append(request.response)
        elif exception_handler and hasattr(request, 'exception'):
            ret.append(exception_handler(request, request.exception))
        else:
            ret.append(None)

    return ret","Concurrently converts a list of Requests to Responses.

    :param requests: a collection of Request objects.
    :param stream: If True, the content will not be downloaded immediately.
    :param size: Specifies the number of requests to make at a time. If None, no throttling occurs.
    :param exception_handler: Callback function, called when exception occured. Params: Request, Exception
    :param gtimeout: Gevent joinall timeout in seconds. (Note: unrelated to requests timeout)"
"def _check_symlink_ownership(path, user, group, win_owner):
    '''
    Check if the symlink ownership matches the specified user and group
    '''
    cur_user, cur_group = _get_symlink_ownership(path)
    if salt.utils.platform.is_windows():
        return win_owner == cur_user
    else:
        return (cur_user == user) and (cur_group == group)",Check if the symlink ownership matches the specified user and group
"def dipole_k(src, rec, depth, res, freq, wavenumber, ab=11, aniso=None,
             epermH=None, epermV=None, mpermH=None, mpermV=None, verb=2):
    r""""""Return the electromagnetic wavenumber-domain field.

    Calculate the electromagnetic wavenumber-domain field due to infinitesimal
    small electric or magnetic dipole source(s), measured by infinitesimal
    small electric or magnetic dipole receiver(s); sources and receivers are
    directed along the principal directions x, y, or z, and all sources are at
    the same depth, as well as all receivers are at the same depth.


    See Also
    --------
    dipole : Electromagnetic field due to an electromagnetic source (dipoles).
    bipole : Electromagnetic field due to an electromagnetic source (bipoles).
    fem : Electromagnetic frequency-domain response.
    tem : Electromagnetic time-domain response.


    Parameters
    ----------
    src, rec : list of floats or arrays
        Source and receiver coordinates (m): [x, y, z].
        The x- and y-coordinates can be arrays, z is a single value.
        The x- and y-coordinates must have the same dimension.
        The x- and y-coordinates only matter for the angle-dependent factor.

        Sources or receivers placed on a layer interface are considered in the
        upper layer.

    depth : list
        Absolute layer interfaces z (m); #depth = #res - 1
        (excluding +/- infinity).

    res : array_like
        Horizontal resistivities rho_h (Ohm.m); #res = #depth + 1.

    freq : array_like
        Frequencies f (Hz), used to calculate etaH/V and zetaH/V.

    wavenumber : array
        Wavenumbers lambda (1/m)

    ab : int, optional
        Source-receiver configuration, defaults to 11.

        +---------------+-------+------+------+------+------+------+------+
        |                       | electric  source   | magnetic source    |
        +===============+=======+======+======+======+======+======+======+
        |                       | **x**| **y**| **z**| **x**| **y**| **z**|
        +---------------+-------+------+------+------+------+------+------+
        |               | **x** |  11  |  12  |  13  |  14  |  15  |  16  |
        + **electric**  +-------+------+------+------+------+------+------+
        |               | **y** |  21  |  22  |  23  |  24  |  25  |  26  |
        + **receiver**  +-------+------+------+------+------+------+------+
        |               | **z** |  31  |  32  |  33  |  34  |  35  |  36  |
        +---------------+-------+------+------+------+------+------+------+
        |               | **x** |  41  |  42  |  43  |  44  |  45  |  46  |
        + **magnetic**  +-------+------+------+------+------+------+------+
        |               | **y** |  51  |  52  |  53  |  54  |  55  |  56  |
        + **receiver**  +-------+------+------+------+------+------+------+
        |               | **z** |  61  |  62  |  63  |  64  |  65  |  66  |
        +---------------+-------+------+------+------+------+------+------+

    aniso : array_like, optional
        Anisotropies lambda = sqrt(rho_v/rho_h) (-); #aniso = #res.
        Defaults to ones.

    epermH, epermV : array_like, optional
        Relative horizontal/vertical electric permittivities
        epsilon_h/epsilon_v (-);
        #epermH = #epermV = #res. Default is ones.

    mpermH, mpermV : array_like, optional
        Relative horizontal/vertical magnetic permeabilities mu_h/mu_v (-);
        #mpermH = #mpermV = #res. Default is ones.

    verb : {0, 1, 2, 3, 4}, optional
        Level of verbosity, default is 2:
            - 0: Print nothing.
            - 1: Print warnings.
            - 2: Print additional runtime and kernel calls
            - 3: Print additional start/stop, condensed parameter information.
            - 4: Print additional full parameter information


    Returns
    -------
    PJ0, PJ1 : array
        Wavenumber-domain EM responses:
            - PJ0: Wavenumber-domain solution for the kernel with a Bessel
              function of the first kind of order zero.
            - PJ1: Wavenumber-domain solution for the kernel with a Bessel
              function of the first kind of order one.


    Examples
    --------
    >>> import numpy as np
    >>> from empymod.model import dipole_k
    >>> src = [0, 0, 100]
    >>> rec = [5000, 0, 200]
    >>> depth = [0, 300, 1000, 1050]
    >>> res = [1e20, .3, 1, 50, 1]
    >>> freq = 1
    >>> wavenrs = np.logspace(-3.7, -3.6, 10)
    >>> PJ0, PJ1 = dipole_k(src, rec, depth, res, freq, wavenrs, verb=0)
    >>> print(PJ0)
    [ -1.02638329e-08 +4.91531529e-09j  -1.05289724e-08 +5.04222413e-09j
      -1.08009148e-08 +5.17238608e-09j  -1.10798310e-08 +5.30588284e-09j
      -1.13658957e-08 +5.44279805e-09j  -1.16592877e-08 +5.58321732e-09j
      -1.19601897e-08 +5.72722830e-09j  -1.22687889e-08 +5.87492067e-09j
      -1.25852765e-08 +6.02638626e-09j  -1.29098481e-08 +6.18171904e-09j]
    >>> print(PJ1)
    [  1.79483705e-10 -6.59235332e-10j   1.88672497e-10 -6.93749344e-10j
       1.98325814e-10 -7.30068377e-10j   2.08466693e-10 -7.68286748e-10j
       2.19119282e-10 -8.08503709e-10j   2.30308887e-10 -8.50823701e-10j
       2.42062030e-10 -8.95356636e-10j   2.54406501e-10 -9.42218177e-10j
       2.67371420e-10 -9.91530051e-10j   2.80987292e-10 -1.04342036e-09j]
    """"""

    # === 1.  LET'S START ============
    t0 = printstartfinish(verb)

    # === 2.  CHECK INPUT ============

    # Check layer parameters (isfullspace not required)
    modl = check_model(depth, res, aniso, epermH, epermV, mpermH, mpermV,
                       False, verb)
    depth, res, aniso, epermH, epermV, mpermH, mpermV, _ = modl

    # Check frequency => get etaH, etaV, zetaH, and zetaV
    f = check_frequency(freq, res, aniso, epermH, epermV, mpermH, mpermV, verb)
    freq, etaH, etaV, zetaH, zetaV = f

    # Check src-rec configuration
    # => Get flags if src or rec or both are magnetic (msrc, mrec)
    ab_calc, msrc, mrec = check_ab(ab, verb)

    # Check src and rec
    src, nsrc = check_dipole(src, 'src', verb)
    rec, nrec = check_dipole(rec, 'rec', verb)

    # Get angle-dependent factor
    off, angle = get_off_ang(src, rec, nsrc, nrec, verb)
    factAng = kernel.angle_factor(angle, ab, msrc, mrec)

    # Get layer number in which src and rec reside (lsrc/lrec)
    lsrc, zsrc = get_layer_nr(src, depth)
    lrec, zrec = get_layer_nr(rec, depth)

    # === 3. EM-FIELD CALCULATION ============

    # Pre-allocate
    if off.size == 1 and np.ndim(wavenumber) == 2:
        PJ0 = np.zeros((freq.size, wavenumber.shape[0], wavenumber.shape[1]),
                       dtype=complex)
        PJ1 = np.zeros((freq.size, wavenumber.shape[0], wavenumber.shape[1]),
                       dtype=complex)
    else:
        PJ0 = np.zeros((freq.size, off.size, wavenumber.size), dtype=complex)
        PJ1 = np.zeros((freq.size, off.size, wavenumber.size), dtype=complex)

    # If <ab> = 36 (or 63), field is zero
    # In `bipole` and in `dipole`, this is taken care of in `fem`. Here we
    # have to take care of it separately
    if ab_calc not in [36, ]:

        # Calculate wavenumber response
        J0, J1, J0b = kernel.wavenumber(zsrc, zrec, lsrc, lrec, depth, etaH,
                                        etaV, zetaH, zetaV,
                                        np.atleast_2d(wavenumber), ab_calc,
                                        False, msrc, mrec, False)

        # Collect output
        if J1 is not None:
            PJ1 += factAng[:, np.newaxis]*J1
            if ab in [11, 12, 21, 22, 14, 24, 15, 25]:  # Because of J2
                # J2(kr) = 2/(kr)*J1(kr) - J0(kr)
                PJ1 /= off[:, None]
        if J0 is not None:
            PJ0 += J0
        if J0b is not None:
            PJ0 += factAng[:, np.newaxis]*J0b

    # === 4.  FINISHED ============
    printstartfinish(verb, t0, 1)

    return np.squeeze(PJ0), np.squeeze(PJ1)","r""""""Return the electromagnetic wavenumber-domain field.

    Calculate the electromagnetic wavenumber-domain field due to infinitesimal
    small electric or magnetic dipole source(s), measured by infinitesimal
    small electric or magnetic dipole receiver(s); sources and receivers are
    directed along the principal directions x, y, or z, and all sources are at
    the same depth, as well as all receivers are at the same depth.


    See Also
    --------
    dipole : Electromagnetic field due to an electromagnetic source (dipoles).
    bipole : Electromagnetic field due to an electromagnetic source (bipoles).
    fem : Electromagnetic frequency-domain response.
    tem : Electromagnetic time-domain response.


    Parameters
    ----------
    src, rec : list of floats or arrays
        Source and receiver coordinates (m): [x, y, z].
        The x- and y-coordinates can be arrays, z is a single value.
        The x- and y-coordinates must have the same dimension.
        The x- and y-coordinates only matter for the angle-dependent factor.

        Sources or receivers placed on a layer interface are considered in the
        upper layer.

    depth : list
        Absolute layer interfaces z (m); #depth = #res - 1
        (excluding +/- infinity).

    res : array_like
        Horizontal resistivities rho_h (Ohm.m); #res = #depth + 1.

    freq : array_like
        Frequencies f (Hz), used to calculate etaH/V and zetaH/V.

    wavenumber : array
        Wavenumbers lambda (1/m)

    ab : int, optional
        Source-receiver configuration, defaults to 11.

        +---------------+-------+------+------+------+------+------+------+
        |                       | electric  source   | magnetic source    |
        +===============+=======+======+======+======+======+======+======+
        |                       | **x**| **y**| **z**| **x**| **y**| **z**|
        +---------------+-------+------+------+------+------+------+------+
        |               | **x** |  11  |  12  |  13  |  14  |  15  |  16  |
        + **electric**  +-------+------+------+------+------+------+------+
        |               | **y** |  21  |  22  |  23  |  24  |  25  |  26  |
        + **receiver**  +-------+------+------+------+------+------+------+
        |               | **z** |  31  |  32  |  33  |  34  |  35  |  36  |
        +---------------+-------+------+------+------+------+------+------+
        |               | **x** |  41  |  42  |  43  |  44  |  45  |  46  |
        + **magnetic**  +-------+------+------+------+------+------+------+
        |               | **y** |  51  |  52  |  53  |  54  |  55  |  56  |
        + **receiver**  +-------+------+------+------+------+------+------+
        |               | **z** |  61  |  62  |  63  |  64  |  65  |  66  |
        +---------------+-------+------+------+------+------+------+------+

    aniso : array_like, optional
        Anisotropies lambda = sqrt(rho_v/rho_h) (-); #aniso = #res.
        Defaults to ones.

    epermH, epermV : array_like, optional
        Relative horizontal/vertical electric permittivities
        epsilon_h/epsilon_v (-);
        #epermH = #epermV = #res. Default is ones.

    mpermH, mpermV : array_like, optional
        Relative horizontal/vertical magnetic permeabilities mu_h/mu_v (-);
        #mpermH = #mpermV = #res. Default is ones.

    verb : {0, 1, 2, 3, 4}, optional
        Level of verbosity, default is 2:
            - 0: Print nothing.
            - 1: Print warnings.
            - 2: Print additional runtime and kernel calls
            - 3: Print additional start/stop, condensed parameter information.
            - 4: Print additional full parameter information


    Returns
    -------
    PJ0, PJ1 : array
        Wavenumber-domain EM responses:
            - PJ0: Wavenumber-domain solution for the kernel with a Bessel
              function of the first kind of order zero.
            - PJ1: Wavenumber-domain solution for the kernel with a Bessel
              function of the first kind of order one.


    Examples
    --------
    >>> import numpy as np
    >>> from empymod.model import dipole_k
    >>> src = [0, 0, 100]
    >>> rec = [5000, 0, 200]
    >>> depth = [0, 300, 1000, 1050]
    >>> res = [1e20, .3, 1, 50, 1]
    >>> freq = 1
    >>> wavenrs = np.logspace(-3.7, -3.6, 10)
    >>> PJ0, PJ1 = dipole_k(src, rec, depth, res, freq, wavenrs, verb=0)
    >>> print(PJ0)
    [ -1.02638329e-08 +4.91531529e-09j  -1.05289724e-08 +5.04222413e-09j
      -1.08009148e-08 +5.17238608e-09j  -1.10798310e-08 +5.30588284e-09j
      -1.13658957e-08 +5.44279805e-09j  -1.16592877e-08 +5.58321732e-09j
      -1.19601897e-08 +5.72722830e-09j  -1.22687889e-08 +5.87492067e-09j
      -1.25852765e-08 +6.02638626e-09j  -1.29098481e-08 +6.18171904e-09j]
    >>> print(PJ1)
    [  1.79483705e-10 -6.59235332e-10j   1.88672497e-10 -6.93749344e-10j
       1.98325814e-10 -7.30068377e-10j   2.08466693e-10 -7.68286748e-10j
       2.19119282e-10 -8.08503709e-10j   2.30308887e-10 -8.50823701e-10j
       2.42062030e-10 -8.95356636e-10j   2.54406501e-10 -9.42218177e-10j
       2.67371420e-10 -9.91530051e-10j   2.80987292e-10 -1.04342036e-09j]"
"def python(source):
    r""""""
    >>> python('def add(a, b): return a + b').add(40, 2)
    42
    """"""
    obj = type('', (object,), {})()
    _exec(source, obj.__dict__, obj.__dict__)
    return obj","r""""""
    >>> python('def add(a, b): return a + b').add(40, 2)
    42"
"def tableType( self ):
        """"""
        Returns the table type for this instance.
        
        :return     <subclass of orb.Table> || None
        """"""
        if not self._tableType:
            if self._tableTypeName:
                self._tableType = Orb.instance().model(nstr(self._tableTypeName))
            
        return self._tableType","Returns the table type for this instance.
        
        :return     <subclass of orb.Table> || None"
"def get_declared_items(self):
        """""" Override to do it manually
        
        """"""
        for k, v in super(AndroidListView, self).get_declared_items():
            if k == 'layout':
                yield k, v
                break",Override to do it manually
"def filter(select, iterable, namespaces=None, flags=0, **kwargs):  # noqa: A001
    """"""Filter list of nodes.""""""

    return compile(select, namespaces, flags, **kwargs).filter(iterable)",Filter list of nodes.
"def add_comment(self, project_id, data):
        """"""
        Adds comments to existing terms.
        >>> data = [
                {
                    ""term"": ""Add new list"",
                    ""context"": """",
                    ""comment"": ""This is a button""
                },
                {
                    ""term"": ""one project found"",
                    ""context"": """",
                    ""comment"": ""Make sure you translate the plural forms""
                },
                {
                    ""term"": ""Show all projects"",
                    ""context"": """",
                    ""comment"": ""This is a button""
                }
            ]
        """"""
        data = self._run(
            url_path=""terms/add_comment"",
            id=project_id,
            data=json.dumps(data)
        )
        return data['result']['terms']","Adds comments to existing terms.
        >>> data = [
                {
                    ""term"": ""Add new list"",
                    ""context"": """",
                    ""comment"": ""This is a button""
                },
                {
                    ""term"": ""one project found"",
                    ""context"": """",
                    ""comment"": ""Make sure you translate the plural forms""
                },
                {
                    ""term"": ""Show all projects"",
                    ""context"": """",
                    ""comment"": ""This is a button""
                }
            ]"
"def start(self,
              join_ring=True,
              no_wait=False,
              verbose=False,
              update_pid=True,
              wait_other_notice=True,
              replace_token=None,
              replace_address=None,
              jvm_args=None,
              wait_for_binary_proto=False,
              profile_options=None,
              use_jna=False,
              quiet_start=False,
              allow_root=False,
              set_migration_task=True):
        """"""
        Start the node. Options includes:
          - join_ring: if false, start the node with -Dcassandra.join_ring=False
          - no_wait: by default, this method returns when the node is started and listening to clients.
            If no_wait=True, the method returns sooner.
          - wait_other_notice: if truthy, this method returns only when all other live node of the cluster
            have marked this node UP. if an integer, sets the timeout for how long to wait
          - replace_token: start the node with the -Dcassandra.replace_token option.
          - replace_address: start the node with the -Dcassandra.replace_address option.
        """"""
        if jvm_args is None:
            jvm_args = []

        if set_migration_task and self.cluster.cassandra_version() >= '3.0.1':
            jvm_args += ['-Dcassandra.migration_task_wait_in_seconds={}'.format(len(self.cluster.nodes) * 2)]

        # Validate Windows env
        if common.is_modern_windows_install(self.cluster.version()) and not common.is_ps_unrestricted():
            raise NodeError(""PS Execution Policy must be unrestricted when running C* 2.1+"")

        if not common.is_win() and quiet_start:
            common.warning(""Tried to set Windows quiet start behavior, but we're not running on Windows."")

        if self.is_running():
            raise NodeError(""{} is already running"".format(self.name))

        for itf in list(self.network_interfaces.values()):
            if itf is not None and replace_address is None:
                common.assert_socket_available(itf)

        if wait_other_notice:
            marks = [(node, node.mark_log()) for node in list(self.cluster.nodes.values()) if node.is_live()]

        self.mark = self.mark_log()

        launch_bin = self.get_launch_bin()

        # If Windows, change entries in .bat file to split conf from binaries
        if common.is_win():
            self.__clean_bat()

        if profile_options is not None:
            config = common.get_config()
            if 'yourkit_agent' not in config:
                raise NodeError(""Cannot enable profile. You need to set 'yourkit_agent' to the path of your agent in a ~/.ccm/config"")
            cmd = '-agentpath:{}'.format(config['yourkit_agent'])
            if 'options' in profile_options:
                cmd = cmd + '=' + profile_options['options']
            print_(cmd)
            # Yes, it's fragile as shit
            pattern = r'cassandra_parms=""-Dlog4j.configuration=log4j-server.properties -Dlog4j.defaultInitOverride=true'
            common.replace_in_file(launch_bin, pattern, '    ' + pattern + ' ' + cmd + '""')

        os.chmod(launch_bin, os.stat(launch_bin).st_mode | stat.S_IEXEC)

        env = self.get_env()

        extension.append_to_server_env(self, env)

        if common.is_win():
            self._clean_win_jmx()

        pidfile = os.path.join(self.get_path(), 'cassandra.pid')
        args = [launch_bin]

        self.add_custom_launch_arguments(args)

        args = args + ['-p', pidfile, '-Dcassandra.join_ring=%s' % str(join_ring)]

        args.append('-Dcassandra.logdir=%s' % os.path.join(self.get_path(), 'logs'))
        if replace_token is not None:
            args.append('-Dcassandra.replace_token=%s' % str(replace_token))
        if replace_address is not None:
            args.append('-Dcassandra.replace_address=%s' % str(replace_address))
        if use_jna is False:
            args.append('-Dcassandra.boot_without_jna=true')
        if allow_root:
            args.append('-R')
        env['JVM_EXTRA_OPTS'] = env.get('JVM_EXTRA_OPTS', """") + "" "" + "" "".join(jvm_args)

        # In case we are restarting a node
        # we risk reading the old cassandra.pid file
        self._delete_old_pid()

        process = None
        FNULL = open(os.devnull, 'w')
        stdout_sink = subprocess.PIPE if verbose else FNULL
        # write stderr to a temporary file to prevent overwhelming pipe (> 65K data).
        stderr_sink = tempfile.SpooledTemporaryFile(max_size=0xFFFF)

        if common.is_win():
            # clean up any old dirty_pid files from prior runs
            if (os.path.isfile(self.get_path() + ""/dirty_pid.tmp"")):
                os.remove(self.get_path() + ""/dirty_pid.tmp"")

            if quiet_start and self.cluster.version() >= '2.2.4':
                args.append('-q')

            process = subprocess.Popen(args, cwd=self.get_bin_dir(), env=env, stdout=stdout_sink, stderr=stderr_sink)
        else:
            process = subprocess.Popen(args, env=env, stdout=stdout_sink, stderr=stderr_sink)

        process.stderr_file = stderr_sink

        # Our modified batch file writes a dirty output with more than just the pid - clean it to get in parity
        # with *nix operation here.
        if verbose:
            stdout = process.communicate()[0]
            print_(stdout)

        if common.is_win():
            self.__clean_win_pid()
            self._update_pid(process)
            print_(""Started: {0} with pid: {1}"".format(self.name, self.pid), file=sys.stderr, flush=True)
        elif update_pid:
            self._update_pid(process)

            if not self.is_running():
                raise NodeError(""Error starting node %s"" % self.name, process)

        # If wait_other_notice is a bool, we don't want to treat it as a
        # timeout. Other intlike types, though, we want to use.
        if common.is_intlike(wait_other_notice) and not isinstance(wait_other_notice, bool):
            for node, mark in marks:
                node.watch_log_for_alive(self, from_mark=mark, timeout=wait_other_notice)
        elif wait_other_notice:
            for node, mark in marks:
                node.watch_log_for_alive(self, from_mark=mark)

        # If wait_for_binary_proto is a bool, we don't want to treat it as a
        # timeout. Other intlike types, though, we want to use.
        if common.is_intlike(wait_for_binary_proto) and not isinstance(wait_for_binary_proto, bool):
            self.wait_for_binary_interface(from_mark=self.mark, timeout=wait_for_binary_proto)
        elif wait_for_binary_proto:
            self.wait_for_binary_interface(from_mark=self.mark)

        return process","Start the node. Options includes:
          - join_ring: if false, start the node with -Dcassandra.join_ring=False
          - no_wait: by default, this method returns when the node is started and listening to clients.
            If no_wait=True, the method returns sooner.
          - wait_other_notice: if truthy, this method returns only when all other live node of the cluster
            have marked this node UP. if an integer, sets the timeout for how long to wait
          - replace_token: start the node with the -Dcassandra.replace_token option.
          - replace_address: start the node with the -Dcassandra.replace_address option."
"def search(cls, term, weights=None, with_score=False, score_alias='score',
               explicit_ordering=False):
        """"""Full-text search using selected `term`.""""""
        return cls._search(
            term,
            weights,
            with_score,
            score_alias,
            cls.rank,
            explicit_ordering)",Full-text search using selected `term`.
"def initialize_plot(self, ranges=None, plots=[]):
        """"""
        Plot all the views contained in the AdjointLayout Object using axes
        appropriate to the layout configuration. All the axes are
        supplied by LayoutPlot - the purpose of the call is to
        invoke subplots with correct options and styles and hide any
        empty axes as necessary.
        """"""
        if plots is None: plots = []
        adjoined_plots = []
        for pos in self.view_positions:
            # Pos will be one of 'main', 'top' or 'right' or None
            subplot = self.subplots.get(pos, None)
            # If no view object or empty position, disable the axis
            if subplot is None:
                adjoined_plots.append(empty_plot(0, 0))
            else:
                passed_plots = plots + adjoined_plots
                adjoined_plots.append(subplot.initialize_plot(ranges=ranges, plots=passed_plots))
        self.drawn = True
        if not adjoined_plots: adjoined_plots = [None]
        return adjoined_plots","Plot all the views contained in the AdjointLayout Object using axes
        appropriate to the layout configuration. All the axes are
        supplied by LayoutPlot - the purpose of the call is to
        invoke subplots with correct options and styles and hide any
        empty axes as necessary."
"def _find_usage_networking_sgs(self):
        """"""calculate usage for VPC-related things""""""
        logger.debug(""Getting usage for EC2 VPC resources"")
        sgs_per_vpc = defaultdict(int)
        rules_per_sg = defaultdict(int)
        for sg in self.resource_conn.security_groups.all():
            if sg.vpc_id is not None:
                sgs_per_vpc[sg.vpc_id] += 1
                rules_per_sg[sg.id] = len(sg.ip_permissions)
        # set usage
        for vpc_id, count in sgs_per_vpc.items():
            self.limits['Security groups per VPC']._add_current_usage(
                count,
                aws_type='AWS::EC2::VPC',
                resource_id=vpc_id,
            )
        for sg_id, count in rules_per_sg.items():
            self.limits['Rules per VPC security group']._add_current_usage(
                count,
                aws_type='AWS::EC2::SecurityGroupRule',
                resource_id=sg_id,
            )",calculate usage for VPC-related things
"def add_file_arg(self, filename):
    """"""
    Add a file argument to the executable. Arguments are appended after any
    options and their order is guaranteed. Also adds the file name to the
    list of required input data for this job.
    @param filename: file to add as argument.
    """"""
    self.__arguments.append(filename)
    if filename not in self.__input_files:
      self.__input_files.append(filename)","Add a file argument to the executable. Arguments are appended after any
    options and their order is guaranteed. Also adds the file name to the
    list of required input data for this job.
    @param filename: file to add as argument."
"def update_delivery_note(self, delivery_note_id, delivery_note_dict):
        """"""
        Updates a delivery note

        :param delivery_note_id: the delivery note id
        :param delivery_note_dict: dict
        :return: dict
        """"""
        return self._create_put_request(
            resource=DELIVERY_NOTES,
            billomat_id=delivery_note_id,
            send_data=delivery_note_dict
        )","Updates a delivery note

        :param delivery_note_id: the delivery note id
        :param delivery_note_dict: dict
        :return: dict"
"def get_go_server(settings=None):
    """"""Returns a `gocd.Server` configured by the `settings`
    object.

    Args:
      settings: a `gocd_cli.settings.Settings` object.
        Default: if falsey calls `get_settings`.

    Returns:
      gocd.Server: a configured gocd.Server instance
    """"""
    if not settings:
        settings = get_settings()

    return gocd.Server(
        settings.get('server'),
        user=settings.get('user'),
        password=settings.get('password'),
    )","Returns a `gocd.Server` configured by the `settings`
    object.

    Args:
      settings: a `gocd_cli.settings.Settings` object.
        Default: if falsey calls `get_settings`.

    Returns:
      gocd.Server: a configured gocd.Server instance"
"def _set_fcoe_fsb(self, v, load=False):
    """"""
    Setter method for fcoe_fsb, mapped from YANG variable /fcoe_fsb (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_fcoe_fsb is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_fcoe_fsb() directly.

    YANG Description: This CLI will disable/enable fsb mode
    """"""
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=fcoe_fsb.fcoe_fsb, is_container='container', presence=False, yang_name=""fcoe-fsb"", rest_name=""fsb"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'callpoint': u'fcoe_fsb_cp', u'info': u'Enable/Disable the fsb mode', u'hidden': u'debug', u'alt-name': u'fsb', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-fcoe', defining_module='brocade-fcoe', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': """"""fcoe_fsb must be of a type compatible with container"""""",
          'defined-type': ""container"",
          'generated-type': """"""YANGDynClass(base=fcoe_fsb.fcoe_fsb, is_container='container', presence=False, yang_name=""fcoe-fsb"", rest_name=""fsb"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'callpoint': u'fcoe_fsb_cp', u'info': u'Enable/Disable the fsb mode', u'hidden': u'debug', u'alt-name': u'fsb', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-fcoe', defining_module='brocade-fcoe', yang_type='container', is_config=True)"""""",
        })

    self.__fcoe_fsb = t
    if hasattr(self, '_set'):
      self._set()","Setter method for fcoe_fsb, mapped from YANG variable /fcoe_fsb (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_fcoe_fsb is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_fcoe_fsb() directly.

    YANG Description: This CLI will disable/enable fsb mode"
"def from_table(fileobj=None, url='http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/knownGene.txt.gz',
                    parser=UCSCTable.KNOWN_GENE, mode='tx', decompress=None):
        '''
        UCSC Genome project provides several tables with gene coordinates (https://genome.ucsc.edu/cgi-bin/hgTables),
        such as knownGene, refGene, ensGene, wgEncodeGencodeBasicV19, etc.
        Indexing the rows of those tables into a ``GenomeIntervalTree`` is a common task, implemented in this method.

        The table can be either specified as a ``fileobj`` (in which case the data is read line by line),
        or via an ``url`` (the ``url`` may be to a ``txt`` or ``txt.gz`` file either online or locally).
        The type of the table is specified using the ``parser`` parameter. This is a function that takes a line
        of the file (with no line ending) and returns a dictionary, mapping field names to values. This dictionary will be assigned
        to the ``data`` field of each interval in the resulting tree.

        Finally, there are different ways genes can be mapped into intervals for the sake of indexing as an interval tree.
        One way is to represent each gene via its transcribed region (``txStart``..``txEnd``). Another is to represent using
        coding region (``cdsStart``..``cdsEnd``). Finally, the third possibility is to map each gene into several intervals,
        corresponding to its exons (``exonStarts``..``exonEnds``).

        The mode, in which genes are mapped to intervals is specified via the ``mode`` parameter. The value can be ``tx``, ``cds`` and
        ``exons``, corresponding to the three mentioned possibilities.

        If a more specific way of interval-mapping is required (e.g. you might want to create 'coding-region+-10k' intervals), you can provide
        an ""interval-maker"" function as the ``mode`` parameter. An interval-maker function takes as input a dictionary, returned by the parser,
        and returns an iterable of Interval objects.

        The ``parser`` function must ensure that its output contains the field named ``chrom``, and also fields named ``txStart``/``txEnd`` if ``mode=='tx'``,
        fields ``cdsStart``/``cdsEnd`` if ``mode=='cds'``, and fields ``exonCount``/``exonStarts``/``exonEnds`` if ``mode=='exons'``.

        The ``decompress`` parameter specifies whether the provided file is gzip-compressed.
        This only applies to the situation when the url is given (no decompression is made if fileobj is provided in any case).
        If decompress is None, data is decompressed if the url ends with .gz, otherwise decompress = True forces decompression.

        >> knownGene = GenomeIntervalTree.from_table()
        >> len(knownGene)
        82960
        >> result = knownGene[b'chr1'].search(100000, 138529)
        >> len(result)
        1
        >> list(result)[0].data['name']
        b'uc021oeg.2'
        '''
        if fileobj is None:
            data = urlopen(url).read()
            if (decompress is None and url.endswith('.gz')) or decompress:
                data = zlib.decompress(data, 16+zlib.MAX_WBITS)
            fileobj = BytesIO(data)

        interval_lists = defaultdict(list)

        if mode == 'tx':
            interval_maker = IntervalMakers.TX
        elif mode == 'cds':
            interval_maker = IntervalMakers.CDS
        elif mode == 'exons':
            interval_maker = IntervalMakers.EXONS
        elif getattr(mode, __call__, None) is None:
            raise Exception(""Parameter `mode` may only be 'tx', 'cds', 'exons' or a callable"")
        else:
            interval_maker = mode

        for ln in fileobj:
            if not isinstance(ln, bytes):
                ln = ln.encode()
            ln = ln.strip()
            d = parser(ln)
            for interval in interval_maker(d):
                interval_lists[d['chrom']].append(_fix(interval))

        # Now convert interval lists into trees
        gtree = GenomeIntervalTree()
        for chrom, lst in getattr(interval_lists, 'iteritems', interval_lists.items)():
            gtree[chrom] = IntervalTree(lst)
        return gtree","UCSC Genome project provides several tables with gene coordinates (https://genome.ucsc.edu/cgi-bin/hgTables),
        such as knownGene, refGene, ensGene, wgEncodeGencodeBasicV19, etc.
        Indexing the rows of those tables into a ``GenomeIntervalTree`` is a common task, implemented in this method.

        The table can be either specified as a ``fileobj`` (in which case the data is read line by line),
        or via an ``url`` (the ``url`` may be to a ``txt`` or ``txt.gz`` file either online or locally).
        The type of the table is specified using the ``parser`` parameter. This is a function that takes a line
        of the file (with no line ending) and returns a dictionary, mapping field names to values. This dictionary will be assigned
        to the ``data`` field of each interval in the resulting tree.

        Finally, there are different ways genes can be mapped into intervals for the sake of indexing as an interval tree.
        One way is to represent each gene via its transcribed region (``txStart``..``txEnd``). Another is to represent using
        coding region (``cdsStart``..``cdsEnd``). Finally, the third possibility is to map each gene into several intervals,
        corresponding to its exons (``exonStarts``..``exonEnds``).

        The mode, in which genes are mapped to intervals is specified via the ``mode`` parameter. The value can be ``tx``, ``cds`` and
        ``exons``, corresponding to the three mentioned possibilities.

        If a more specific way of interval-mapping is required (e.g. you might want to create 'coding-region+-10k' intervals), you can provide
        an ""interval-maker"" function as the ``mode`` parameter. An interval-maker function takes as input a dictionary, returned by the parser,
        and returns an iterable of Interval objects.

        The ``parser`` function must ensure that its output contains the field named ``chrom``, and also fields named ``txStart``/``txEnd`` if ``mode=='tx'``,
        fields ``cdsStart``/``cdsEnd`` if ``mode=='cds'``, and fields ``exonCount``/``exonStarts``/``exonEnds`` if ``mode=='exons'``.

        The ``decompress`` parameter specifies whether the provided file is gzip-compressed.
        This only applies to the situation when the url is given (no decompression is made if fileobj is provided in any case).
        If decompress is None, data is decompressed if the url ends with .gz, otherwise decompress = True forces decompression.

        >> knownGene = GenomeIntervalTree.from_table()
        >> len(knownGene)
        82960
        >> result = knownGene[b'chr1'].search(100000, 138529)
        >> len(result)
        1
        >> list(result)[0].data['name']
        b'uc021oeg.2'"
"async def delete(self, *names):
        """"""
        ""Delete one or more keys specified by ``names``""

        Cluster impl:
            Iterate all keys and send DELETE for each key.
            This will go a lot slower than a normal delete call in StrictRedis.

            Operation is no longer atomic.
        """"""
        count = 0

        for arg in names:
            count += await self.execute_command('DEL', arg)

        return count","""Delete one or more keys specified by ``names``""

        Cluster impl:
            Iterate all keys and send DELETE for each key.
            This will go a lot slower than a normal delete call in StrictRedis.

            Operation is no longer atomic."
"def process_reference_line(working_line,
                           journals_matches,
                           pprint_repnum_len,
                           pprint_repnum_matchtext,
                           publishers_matches,
                           removed_spaces,
                           standardised_titles,
                           kbs):
    """"""After the phase of identifying and tagging citation instances
       in a reference line, this function is called to go through the
       line and the collected information about the recognised citations,
       and to transform the line into a string of MARC XML in which the
       recognised citations are grouped under various datafields and
       subfields, depending upon their type.
       @param line_marker: (string) - this is the marker for this
        reference line (e.g. [1]).
       @param working_line: (string) - this is the line before the
        punctuation was stripped. At this stage, it has not been
        capitalised, and neither TITLES nor REPORT NUMBERS have been
        stripped from it. However, any recognised numeration and/or URLs
        have been tagged with <cds.YYYY> tags.
        The working_line could, for example, look something like this:
         [1] CDS <cds.URL description=""http //invenio-software.org/"">
         http //invenio-software.org/</cds.URL>.
       @param found_title_len: (dictionary) - the lengths of the title
        citations that have been recognised in the line. Keyed by the index
        within the line of each match.
       @param found_title_matchtext: (dictionary) - The text that was found
        for each matched title citation in the line. Keyed by the index within
        the line of each match.
       @param pprint_repnum_len: (dictionary) - the lengths of the matched
        institutional preprint report number citations found within the line.
        Keyed by the index within the line of each match.
       @param pprint_repnum_matchtext: (dictionary) - The matched text for each
        matched institutional report number. Keyed by the index within the line
        of each match.
       @param identified_dois (list) - The list of dois inside the citation
       @identified_urls: (list) - contains 2-cell tuples, each of which
        represents an idenitfied URL and its description string.
        The list takes the order in which the URLs were identified in the line
        (i.e. first-found, second-found, etc).
       @param removed_spaces: (dictionary) - The number of spaces removed from
        the various positions in the line. Keyed by the index of the position
        within the line at which the spaces were removed.
       @param standardised_titles: (dictionary) - The standardised journal
        titles, keyed by the non-standard version of those titles.
       @return: (tuple) of 5 components:
                  ( string  -> a MARC XML-ized reference line.
                    integer -> number of fields of miscellaneous text marked-up
                               for the line.
                    integer -> number of title citations marked-up for the line.
                    integer -> number of institutional report-number citations
                               marked-up for the line.
                    integer -> number of URL citations marked-up for the record.
                    integer -> number of DOI's found for the record
                    integer -> number of author groups found
                  )

    """"""
    if len(journals_matches) + len(pprint_repnum_len) + len(publishers_matches) == 0:
        # no TITLE or REPORT-NUMBER citations were found within this line,
        # use the raw line: (This 'raw' line could still be tagged with
        # recognised URLs or numeration.)
        tagged_line = working_line
    else:
        # TITLE and/or REPORT-NUMBER citations were found in this line,
        # build a new version of the working-line in which the standard
        # versions of the REPORT-NUMBERs and TITLEs are tagged:
        startpos = 0          # First cell of the reference line...
        previous_match = {}   # previously matched TITLE within line (used
        # for replacement of IBIDs.
        replacement_types = {}
        journals_keys = journals_matches.keys()
        journals_keys.sort()
        reports_keys = pprint_repnum_matchtext.keys()
        reports_keys.sort()
        publishers_keys = publishers_matches.keys()
        publishers_keys.sort()
        spaces_keys = removed_spaces.keys()
        spaces_keys.sort()
        replacement_types = get_replacement_types(journals_keys,
                                                  reports_keys,
                                                  publishers_keys)
        replacement_locations = replacement_types.keys()
        replacement_locations.sort()

        tagged_line = u""""  # This is to be the new 'working-line'. It will
        # contain the tagged TITLEs and REPORT-NUMBERs,
        # as well as any previously tagged URLs and
        # numeration components.
        # begin:
        for replacement_index in replacement_locations:
            # first, factor in any stripped spaces before this 'replacement'
            true_replacement_index, extras = \
                account_for_stripped_whitespace(spaces_keys,
                                                removed_spaces,
                                                replacement_types,
                                                pprint_repnum_len,
                                                journals_matches,
                                                replacement_index)

            if replacement_types[replacement_index] == u""journal"":
                # Add a tagged periodical TITLE into the line:
                rebuilt_chunk, startpos, previous_match = \
                    add_tagged_journal(
                        reading_line=working_line,
                        journal_info=journals_matches[replacement_index],
                        previous_match=previous_match,
                        startpos=startpos,
                        true_replacement_index=true_replacement_index,
                        extras=extras,
                        standardised_titles=standardised_titles)
                tagged_line += rebuilt_chunk

            elif replacement_types[replacement_index] == u""reportnumber"":
                # Add a tagged institutional preprint REPORT-NUMBER
                # into the line:
                rebuilt_chunk, startpos = \
                    add_tagged_report_number(
                        reading_line=working_line,
                        len_reportnum=pprint_repnum_len[replacement_index],
                        reportnum=pprint_repnum_matchtext[replacement_index],
                        startpos=startpos,
                        true_replacement_index=true_replacement_index,
                        extras=extras
                    )
                tagged_line += rebuilt_chunk

            elif replacement_types[replacement_index] == u""publisher"":
                rebuilt_chunk, startpos = \
                    add_tagged_publisher(
                        reading_line=working_line,
                        matched_publisher=publishers_matches[
                            replacement_index],
                        startpos=startpos,
                        true_replacement_index=true_replacement_index,
                        extras=extras,
                        kb_publishers=kbs['publishers']
                    )
                tagged_line += rebuilt_chunk

        # add the remainder of the original working-line into the rebuilt line:
        tagged_line += working_line[startpos:]

        # we have all the numeration
        # we can make sure there's no space between the volume
        # letter and the volume number
        # e.g. B 20 -> B20
        tagged_line = wash_volume_tag(tagged_line)

    # Try to find any authors in the line
    tagged_line = identify_and_tag_authors(tagged_line, kbs['authors'])
    # Try to find any collaboration in the line
    tagged_line = identify_and_tag_collaborations(tagged_line,
                                                  kbs['collaborations'])

    return tagged_line.replace('\n', '')","After the phase of identifying and tagging citation instances
       in a reference line, this function is called to go through the
       line and the collected information about the recognised citations,
       and to transform the line into a string of MARC XML in which the
       recognised citations are grouped under various datafields and
       subfields, depending upon their type.
       @param line_marker: (string) - this is the marker for this
        reference line (e.g. [1]).
       @param working_line: (string) - this is the line before the
        punctuation was stripped. At this stage, it has not been
        capitalised, and neither TITLES nor REPORT NUMBERS have been
        stripped from it. However, any recognised numeration and/or URLs
        have been tagged with <cds.YYYY> tags.
        The working_line could, for example, look something like this:
         [1] CDS <cds.URL description=""http //invenio-software.org/"">
         http //invenio-software.org/</cds.URL>.
       @param found_title_len: (dictionary) - the lengths of the title
        citations that have been recognised in the line. Keyed by the index
        within the line of each match.
       @param found_title_matchtext: (dictionary) - The text that was found
        for each matched title citation in the line. Keyed by the index within
        the line of each match.
       @param pprint_repnum_len: (dictionary) - the lengths of the matched
        institutional preprint report number citations found within the line.
        Keyed by the index within the line of each match.
       @param pprint_repnum_matchtext: (dictionary) - The matched text for each
        matched institutional report number. Keyed by the index within the line
        of each match.
       @param identified_dois (list) - The list of dois inside the citation
       @identified_urls: (list) - contains 2-cell tuples, each of which
        represents an idenitfied URL and its description string.
        The list takes the order in which the URLs were identified in the line
        (i.e. first-found, second-found, etc).
       @param removed_spaces: (dictionary) - The number of spaces removed from
        the various positions in the line. Keyed by the index of the position
        within the line at which the spaces were removed.
       @param standardised_titles: (dictionary) - The standardised journal
        titles, keyed by the non-standard version of those titles.
       @return: (tuple) of 5 components:
                  ( string  -> a MARC XML-ized reference line.
                    integer -> number of fields of miscellaneous text marked-up
                               for the line.
                    integer -> number of title citations marked-up for the line.
                    integer -> number of institutional report-number citations
                               marked-up for the line.
                    integer -> number of URL citations marked-up for the record.
                    integer -> number of DOI's found for the record
                    integer -> number of author groups found
                  )"
"def get_ntlm_response(self, flags, challenge, target_info=None, channel_binding=None):
        """"""
        Computes the 24 byte NTLM challenge response given the 8 byte server challenge, along with the session key.
        If NTLMv2 is used, the TargetInfo structure must be supplied, the updated TargetInfo structure will be returned
        :param challenge: The 8-byte challenge message generated by the server
        :return: A tuple containing the 24 byte NTLM Hash, Session Key and TargetInfo
        """"""
        # TODO: IMPLEMENT THE FOLLOWING FEATURES
        # If NTLM v2 authentication is used and the CHALLENGE_MESSAGE does not contain both MsvAvNbComputerName and
        # MsvAvNbDomainName AVPairs and either Integrity is TRUE or Confidentiality is TRUE, then return STATUS_LOGON_FAILURE.

        # If NTLM v2 authentication is used and the CHALLENGE_MESSAGE contains a TargetInfo field, the client SHOULD NOT send
        # the LmChallengeResponse and SHOULD set the LmChallengeResponseLen and LmChallengeResponseMaxLen fields in the
        # AUTHENTICATE_MESSAGE to zero.

        # If lm compatibility level is 3 or lower, but the server negotiated NTLM2, generate an
        # NTLM2 response in preference to the weaker NTLMv1.
        if flags & NegotiateFlag.NTLMSSP_NTLM2_KEY and self._lm_compatibility < 3:
            response, key = PasswordAuthentication.get_ntlm2_response(self._password, challenge, self._client_challenge)

        elif 0 <= self._lm_compatibility < 3:
            response, key = PasswordAuthentication.get_ntlmv1_response(self._password, challenge)
        else:
            # We should use the timestamp included in TargetInfo, if no timestamp is set we generate one and add it to
            # the outgoing TargetInfo. If the timestamp is set, we should also set the MIC flag
            if target_info is None:
                target_info = TargetInfo()
            if target_info[TargetInfo.NTLMSSP_AV_TIME] is None:
                timestamp = PasswordAuthentication._get_ntlm_timestamp()
            else:
                # TODO: If the CHALLENGE_MESSAGE TargetInfo field (section 2.2.1.2) has an MsvAvTimestamp present,
                # TODO: the client SHOULD provide a MIC.
                timestamp = target_info[TargetInfo.NTLMSSP_AV_TIME][1]

            #target_info[TargetInfo.NTLMSSP_AV_FLAGS] = struct.pack('<I', 2)
            # Calculating channel bindings is poorly documented. It is implemented in winrmlib, and needs to be
            # moved here
            # if self._av_channel_bindings is True and channel_binding is not None:
            #     target_info[TargetInfo.NTLMSSP_AV_CHANNEL_BINDINGS] = channel_binding

            response, key, target_info = PasswordAuthentication.get_ntlmv2_response(
                self._domain, self._username, self._password.encode('utf-16le'), challenge,
                self._client_challenge, timestamp, target_info)

        return response, key, target_info","Computes the 24 byte NTLM challenge response given the 8 byte server challenge, along with the session key.
        If NTLMv2 is used, the TargetInfo structure must be supplied, the updated TargetInfo structure will be returned
        :param challenge: The 8-byte challenge message generated by the server
        :return: A tuple containing the 24 byte NTLM Hash, Session Key and TargetInfo"
"def predict(self, temp_type):
        """"""
        Transpile the predict method.

        Parameters
        ----------
        :param temp_type : string
            The kind of export type (embedded, separated, exported).

        Returns
        -------
        :return : string
            The transpiled predict method as string.
        """"""
        # Exported:
        if temp_type == 'exported':
            temp = self.temp('exported.class')
            return temp.format(class_name=self.class_name,
                               method_name=self.method_name,
                               n_features=self.n_features)
        # Embedded:
        if temp_type == 'embedded':
            method = self.create_method_embedded()
            return self.create_class_embedded(method)","Transpile the predict method.

        Parameters
        ----------
        :param temp_type : string
            The kind of export type (embedded, separated, exported).

        Returns
        -------
        :return : string
            The transpiled predict method as string."
"def remote_command(task: Task, command: str) -> Result:
    """"""
    Executes a command remotely on the host

    Arguments:
        command (``str``): command to execute

    Returns:
        Result object with the following attributes set:
          * result (``str``): stderr or stdout
          * stdout (``str``): stdout
          * stderr (``str``): stderr

    Raises:
        :obj:`nornir.core.exceptions.CommandError`: when there is a command error
    """"""
    client = task.host.get_connection(""paramiko"", task.nornir.config)
    connection_state = task.host.get_connection_state(""paramiko"")

    chan = client.get_transport().open_session()

    if connection_state[""ssh_forward_agent""]:
        AgentRequestHandler(chan)

    chan.exec_command(command)

    with chan.makefile() as f:
        stdout = f.read().decode()
    with chan.makefile_stderr() as f:
        stderr = f.read().decode()

    exit_status_code = chan.recv_exit_status()

    if exit_status_code:
        raise CommandError(command, exit_status_code, stdout, stderr)

    result = stderr if stderr else stdout
    return Result(result=result, host=task.host, stderr=stderr, stdout=stdout)","Executes a command remotely on the host

    Arguments:
        command (``str``): command to execute

    Returns:
        Result object with the following attributes set:
          * result (``str``): stderr or stdout
          * stdout (``str``): stdout
          * stderr (``str``): stderr

    Raises:
        :obj:`nornir.core.exceptions.CommandError`: when there is a command error"
"def get_session(session, baseurl, config):
    """"""
    Try to get a valid session for this baseurl, using login found in config.
    This function invoques Firefox if necessary
    """"""
    # Read proxy for firefox
    if environ.get(""HTTP_PROXY""):
        myProxy = environ.get(""HTTP_PROXY"")
        proxy = Proxy({
            'proxyType': ProxyType.MANUAL,
            'httpProxy': myProxy,
            'ftpProxy': myProxy,
            'sslProxy': myProxy,
            'noProxy': ''  # set this value as desired
        })
    else:
        proxy = None

    if 'login' in config['DEFAULT']:
        login, password = credentials(config['DEFAULT']['login'])
    else:
        login, password = credentials()

    browser = webdriver.Firefox(proxy=proxy)
    browser.get(baseurl)
    browser.find_element_by_name('login').send_keys(login)
    browser.find_element_by_name('passwd').send_keys(password)

    cookie = {'PHPSESSID': browser.get_cookie('PHPSESSID')['value']}
    prof_session.cookies = requests.utils.cookiejar_from_dict(cookie)
    print(""Please log using firefox"")
    while True:
        try:
            browser.find_element_by_css_selector(""select"")
            break
        except:
            sleep(0.5)
    browser.close()
    set_sessid(cookie['PHPSESSID'])
    if not verify_session(session, baseurl):
        print(""Cannot get a valid session, retry"")
        get_session(session, baseurl, {'DEFAULT': {}})","Try to get a valid session for this baseurl, using login found in config.
    This function invoques Firefox if necessary"
"def ppj(json_data):
    """"""ppj

    :param json_data: dictionary to print
    """"""
    return str(json.dumps(
                json_data,
                sort_keys=True,
                indent=4,
                separators=(',', ': ')))","ppj

    :param json_data: dictionary to print"
"def register_element(self, model, idx):
        """"""
        Register element with index ``idx`` to ``model``

        :param model: model name
        :param idx: element idx
        :return: final element idx
        """"""

        if idx is None:
            idx = model + '_' + str(len(self._idx_model))

        self._idx_model[idx] = model
        self._idx.append(idx)

        return idx","Register element with index ``idx`` to ``model``

        :param model: model name
        :param idx: element idx
        :return: final element idx"
"def repeat_last_axis(array, count):
    """"""
    Restride `array` to repeat `count` times along the last axis.

    Parameters
    ----------
    array : np.array
        The array to restride.
    count : int
        Number of times to repeat `array`.

    Returns
    -------
    result : array
        Array of shape array.shape + (count,) composed of `array` repeated
        `count` times along the last axis.

    Example
    -------
    >>> from numpy import arange
    >>> a = arange(3); a
    array([0, 1, 2])
    >>> repeat_last_axis(a, 2)
    array([[0, 0],
           [1, 1],
           [2, 2]])
    >>> repeat_last_axis(a, 4)
    array([[0, 0, 0, 0],
           [1, 1, 1, 1],
           [2, 2, 2, 2]])

    Notes
    ----
    The resulting array will share memory with `array`.  If you need to assign
    to the input or output, you should probably make a copy first.

    See Also
    --------
    repeat_last_axis
    """"""
    return as_strided(array, array.shape + (count,), array.strides + (0,))","Restride `array` to repeat `count` times along the last axis.

    Parameters
    ----------
    array : np.array
        The array to restride.
    count : int
        Number of times to repeat `array`.

    Returns
    -------
    result : array
        Array of shape array.shape + (count,) composed of `array` repeated
        `count` times along the last axis.

    Example
    -------
    >>> from numpy import arange
    >>> a = arange(3); a
    array([0, 1, 2])
    >>> repeat_last_axis(a, 2)
    array([[0, 0],
           [1, 1],
           [2, 2]])
    >>> repeat_last_axis(a, 4)
    array([[0, 0, 0, 0],
           [1, 1, 1, 1],
           [2, 2, 2, 2]])

    Notes
    ----
    The resulting array will share memory with `array`.  If you need to assign
    to the input or output, you should probably make a copy first.

    See Also
    --------
    repeat_last_axis"
"def _build_path(self, path, metric_type):
        """"""Return a normalized path.

        :param list path: elements of the metric path to record
        :param str metric_type: The metric type
        :rtype: str

        """"""
        path = self._get_prefixes(metric_type) + list(path)
        return '{}.{}'.format(self._namespace,
                              '.'.join(str(p).replace('.', '-') for p in path))","Return a normalized path.

        :param list path: elements of the metric path to record
        :param str metric_type: The metric type
        :rtype: str"
"def _parse_content(self, snv_entries):
        """"""
        Parses SNV entries to SNVItems, objects
        representing the content for every entry, that
        can be used for further processing.
        """"""
        if len(snv_entries) == 1:
            return
        for line in snv_entries[1:]:
            info_dict = self._map_info_to_col(line)
            self._snv_list.append(SNVItem(**info_dict))","Parses SNV entries to SNVItems, objects
        representing the content for every entry, that
        can be used for further processing."
"def find_mode(data):
    """"""\
    Returns the appropriate QR Code mode (an integer constant) for the
    provided `data`.

    :param bytes data: Data to check.
    :rtype: int
    """"""
    if data.isdigit():
        return consts.MODE_NUMERIC
    if is_alphanumeric(data):
        return consts.MODE_ALPHANUMERIC
    if is_kanji(data):
        return consts.MODE_KANJI
    return consts.MODE_BYTE","\
    Returns the appropriate QR Code mode (an integer constant) for the
    provided `data`.

    :param bytes data: Data to check.
    :rtype: int"
"def example_panel(self, ax, feature):
        """"""
        A example panel that just prints the text of the feature.
        """"""
        txt = '%s:%s-%s' % (feature.chrom, feature.start, feature.stop)
        ax.text(0.5, 0.5, txt, transform=ax.transAxes)
        return feature",A example panel that just prints the text of the feature.
"def fit_mle(self,
                init_vals,
                num_draws,
                seed=None,
                constrained_pos=None,
                print_res=True,
                method=""BFGS"",
                loss_tol=1e-06,
                gradient_tol=1e-06,
                maxiter=1000,
                ridge=None,
                just_point=False,
                **kwargs):
        """"""
        Parameters
        ----------
        init_vals : 1D ndarray.
            Should contain the initial values to start the optimization process
            with. There should be one value for each utility coefficient and
            shape parameter being estimated.
        num_draws : int.
            Should be greater than zero. Denotes the number of draws that we
            are making from each normal distribution.
        seed : int or None, optional.
            If an int is passed, it should be greater than zero. Denotes the
            value to be used in seeding the random generator used to generate
            the draws from the normal distribution. Default == None.
        constrained_pos : list or None, optional.
            Denotes the positions of the array of estimated parameters that are
            not to change from their initial values. If a list is passed, the
            elements are to be integers where no such integer is greater than
            `init_values.size.` Default == None.
        print_res : bool, optional.
            Determines whether the timing and initial and final log likelihood
            results will be printed as they they are determined.
        method : str, optional.
            Should be a valid string which can be passed to
            scipy.optimize.minimize. Determines the optimization algorithm
            that is used for this problem.
        loss_tol : float, optional.
            Determines the tolerance on the difference in objective function
            values from one iteration to the next which is needed to determine
            convergence. Default = 1e-06.
        gradient_tol : float, optional.
            Determines the tolerance on the difference in gradient values from
            one iteration to the next which is needed to determine convergence.
            Default = 1e-06.
        maxiter : int, optional.
            Denotes the maximum number of iterations of the algorithm specified
            by `method` that will be used to estimate the parameters of the
            given model. Default == 1000.
        ridge : int, float, long, or None, optional.
            Determines whether or not ridge regression is performed. If a float
            is passed, then that float determines the ridge penalty for the
            optimization. Default = None.
        just_point : bool, optional.
            Determines whether (True) or not (False) calculations that are non-
            critical for obtaining the maximum likelihood point estimate will
            be performed. If True, this function will return the results
            dictionary from scipy.optimize. Default == False.

        Returns
        -------
        None. Estimation results are saved to the model instance.
        """"""
        # Check integrity of passed arguments
        kwargs_to_be_ignored = [""init_shapes"", ""init_intercepts"", ""init_coefs""]
        if any([x in kwargs for x in kwargs_to_be_ignored]):
            msg = ""MNL model does not use of any of the following kwargs:\n{}""
            msg_2 = ""Remove such kwargs and pass a single init_vals argument""
            raise ValueError(msg.format(kwargs_to_be_ignored) + msg_2)

        # Store the optimization method
        self.optimization_method = method

        # Store the ridge parameter
        self.ridge_param = ridge

        if ridge is not None:
            warnings.warn(_ridge_warning_msg)

        # Construct the mappings from alternatives to observations and from
        # chosen alternatives to observations
        mapping_res = self.get_mappings_for_fit()
        rows_to_mixers = mapping_res[""rows_to_mixers""]

        # Get the draws for each random coefficient
        num_mixing_units = rows_to_mixers.shape[1]
        draw_list = mlc.get_normal_draws(num_mixing_units,
                                         num_draws,
                                         len(self.mixing_pos),
                                         seed=seed)

        # Create the 3D design matrix
        self.design_3d = mlc.create_expanded_design_for_mixing(self.design,
                                                               draw_list,
                                                               self.mixing_pos,
                                                               rows_to_mixers)

        # Create the estimation object
        zero_vector = np.zeros(init_vals.shape)
        mixl_estimator = MixedEstimator(self,
                                        mapping_res,
                                        ridge,
                                        zero_vector,
                                        split_param_vec,
                                        constrained_pos=constrained_pos)

        # Perform one final check on the length of the initial values
        mixl_estimator.check_length_of_initial_values(init_vals)

        # Get the estimation results
        estimation_res = estimate(init_vals,
                                  mixl_estimator,
                                  method,
                                  loss_tol,
                                  gradient_tol,
                                  maxiter,
                                  print_res,
                                  use_hessian=True,
                                  just_point=just_point)

        if not just_point:
            # Store the mixed logit specific estimation results
            args = [mixl_estimator, estimation_res]
            estimation_res = add_mixl_specific_results_to_estimation_res(*args)

            # Store the estimation results
            self.store_fit_results(estimation_res)

            return None
        else:
            return estimation_res","Parameters
        ----------
        init_vals : 1D ndarray.
            Should contain the initial values to start the optimization process
            with. There should be one value for each utility coefficient and
            shape parameter being estimated.
        num_draws : int.
            Should be greater than zero. Denotes the number of draws that we
            are making from each normal distribution.
        seed : int or None, optional.
            If an int is passed, it should be greater than zero. Denotes the
            value to be used in seeding the random generator used to generate
            the draws from the normal distribution. Default == None.
        constrained_pos : list or None, optional.
            Denotes the positions of the array of estimated parameters that are
            not to change from their initial values. If a list is passed, the
            elements are to be integers where no such integer is greater than
            `init_values.size.` Default == None.
        print_res : bool, optional.
            Determines whether the timing and initial and final log likelihood
            results will be printed as they they are determined.
        method : str, optional.
            Should be a valid string which can be passed to
            scipy.optimize.minimize. Determines the optimization algorithm
            that is used for this problem.
        loss_tol : float, optional.
            Determines the tolerance on the difference in objective function
            values from one iteration to the next which is needed to determine
            convergence. Default = 1e-06.
        gradient_tol : float, optional.
            Determines the tolerance on the difference in gradient values from
            one iteration to the next which is needed to determine convergence.
            Default = 1e-06.
        maxiter : int, optional.
            Denotes the maximum number of iterations of the algorithm specified
            by `method` that will be used to estimate the parameters of the
            given model. Default == 1000.
        ridge : int, float, long, or None, optional.
            Determines whether or not ridge regression is performed. If a float
            is passed, then that float determines the ridge penalty for the
            optimization. Default = None.
        just_point : bool, optional.
            Determines whether (True) or not (False) calculations that are non-
            critical for obtaining the maximum likelihood point estimate will
            be performed. If True, this function will return the results
            dictionary from scipy.optimize. Default == False.

        Returns
        -------
        None. Estimation results are saved to the model instance."
"async def clear_topic_channel(self, channel):
        """"""Set the topic channel for this server""""""
        try:
            if self.topicchannel:
                await client.edit_channel(self.topicchannel, topic="""")
        except Exception as e:
            logger.exception(e)

        self.topicchannel = None
        logger.debug(""Clearing topic channel"")

        data = datatools.get_data()
        data[""discord""][""servers""][self.server_id][_data.modulename][""topic_id""] = """"
        datatools.write_data(data)

        await client.send_typing(channel)
        embed = ui_embed.topic_update(channel, self.topicchannel)
        await embed.send()",Set the topic channel for this server
"def gen_smul(src1, src2, dst):
        """"""Return a SMUL instruction.
        """"""
        assert src1.size == src2.size

        return ReilBuilder.build(ReilMnemonic.SMUL, src1, src2, dst)",Return a SMUL instruction.
"def add_colour_scaled_points(self, longitude, latitude, data, shape='s',
                                 alpha=1.0, size=20, norm=None, overlay=False):
        """"""
        Overlays a set of points on a map with a fixed size but colour scaled
        according to the data

        :param np.ndarray longitude:
            Longitude
        :param np.ndarray latitude:
            Latitude
        :param np.ndarray data:
            Data for plotting
        :param str shape:
            Marker style
        :param float alpha:
            Sets the transparency of the marker (0 for transparent, 1 opaque)
        :param int size:
            Marker size
        :param norm:
            Normalisation as instance of :class: matplotlib.colors.Normalize
        """"""
        if not norm:
            norm = Normalize(vmin=np.min(data), vmax=np.max(data))
        x, y, = self.m(longitude, latitude)
        mappable = self.m.scatter(x, y,
                                  marker=shape,
                                  s=size,
                                  c=data,
                                  norm=norm,
                                  alpha=alpha,
                                  linewidths=0.0,
                                  zorder=4)
        self.m.colorbar(mappable=mappable, fig=self.fig, ax=self.ax)
        if not overlay:
            plt.show()","Overlays a set of points on a map with a fixed size but colour scaled
        according to the data

        :param np.ndarray longitude:
            Longitude
        :param np.ndarray latitude:
            Latitude
        :param np.ndarray data:
            Data for plotting
        :param str shape:
            Marker style
        :param float alpha:
            Sets the transparency of the marker (0 for transparent, 1 opaque)
        :param int size:
            Marker size
        :param norm:
            Normalisation as instance of :class: matplotlib.colors.Normalize"
"def send_packed_virtual_touch_event(xpos, ypos, phase, device_id, finger):
    """"""Create a new WAKE_DEVICE_MESSAGE.""""""
    message = create(protobuf.SEND_PACKED_VIRTUAL_TOUCH_EVENT_MESSAGE)
    event = message.inner()

    # The packed version of VirtualTouchEvent contains X, Y, phase, deviceID
    # and finger stored as a byte array. Each value is written as 16bit little
    # endian integers.
    event.data = xpos.to_bytes(2, byteorder='little')
    event.data += ypos.to_bytes(2, byteorder='little')
    event.data += phase.to_bytes(2, byteorder='little')
    event.data += device_id.to_bytes(2, byteorder='little')
    event.data += finger.to_bytes(2, byteorder='little')

    return message",Create a new WAKE_DEVICE_MESSAGE.
"def within_joyner_boore_distance(self, surface, distance, **kwargs):
        '''
        Select events within a Joyner-Boore distance of a fault

        :param surface:
            Fault surface as instance of
            nhlib.geo.surface.base.SimpleFaultSurface  or as instance of
            nhlib.geo.surface.ComplexFaultSurface

        :param float distance:
            Rupture distance (km)

        :returns:
            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`
            containing only selected events
        '''

        upper_depth, lower_depth = _check_depth_limits(kwargs)

        rjb = surface.get_joyner_boore_distance(
            self.catalogue.hypocentres_as_mesh())
        is_valid = np.logical_and(
            rjb <= distance,
            np.logical_and(self.catalogue.data['depth'] >= upper_depth,
                           self.catalogue.data['depth'] < lower_depth))
        return self.select_catalogue(is_valid)","Select events within a Joyner-Boore distance of a fault

        :param surface:
            Fault surface as instance of
            nhlib.geo.surface.base.SimpleFaultSurface  or as instance of
            nhlib.geo.surface.ComplexFaultSurface

        :param float distance:
            Rupture distance (km)

        :returns:
            Instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`
            containing only selected events"
"def decode(self, encoded_payload):
        """"""Decode a transmitted payload.""""""
        self.packets = []
        while encoded_payload:
            if six.byte2int(encoded_payload[0:1]) <= 1:
                packet_len = 0
                i = 1
                while six.byte2int(encoded_payload[i:i + 1]) != 255:
                    packet_len = packet_len * 10 + six.byte2int(
                        encoded_payload[i:i + 1])
                    i += 1
                self.packets.append(packet.Packet(
                    encoded_packet=encoded_payload[i + 1:i + 1 + packet_len]))
            else:
                i = encoded_payload.find(b':')
                if i == -1:
                    raise ValueError('invalid payload')

                # extracting the packet out of the payload is extremely
                # inefficient, because the payload needs to be treated as
                # binary, but the non-binary packets have to be parsed as
                # unicode. Luckily this complication only applies to long
                # polling, as the websocket transport sends packets
                # individually wrapped.
                packet_len = int(encoded_payload[0:i])
                pkt = encoded_payload.decode('utf-8', errors='ignore')[
                    i + 1: i + 1 + packet_len].encode('utf-8')
                self.packets.append(packet.Packet(encoded_packet=pkt))

                # the engine.io protocol sends the packet length in
                # utf-8 characters, but we need it in bytes to be able to
                # jump to the next packet in the payload
                packet_len = len(pkt)
            encoded_payload = encoded_payload[i + 1 + packet_len:]",Decode a transmitted payload.
"def destroy(self):
        """"""
        Destroy the consumer group.
        """"""
        resp = {}
        for key in self.keys:
            resp[key] = self.database.xgroup_destroy(key, self.name)
        return resp",Destroy the consumer group.
"def search_orcid(orcid):
    """"""
    Search the ORCID public API

    Specfically, return a dictionary with the personal details
    (name, etc.) of the person associated with the given ORCID

    Args:
        orcid (`str`): The ORCID to be searched

    Returns:
        `dict`: Dictionary with the JSON response from the API

    Raises:
        `~requests.HTTPError`: If the given ORCID cannot be found, an `~requests.HTTPError`
            is raised with status code 404
    """"""
    url = 'https://pub.orcid.org/v2.1/{orcid}/person'.format(orcid=orcid)
    r = requests.get(url, headers=headers)
    if r.status_code != 200:
        r.raise_for_status()
    return r.json()","Search the ORCID public API

    Specfically, return a dictionary with the personal details
    (name, etc.) of the person associated with the given ORCID

    Args:
        orcid (`str`): The ORCID to be searched

    Returns:
        `dict`: Dictionary with the JSON response from the API

    Raises:
        `~requests.HTTPError`: If the given ORCID cannot be found, an `~requests.HTTPError`
            is raised with status code 404"
"def _image_url(array, fmt='png', mode=""data"", quality=90, domain=None):
  """"""Create a data URL representing an image from a PIL.Image.

  Args:
    image: a numpy
    mode: presently only supports ""data"" for data URL

  Returns:
    URL representing image
  """"""
  supported_modes = (""data"")
  if mode not in supported_modes:
    message = ""Unsupported mode '%s', should be one of '%s'.""
    raise ValueError(message, mode, supported_modes)

  image_data = serialize_array(array, fmt=fmt, quality=quality)
  base64_byte_string = base64.b64encode(image_data).decode('ascii')
  return ""data:image/"" + fmt.upper() + "";base64,"" + base64_byte_string","Create a data URL representing an image from a PIL.Image.

  Args:
    image: a numpy
    mode: presently only supports ""data"" for data URL

  Returns:
    URL representing image"
"def badge(self, *args, **kwargs):
        """"""
        Latest Build Status Badge

        Checks the status of the latest build of a given branch
        and returns corresponding badge svg.

        This method is ``experimental``
        """"""

        return self._makeApiCall(self.funcinfo[""badge""], *args, **kwargs)","Latest Build Status Badge

        Checks the status of the latest build of a given branch
        and returns corresponding badge svg.

        This method is ``experimental``"
"def read_and_write(self, reader, writer, chunk_size):
        """"""
        Read ``chunk_size`` from ``reader``, writing result to ``writer``.

        Returns ``None`` if successful, or ``True`` if the read was empty.

        .. versionadded:: 2.0
        """"""
        data = reader.recv(chunk_size)
        if len(data) == 0:
            return True
        writer.sendall(data)","Read ``chunk_size`` from ``reader``, writing result to ``writer``.

        Returns ``None`` if successful, or ``True`` if the read was empty.

        .. versionadded:: 2.0"
"def install_tracer(tracer=tracer, pattern=r"".*"", flags=0):
    """"""
    Installs given tracer in the candidates modules for tracing matching given pattern.

    :param tracer: Tracer.
    :type tracer: object
    :param pattern: Matching pattern.
    :type pattern: unicode
    :param flags: Matching regex flags.
    :type flags: int
    :return: Definition success.
    :rtype: bool
    """"""

    for module in REGISTERED_MODULES:
        if not re.search(pattern, module.__name__, flags=flags):
            continue

        trace_module(module, tracer)
    return True","Installs given tracer in the candidates modules for tracing matching given pattern.

    :param tracer: Tracer.
    :type tracer: object
    :param pattern: Matching pattern.
    :type pattern: unicode
    :param flags: Matching regex flags.
    :type flags: int
    :return: Definition success.
    :rtype: bool"
"def inConfig(self, config):
        """"""
        Save incoming config received from Polyglot to Interface.config and then do any functions
        that are waiting on the config to be received.
        """"""
        self.config = config
        self.isyVersion = config['isyVersion']
        try:
            for watcher in self.__configObservers:
                watcher(config)

            self.send_custom_config_docs()

        except KeyError as e:
            LOGGER.error('KeyError in gotConfig: {}'.format(e), exc_info=True)","Save incoming config received from Polyglot to Interface.config and then do any functions
        that are waiting on the config to be received."
"def get_objective_bank_hierarchy_design_session(self):
        """"""Gets the session designing objective bank hierarchies.

        return: (osid.learning.ObjectiveBankHierarchyDesignSession) - an
                ObjectiveBankHierarchyDesignSession
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented -
                supports_objective_bank_hierarchy_design() is false
        compliance: optional - This method must be implemented if
                    supports_objective_bank_hierarchy_design() is true.

        """"""
        if not self.supports_objective_bank_hierarchy_design():
            raise Unimplemented()
        try:
            from . import sessions
        except ImportError:
            raise OperationFailed()
        try:
            session = sessions.ObjectiveBankHierarchyDesignSession(runtime=self._runtime)
        except AttributeError:
            raise OperationFailed()
        return session","Gets the session designing objective bank hierarchies.

        return: (osid.learning.ObjectiveBankHierarchyDesignSession) - an
                ObjectiveBankHierarchyDesignSession
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented -
                supports_objective_bank_hierarchy_design() is false
        compliance: optional - This method must be implemented if
                    supports_objective_bank_hierarchy_design() is true."
"def add_comment(self, topic_id, content, reply_id=None):
        """"""
        
        
        :param topic_id: ID
        :param content: 
        :param reply_id: ID
        :return: None
        """"""
        return self.api.req(API_GROUP_ADD_COMMENT % topic_id, 'post', data={
            'ck': self.api.ck(),
            'ref_cid': reply_id,
            'rv_comment': content,
            'start': 0,
            'submit_btn': '',
        })","
        
        :param topic_id: ID
        :param content: 
        :param reply_id: ID
        :return: None"
"def get_instance(self, payload):
        """"""
        Build an instance of TaskActionsInstance

        :param dict payload: Payload response from the API

        :returns: twilio.rest.autopilot.v1.assistant.task.task_actions.TaskActionsInstance
        :rtype: twilio.rest.autopilot.v1.assistant.task.task_actions.TaskActionsInstance
        """"""
        return TaskActionsInstance(
            self._version,
            payload,
            assistant_sid=self._solution['assistant_sid'],
            task_sid=self._solution['task_sid'],
        )","Build an instance of TaskActionsInstance

        :param dict payload: Payload response from the API

        :returns: twilio.rest.autopilot.v1.assistant.task.task_actions.TaskActionsInstance
        :rtype: twilio.rest.autopilot.v1.assistant.task.task_actions.TaskActionsInstance"
"def parse_transaction(self, block_id, tx):
        """"""
        Given a block ID and an data-bearing transaction, 
        try to parse it into a virtual chain operation.
        
        Use the implementation's 'db_parse' method to do so.

        Data transactions that do not have the magic bytes or a valid opcode
        will be skipped automatically.  The db_parse method does not need
        to know how to handle them.

        @tx is a dict with
        `txid`: the transaction ID
        `txindex`: the offset in the block where this tx occurs
        `nulldata`: the hex-encoded scratch data from the transaction
        `ins`: the list of transaction inputs
        `outs`: the list of transaction outputs
        `senders`: the list of transaction senders
        `fee`: the transaction fee
        `txhex`: the hex-encoded raw transaction
        
        Return a dict representing the data on success.
        Return None on error
        """"""
        
        data_hex = tx['nulldata']
        inputs = tx['ins']
        outputs = tx['outs']
        senders = tx['senders']
        fee = tx['fee']
        txhex = tx['hex']
        merkle_path = tx['tx_merkle_path']
        
        if not is_hex(data_hex):
            # should always work; the tx downloader converts the binary string to hex
            # not a valid hex string 
            raise ValueError(""Invalid nulldata: not hex-encoded"")
        
        if len(data_hex) % 2 != 0:
            # should always work; the tx downloader converts the binary string to hex
            # not valid hex string 
            raise ValueError(""Invalid nulldata: not hex-encoded"")
        
        data_bin = None
        try:
            # should always work; the tx downloader converts the binary string to hex
            data_bin = data_hex.decode('hex')
        except Exception, e:
            log.error(""Failed to parse transaction: %s (data_hex = %s)"" % (tx, data_hex))
            raise ValueError(""Invalid nulldata: not hex-encoded"")
        
        if not data_bin.startswith(self.magic_bytes):
            # not for us
            return None
        
        if len(data_bin) < len(self.magic_bytes) + 1:
            # invalid operation--no opcode
            return None

        # 3rd byte is always the operation code
        op_code = data_bin[len(self.magic_bytes)]
        if op_code not in self.opcodes:
            return None 
        
        # looks like an op.  Try to parse it.
        op_payload = data_bin[len(self.magic_bytes)+1:]
        
        op = self.impl.db_parse(block_id, tx['txid'], tx['txindex'], op_code, op_payload, senders, inputs, outputs, fee, db_state=self.state, raw_tx=txhex)
        if op is None:
            # not valid 
            return None 
        
        # store it
        op['virtualchain_opcode'] = op_code
        op['virtualchain_txid'] = tx['txid']
        op['virtualchain_txindex'] = tx['txindex']
        op['virtualchain_txhex'] = txhex
        op['virtualchain_tx_merkle_path'] = merkle_path
        op['virtualchain_senders'] = senders
        op['virtualchain_fee'] = fee
        op['virtualchain_data_hex'] = op_payload.encode('hex')
        
        return op","Given a block ID and an data-bearing transaction, 
        try to parse it into a virtual chain operation.
        
        Use the implementation's 'db_parse' method to do so.

        Data transactions that do not have the magic bytes or a valid opcode
        will be skipped automatically.  The db_parse method does not need
        to know how to handle them.

        @tx is a dict with
        `txid`: the transaction ID
        `txindex`: the offset in the block where this tx occurs
        `nulldata`: the hex-encoded scratch data from the transaction
        `ins`: the list of transaction inputs
        `outs`: the list of transaction outputs
        `senders`: the list of transaction senders
        `fee`: the transaction fee
        `txhex`: the hex-encoded raw transaction
        
        Return a dict representing the data on success.
        Return None on error"
"def write_command(self, command: Command):
        '''Write a command to the stream.

        Args:
            command: The command.

        Coroutine.
        '''
        _logger.debug('Write command.')
        data = command.to_bytes()
        yield from self._connection.write(data)
        self._data_event_dispatcher.notify_write(data)","Write a command to the stream.

        Args:
            command: The command.

        Coroutine."
"def load(*args, **kwargs):
    """"""Load an numpy.ndarray from a file stream.

    This works exactly like the usual `json.load()` function,
    but it uses our custom deserializer.
    """"""
    kwargs.update(dict(object_hook=json_numpy_obj_hook))
    return _json.load(*args, **kwargs)","Load an numpy.ndarray from a file stream.

    This works exactly like the usual `json.load()` function,
    but it uses our custom deserializer."
"def _formatNumbers(self, line):
        """"""
        Format the numbers so that there are commas inserted.

        For example: 1200300 becomes 1,200,300.
        """"""
        # below thousands separator syntax only works for
        # python 2.7, skip for 2.6
        if sys.version_info < (2, 7):
            return line

        last_index = 0
        try:
            # find the index of the last } character
            last_index = (line.rindex('}') + 1)
            end = line[last_index:]
        except ValueError:
            return line
        else:
            # split the string on numbers to isolate them
            splitted = re.split(""(\d+)"", end)
            for index, val in enumerate(splitted):
                converted = 0
                try:
                    converted = int(val)
                # if it's not an int pass and don't change the string
                except ValueError:
                    pass
                else:
                    if converted > 1000:
                        splitted[index] = format(converted, "",d"")
            return line[:last_index] + ("""").join(splitted)","Format the numbers so that there are commas inserted.

        For example: 1200300 becomes 1,200,300."
"def remove(self, child):
        '''Remove a ``child`` from the list of :attr:`children`.'''
        try:
            self.children.remove(child)
            if isinstance(child, String):
                child._parent = None
        except ValueError:
            pass",Remove a ``child`` from the list of :attr:`children`.
"def cleanup(connector_manager, red_data, tmp_dir):
    """"""
    Invokes the cleanup functions for all inputs.
    """"""
    for key, arg in red_data['inputs'].items():
        val = arg

        if isinstance(arg, list):
            for index, i in enumerate(arg):
                if not isinstance(i, dict):
                    continue

                # connector_class should be one of 'File' or 'Directory'
                connector_class = i['class']
                input_key = '{}_{}'.format(key, index)
                path = os.path.join(tmp_dir, input_key)
                connector_data = i['connector']
                internal = {URL_SCHEME_IDENTIFIER: path}

                if connector_class == 'File':
                    connector_manager.receive_cleanup(connector_data, input_key, internal)
                elif connector_class == 'Directory':
                    connector_manager.receive_directory_cleanup(connector_data, input_key, internal)

        elif isinstance(arg, dict):
            # connector_class should be one of 'File' or 'Directory'
            connector_class = arg['class']
            input_key = key
            path = os.path.join(tmp_dir, input_key)
            connector_data = val['connector']
            internal = {URL_SCHEME_IDENTIFIER: path}

            if connector_class == 'File':
                connector_manager.receive_cleanup(connector_data, input_key, internal)
            elif connector_class == 'Directory':
                connector_manager.receive_directory_cleanup(connector_data, input_key, internal)

    try:
        os.rmdir(tmp_dir)
    except (OSError, FileNotFoundError):
        # Maybe, raise a warning here, because not all connectors have cleaned up their contents correctly.
        pass",Invokes the cleanup functions for all inputs.
"def prune_basis(basis, use_copy=True):
    """"""
    Removes primitives that have a zero coefficient, and
    removes duplicate primitives and shells

    This only finds EXACT duplicates, and is meant to be used
    after other manipulations

    If use_copy is True, the input basis set is not modified.
    """"""

    if use_copy:
        basis = copy.deepcopy(basis)

    for k, el in basis['elements'].items():
        if not 'electron_shells' in el:
            continue

        shells = el.pop('electron_shells')
        shells = [prune_shell(sh, False) for sh in shells]

        # Remove any duplicates
        el['electron_shells'] = []

        for sh in shells:
            if sh not in el['electron_shells']:
                el['electron_shells'].append(sh)

    return basis","Removes primitives that have a zero coefficient, and
    removes duplicate primitives and shells

    This only finds EXACT duplicates, and is meant to be used
    after other manipulations

    If use_copy is True, the input basis set is not modified."
"def stream_text(text, chunk_size=default_chunk_size):
    """"""Gets a buffered generator for streaming text.

    Returns a buffered generator which encodes a string as
    :mimetype:`multipart/form-data` with the corresponding headers.

    Parameters
    ----------
    text : str
        The data bytes to stream
    chunk_size : int
        The maximum size of each stream chunk

    Returns
    -------
        (generator, dict)
    """"""
    if isgenerator(text):
        def binary_stream():
            for item in text:
                if six.PY2 and isinstance(text, six.binary_type):
                    #PY2: Allow binary strings under Python 2 since
                    # Python 2 code is not expected to always get the
                    # distinction between text and binary strings right.
                    yield text
                else:
                    yield text.encode(""utf-8"")
        data = binary_stream()
    elif six.PY2 and isinstance(text, six.binary_type):
        #PY2: See above.
        data = text
    else:
        data = text.encode(""utf-8"")

    return stream_bytes(data, chunk_size)","Gets a buffered generator for streaming text.

    Returns a buffered generator which encodes a string as
    :mimetype:`multipart/form-data` with the corresponding headers.

    Parameters
    ----------
    text : str
        The data bytes to stream
    chunk_size : int
        The maximum size of each stream chunk

    Returns
    -------
        (generator, dict)"
"def p(self, type):
        """"""Returns the probability (relative frequency) of the token""""""
        if self.dovalidation: type = self._validate(type)
        return self._count[type] / float(self.total)",Returns the probability (relative frequency) of the token
"def handle_processing_packets():
    """"""handle_processing_packets

    Replacement packet processing engine. This is not done.

    """"""

    host = os.getenv(
        ""LISTEN_ON_HOST"",
        ""127.0.0.1"").strip().lstrip()
    port = int(os.getenv(
        ""LISTEN_ON_PORT"",
        ""80"").strip().lstrip())
    backlog = int(os.getenv(
        ""LISTEN_BACKLOG"",
        ""5"").strip().lstrip())
    size = int(os.getenv(
        ""LISTEN_SIZE"",
        ""102400"").strip().lstrip())
    sleep_in_seconds = float(os.getenv(
        ""LISTEN_SLEEP"",
        ""0.5"").strip().lstrip())
    needs_response = bool(os.getenv(
        ""LISTEN_SEND_RESPONSE"",
        ""0"").strip().lstrip() == ""1"")
    shutdown_hook = os.getenv(
        ""LISTEN_SHUTDOWN_HOOK"",
        ""/tmp/shutdown-listen-server-{}-{}"".format(
            host,
            port)).strip().lstrip()
    filter_key = os.getenv(
        ""IGNORE_KEY"",
        INCLUDED_IGNORE_KEY).strip().lstrip()

    if os.path.exists(shutdown_hook):
        log.info((""Please remove the shutdown hook file: ""
                  ""\nrm -f {}"")
                 .format(
                    shutdown_hook))
        sys.exit(1)

    default_filter_key = filter_key
    bytes_for_filter_key = len(default_filter_key)
    offset_to_filter_key = (-1 * bytes_for_filter_key)
    offset_to_msg = offset_to_filter_key - 1

    now = datetime.datetime.now().isoformat()
    log.info((""{} - Starting Server address={}:{} ""
              ""backlog={} size={} sleep={} shutdown={} ""
              ""filter_key={}"")
             .format(
                now,
                host,
                port,
                backlog,
                size,
                sleep_in_seconds,
                shutdown_hook,
                default_filter_key))

    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind((host, port))
    s.listen(backlog)
    client, address = s.accept()

    midx = 0
    while 1:
        data = None
        address = None
        ignore_key = None

        try:
            if not client:
                client, address = s.accept()
        except Exception as e:
            log.error((""socket accept with ex={}"")
                      .format(
                        e))
        try:
            if client:
                data = client.recv(size)
        except Exception as e:
            log.error((""recv - disconnected with ex={}"")
                      .format(
                        e))
        if data:
            now = datetime.datetime.now().isoformat()
            packet_to_process = data[0:offset_to_msg]
            ignore_key = data[offset_to_filter_key:]
            log.info((""decoding data={} key={}"")
                     .format(
                        packet_to_process,
                        ignore_key))

            msg = None
            try:
                msg = json.loads(
                            packet_to_process.decode(""utf-8""))
            except Exception as e:
                msg = None
                log.error((""Invalid data={} with ex={}"")
                          .format(
                            packet_to_process,
                            e))

            if msg:

                log.info((""received msg={} ""
                          ""data={} replying - ignore='{}'"")
                         .format(
                            ppj(msg),
                            packet_to_process,
                            ignore_key))

                if msg[""status""] == VALID:
                    if msg[""data_type""] == TCP:
                        log.info(""TCP"")
                    elif msg[""data_type""] == UDP:
                        log.info(""TCP"")
                    elif msg[""data_type""] == ARP:
                        log.info(""TCP"")
                    elif msg[""data_type""] == ICMP:
                        log.info(""TCP"")
                    else:
                        log.error((""unsuppported type={}"")
                                  .format(
                                    msg[""data_type""]))
                    # end of supported eth protocol message types
                else:
                    log.error((""unsuppported msg status={}"")
                              .format(
                                msg[""status""]))
                # end if msg was VALID
            # end of if found msg

            midx += 1
            if midx > 1000000:
                midx = 0
        else:
            log.debug(""ignoring invalid data"")
        # end of if valid msg or not

        if needs_response:
            client.send(ignore_key)
        else:
            log.info(""no response"")
            time.sleep(sleep_in_seconds)

        if os.path.exists(shutdown_hook):
            now = datetime.datetime.now().isoformat()
            log.info((""{} detected shutdown ""
                      ""file={}"")
                     .format(
                        now,
                        shutdown_hook))
    # end of loop

    log.info(""shutting down"")
    client.close()

    log.info(""done"")","handle_processing_packets

    Replacement packet processing engine. This is not done."
"def QueryItems(self, database_or_Container_link, query, options=None, partition_key=None):
        """"""Queries documents in a collection.

        :param str database_or_Container_link:
            The link to the database when using partitioning, otherwise link to the document collection.
        :param (str or dict) query:
        :param dict options:
            The request options for the request.
        :param str partition_key:
            Partition key for the query(default value None)

        :return:
            Query Iterable of Documents.
        :rtype:
            query_iterable.QueryIterable

        """"""
        database_or_Container_link = base.TrimBeginningAndEndingSlashes(database_or_Container_link)

        if options is None:
            options = {}

        if(base.IsDatabaseLink(database_or_Container_link)):
            # Python doesn't have a good way of specifying an overloaded constructor, and this is how it's generally overloaded constructors are specified(by calling a @classmethod) and returning the 'self' instance
            return query_iterable.QueryIterable.PartitioningQueryIterable(self, query, options, database_or_Container_link, partition_key)
        else:    
            path = base.GetPathFromLink(database_or_Container_link, 'docs')
            collection_id = base.GetResourceIdOrFullNameFromLink(database_or_Container_link)
            def fetch_fn(options):
                return self.__QueryFeed(path,
                                        'docs',
                                        collection_id,
                                        lambda r: r['Documents'],
                                        lambda _, b: b,
                                        query,
                                        options), self.last_response_headers
            return query_iterable.QueryIterable(self, query, options, fetch_fn, database_or_Container_link)","Queries documents in a collection.

        :param str database_or_Container_link:
            The link to the database when using partitioning, otherwise link to the document collection.
        :param (str or dict) query:
        :param dict options:
            The request options for the request.
        :param str partition_key:
            Partition key for the query(default value None)

        :return:
            Query Iterable of Documents.
        :rtype:
            query_iterable.QueryIterable"
"def check_key(data_object, key, cardinal=False):
    """"""
    Update the value of an index key by matching values or getting positionals.
    """"""
    itype = (int, np.int32, np.int64)
    if not isinstance(key, itype + (slice, tuple, list, np.ndarray)):
        raise KeyError(""Unknown key type {} for key {}"".format(type(key), key))
    keys = data_object.index.values
    if cardinal and data_object._cardinal is not None:
        keys = data_object[data_object._cardinal[0]].unique()
    elif isinstance(key, itype) and key in keys:
        key = list(sorted(data_object.index.values[key]))
    elif isinstance(key, itype) and key < 0:
        key = list(sorted(data_object.index.values[key]))
    elif isinstance(key, itype):
        key = [key]
    elif isinstance(key, slice):
        key = list(sorted(data_object.index.values[key]))
    elif isinstance(key, (tuple, list, pd.Index)) and not np.all(k in keys for k in key):
        key = list(sorted(data_object.index.values[key]))
    return key",Update the value of an index key by matching values or getting positionals.
"def GetSectionByIndex(self, section_index):
    """"""Retrieves a specific section based on the index.

    Args:
      section_index (int): index of the section.

    Returns:
      VolumeExtent: a volume extent or None if not available.
    """"""
    if not self._is_parsed:
      self._Parse()
      self._is_parsed = True

    if section_index < 0 or section_index >= len(self._sections):
      return None

    return self._sections[section_index]","Retrieves a specific section based on the index.

    Args:
      section_index (int): index of the section.

    Returns:
      VolumeExtent: a volume extent or None if not available."
"def _normalize(self, name, columns, points):
        """"""Normalize data for the InfluxDB's data model.""""""

        for i, _ in enumerate(points):
            # Supported type:
            # https://docs.influxdata.com/influxdb/v1.5/write_protocols/line_protocol_reference/
            if points[i] is None:
                # Ignore points with None value
                del(points[i])
                del(columns[i])
                continue
            try:
                points[i] = float(points[i])
            except (TypeError, ValueError):
                pass
            else:
                continue
            try:
                points[i] = str(points[i])
            except (TypeError, ValueError):
                pass
            else:
                continue

        return [{'measurement': name,
                 'tags': self.parse_tags(self.tags),
                 'fields': dict(zip(columns, points))}]",Normalize data for the InfluxDB's data model.
"def response_doc(self):
        """"""The XML document received from the service.""""""
        try:
            return self.__dict__['response_doc']
        except KeyError:
            self.__dict__['response_doc'] = ElementTree.fromstring(
                self.response_xml
            )
            return self.__dict__['response_doc']",The XML document received from the service.
"def policies(self):
        """"""Return list of policies zipped with their respective data type""""""
        policies = [self._pb.integer_policy, self._pb.float_policy,
                    self._pb.string_policy, self._pb.bool_policy]
        key_types = [""integer"", ""float"", ""string"", ""bool""]
        return zip(key_types, policies)",Return list of policies zipped with their respective data type
"def basic_consume(self, queue='', consumer_tag='', no_local=False,
        no_ack=False, exclusive=False, nowait=False,
        callback=None, ticket=None):
        """"""
        start a queue consumer

        This method asks the server to start a ""consumer"", which is a
        transient request for messages from a specific queue.
        Consumers last as long as the channel they were created on, or
        until the client cancels them.

        RULE:

            The server SHOULD support at least 16 consumers per queue,
            unless the queue was declared as private, and ideally,
            impose no limit except as defined by available resources.

        PARAMETERS:
            queue: shortstr

                Specifies the name of the queue to consume from.  If
                the queue name is null, refers to the current queue
                for the channel, which is the last declared queue.

                RULE:

                    If the client did not previously declare a queue,
                    and the queue name in this method is empty, the
                    server MUST raise a connection exception with
                    reply code 530 (not allowed).

            consumer_tag: shortstr

                Specifies the identifier for the consumer. The
                consumer tag is local to a connection, so two clients
                can use the same consumer tags. If this field is empty
                the server will generate a unique tag.

                RULE:

                    The tag MUST NOT refer to an existing consumer. If
                    the client attempts to create two consumers with
                    the same non-empty tag the server MUST raise a
                    connection exception with reply code 530 (not
                    allowed).

            no_local: boolean

                do not deliver own messages

                If the no-local field is set the server will not send
                messages to the client that published them.

            no_ack: boolean

                no acknowledgement needed

                If this field is set the server does not expect
                acknowledgments for messages.  That is, when a message
                is delivered to the client the server automatically and
                silently acknowledges it on behalf of the client.  This
                functionality increases performance but at the cost of
                reliability.  Messages can get lost if a client dies
                before it can deliver them to the application.

            exclusive: boolean

                request exclusive access

                Request exclusive consumer access, meaning only this
                consumer can access the queue.

                RULE:

                    If the server cannot grant exclusive access to the
                    queue when asked, - because there are other
                    consumers active - it MUST raise a channel
                    exception with return code 403 (access refused).

            nowait: boolean

                do not send a reply method

                If set, the server will not respond to the method. The
                client should not wait for a reply method.  If the
                server could not complete the method it will raise a
                channel or connection exception.

            callback: Python callable

                function/method called with each delivered message

                For each message delivered by the broker, the
                callable will be called with a Message object
                as the single argument.  If no callable is specified,
                messages are quietly discarded, no_ack should probably
                be set to True in that case.

            ticket: short

                RULE:

                    The client MUST provide a valid access ticket
                    giving ""read"" access rights to the realm for the
                    queue.

        """"""
        args = AMQPWriter()
        if ticket is not None:
            args.write_short(ticket)
        else:
            args.write_short(self.default_ticket)
        args.write_shortstr(queue)
        args.write_shortstr(consumer_tag)
        args.write_bit(no_local)
        args.write_bit(no_ack)
        args.write_bit(exclusive)
        args.write_bit(nowait)
        self._send_method((60, 20), args)

        if not nowait:
            consumer_tag = self.wait(allowed_methods=[
                              (60, 21),    # Channel.basic_consume_ok
                            ])

        self.callbacks[consumer_tag] = callback

        return consumer_tag","start a queue consumer

        This method asks the server to start a ""consumer"", which is a
        transient request for messages from a specific queue.
        Consumers last as long as the channel they were created on, or
        until the client cancels them.

        RULE:

            The server SHOULD support at least 16 consumers per queue,
            unless the queue was declared as private, and ideally,
            impose no limit except as defined by available resources.

        PARAMETERS:
            queue: shortstr

                Specifies the name of the queue to consume from.  If
                the queue name is null, refers to the current queue
                for the channel, which is the last declared queue.

                RULE:

                    If the client did not previously declare a queue,
                    and the queue name in this method is empty, the
                    server MUST raise a connection exception with
                    reply code 530 (not allowed).

            consumer_tag: shortstr

                Specifies the identifier for the consumer. The
                consumer tag is local to a connection, so two clients
                can use the same consumer tags. If this field is empty
                the server will generate a unique tag.

                RULE:

                    The tag MUST NOT refer to an existing consumer. If
                    the client attempts to create two consumers with
                    the same non-empty tag the server MUST raise a
                    connection exception with reply code 530 (not
                    allowed).

            no_local: boolean

                do not deliver own messages

                If the no-local field is set the server will not send
                messages to the client that published them.

            no_ack: boolean

                no acknowledgement needed

                If this field is set the server does not expect
                acknowledgments for messages.  That is, when a message
                is delivered to the client the server automatically and
                silently acknowledges it on behalf of the client.  This
                functionality increases performance but at the cost of
                reliability.  Messages can get lost if a client dies
                before it can deliver them to the application.

            exclusive: boolean

                request exclusive access

                Request exclusive consumer access, meaning only this
                consumer can access the queue.

                RULE:

                    If the server cannot grant exclusive access to the
                    queue when asked, - because there are other
                    consumers active - it MUST raise a channel
                    exception with return code 403 (access refused).

            nowait: boolean

                do not send a reply method

                If set, the server will not respond to the method. The
                client should not wait for a reply method.  If the
                server could not complete the method it will raise a
                channel or connection exception.

            callback: Python callable

                function/method called with each delivered message

                For each message delivered by the broker, the
                callable will be called with a Message object
                as the single argument.  If no callable is specified,
                messages are quietly discarded, no_ack should probably
                be set to True in that case.

            ticket: short

                RULE:

                    The client MUST provide a valid access ticket
                    giving ""read"" access rights to the realm for the
                    queue."
"def to_mozlog_format(self):
        """"""Convert a FailureLine into a mozlog formatted dictionary.""""""
        data = {
            ""action"": self.action,
            ""line_number"": self.line,
            ""test"": self.test,
            ""subtest"": self.subtest,
            ""status"": self.status,
            ""expected"": self.expected,
            ""message"": self.message,
            ""signature"": self.signature,
            ""level"": self.level,
            ""stack"": self.stack,
            ""stackwalk_stdout"": self.stackwalk_stdout,
            ""stackwalk_stderr"": self.stackwalk_stderr,
        }

        # Remove empty values
        data = {k: v for k, v in data.items() if v}

        return data",Convert a FailureLine into a mozlog formatted dictionary.
"def _load_ancillary_variables(self, datasets):
        """"""Load the ancillary variables of `datasets`.""""""
        all_av_ids = set()
        for dataset in datasets.values():
            ancillary_variables = dataset.attrs.get('ancillary_variables', [])
            if not isinstance(ancillary_variables, (list, tuple, set)):
                ancillary_variables = ancillary_variables.split(' ')
            av_ids = []
            for key in ancillary_variables:
                try:
                    av_ids.append(self.get_dataset_key(key))
                except KeyError:
                    logger.warning(""Can't load ancillary dataset %s"", str(key))

            all_av_ids |= set(av_ids)
            dataset.attrs['ancillary_variables'] = av_ids
        loadable_av_ids = [av_id for av_id in all_av_ids if av_id not in datasets]
        if not all_av_ids:
            return
        if loadable_av_ids:
            self.load(loadable_av_ids, previous_datasets=datasets)

        for dataset in datasets.values():
            new_vars = []
            for av_id in dataset.attrs.get('ancillary_variables', []):
                if isinstance(av_id, DatasetID):
                    new_vars.append(datasets[av_id])
                else:
                    new_vars.append(av_id)
            dataset.attrs['ancillary_variables'] = new_vars",Load the ancillary variables of `datasets`.
"def create_table_rate_rule(cls, table_rate_rule, **kwargs):
        """"""Create TableRateRule

        Create a new TableRateRule
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.create_table_rate_rule(table_rate_rule, async=True)
        >>> result = thread.get()

        :param async bool
        :param TableRateRule table_rate_rule: Attributes of tableRateRule to create (required)
        :return: TableRateRule
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async'):
            return cls._create_table_rate_rule_with_http_info(table_rate_rule, **kwargs)
        else:
            (data) = cls._create_table_rate_rule_with_http_info(table_rate_rule, **kwargs)
            return data","Create TableRateRule

        Create a new TableRateRule
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async=True
        >>> thread = api.create_table_rate_rule(table_rate_rule, async=True)
        >>> result = thread.get()

        :param async bool
        :param TableRateRule table_rate_rule: Attributes of tableRateRule to create (required)
        :return: TableRateRule
                 If the method is called asynchronously,
                 returns the request thread."
"def publishing(self, service):
        """"""
            the purpose of this tasks is to get the data from the cache
            then publish them
            :param service: service object where we will publish
            :type service: object
        """"""
        # flag to know if we have to update
        to_update = False
        # flag to get the status of a service
        status = False
        # provider - the service that offer data
        # check if the service has already been triggered
        # if date_triggered is None, then it's the first run
        if service.date_triggered is None:
            logger.debug(""first run {}"".format(service))
            to_update = True
            status = True
        # run run run
        data = self.provider(service)
        count_new_data = len(data) if data else 0
        if count_new_data > 0:
            to_update, status = self.consumer(service, data, to_update, status)
            # let's log
        self.log_update(service, to_update, status, count_new_data)
        # let's update
        if to_update and status:
            self.update_trigger(service)","the purpose of this tasks is to get the data from the cache
            then publish them
            :param service: service object where we will publish
            :type service: object"
"def data_to_imagesurface (data, **kwargs):
    """"""Turn arbitrary data values into a Cairo ImageSurface.

    The method and arguments are the same as data_to_argb32, except that the
    data array will be treated as 2D, and higher dimensionalities are not
    allowed. The return value is a Cairo ImageSurface object.

    Combined with the write_to_png() method on ImageSurfaces, this is an easy
    way to quickly visualize 2D data.

    """"""
    import cairo

    data = np.atleast_2d (data)
    if data.ndim != 2:
        raise ValueError ('input array may not have more than 2 dimensions')

    argb32 = data_to_argb32 (data, **kwargs)

    format = cairo.FORMAT_ARGB32
    height, width = argb32.shape
    stride = cairo.ImageSurface.format_stride_for_width (format, width)

    if argb32.strides[0] != stride:
        raise ValueError ('stride of data array not compatible with ARGB32')

    return cairo.ImageSurface.create_for_data (argb32, format,
                                               width, height, stride)","Turn arbitrary data values into a Cairo ImageSurface.

    The method and arguments are the same as data_to_argb32, except that the
    data array will be treated as 2D, and higher dimensionalities are not
    allowed. The return value is a Cairo ImageSurface object.

    Combined with the write_to_png() method on ImageSurfaces, this is an easy
    way to quickly visualize 2D data."
"async def set_as_default_gateway(self):
        """"""Set this link as the default gateway for the node.""""""
        interface = self._data['interface']
        await interface._handler.set_default_gateway(
            system_id=interface.node.system_id, id=interface.id,
            link_id=self.id)",Set this link as the default gateway for the node.
"def viewport(self):
        """"""Rect: The drawing area for rendering on the current target.""""""
        viewport = rect.Rect(0, 0, 0, 0)
        check_int_err(lib.SDL_RenderGetViewport(self._ptr, viewport._ptr))
        return viewport",Rect: The drawing area for rendering on the current target.
"def upload_progress(request):
    """"""
    Used by Ajax calls

    Return the upload progress and total length values
    """"""
    if 'X-Progress-ID' in request.GET:
        progress_id = request.GET['X-Progress-ID']
    elif 'X-Progress-ID' in request.META:
        progress_id = request.META['X-Progress-ID']
    if progress_id:
        cache_key = ""%s_%s"" % (request.META['REMOTE_ADDR'], progress_id)
        data = cache.get(cache_key)
        return HttpResponse(simplejson.dumps(data))","Used by Ajax calls

    Return the upload progress and total length values"
"def get_specidentitem_percolator_data(item, xmlns):
    """"""Loop through SpecIdentificationItem children. Find
    percolator data by matching to a dict lookup. Return a
    dict containing percolator data""""""
    percomap = {'{0}userParam'.format(xmlns): PERCO_HEADERMAP, }
    percodata = {}
    for child in item:
        try:
            percoscore = percomap[child.tag][child.attrib['name']]
        except KeyError:
            continue
        else:
            percodata[percoscore] = child.attrib['value']
    outkeys = [y for x in list(percomap.values()) for y in list(x.values())]
    for key in outkeys:
        try:
            percodata[key]
        except KeyError:
            percodata[key] = 'NA'
    return percodata","Loop through SpecIdentificationItem children. Find
    percolator data by matching to a dict lookup. Return a
    dict containing percolator data"
"def writeClient(self, fd, sdClass=None, **kw):
        """"""write out client module to file descriptor.
        Parameters and Keywords arguments:
            fd -- file descriptor
            sdClass -- service description class name
            imports -- list of imports
            readerclass -- class name of ParsedSoap reader
            writerclass -- class name of SoapWriter writer
        """"""
        sdClass = sdClass or ServiceDescription
        assert issubclass(sdClass, ServiceDescription), \
            'parameter sdClass must subclass ServiceDescription'

#        header = '%s \n# %s.py \n# generated by %s\n%s\n'\
#                  %('#'*50, self.getClientModuleName(), self.__module__, '#'*50)
        print >>fd, '#'*50
        print >>fd, '# file: %s.py' %self.getClientModuleName()
        print >>fd, '# '
        print >>fd, '# client stubs generated by ""%s""' %self.__class__
        print >>fd, '#     %s' %' '.join(sys.argv)
        print >>fd, '# '
        print >>fd, '#'*50

        self.services = []
        for service in self._wsdl.services:
            sd = sdClass(self._addressing, do_extended=self.do_extended, 
                         wsdl=self._wsdl)
            if len(self._wsdl.types) > 0:
                sd.setTypesModuleName(self.getTypesModuleName(), 
                                      self.getTypesModulePath())
#                sd.setMessagesModuleName(self.getMessagesModuleName(), 
#                                         self.getMessagesModulePath())

            self.gatherNamespaces()
            sd.fromWsdl(service, **kw)
            sd.write(fd)
            self.services.append(sd)","write out client module to file descriptor.
        Parameters and Keywords arguments:
            fd -- file descriptor
            sdClass -- service description class name
            imports -- list of imports
            readerclass -- class name of ParsedSoap reader
            writerclass -- class name of SoapWriter writer"
"def drain_ND(self):
        """"""Returns the diameter of the drain pipe.
        Each drain pipe will drain two channels because channels are connected by
        a port at the far end and the first channel can't have a drain because
        of the entrance tank. Need to review the design to see if this is a good
        assumption.
        D_{Pipe} = \sqrt{ \frac{8 A_{Tank}}{\pi t_{Drain}} \sqrt{ \frac{h_0 \sum K}{2g} } }
        :returns: list of designed values
        :rtype: float * centimeter
        """"""
        drain_ND = pipes.ND_SDR_available(self.drain_D, self.SDR)
        return drain_ND","Returns the diameter of the drain pipe.
        Each drain pipe will drain two channels because channels are connected by
        a port at the far end and the first channel can't have a drain because
        of the entrance tank. Need to review the design to see if this is a good
        assumption.
        D_{Pipe} = \sqrt{ \frac{8 A_{Tank}}{\pi t_{Drain}} \sqrt{ \frac{h_0 \sum K}{2g} } }
        :returns: list of designed values
        :rtype: float * centimeter"
"def configure(self, args):
        """"""Configure the set of plugins with the given args.

        After configuration, disabled plugins are removed from the plugins list.
        """"""
        for plug in self._plugins:
            plug_name = self.plugin_name(plug)
            plug.enabled = getattr(args, ""plugin_%s"" % plug_name, False)
            if plug.enabled and getattr(plug, ""configure"", None):
                if callable(getattr(plug, ""configure"", None)):
                    plug.configure(args)
        LOG.debug(""Available plugins: %s"", self._plugins)
        self.plugins = [plugin for plugin in self._plugins if getattr(plugin, ""enabled"", False)]
        LOG.debug(""Enabled plugins: %s"", self.plugins)","Configure the set of plugins with the given args.

        After configuration, disabled plugins are removed from the plugins list."
"def _imm_setattr(self, name, value):
    '''
    A persistent immutable's setattr simply does not allow attributes to be set.
    '''
    if _imm_is_persist(self):
        raise TypeError('Attempt to change parameter \'%s\' of non-transient immutable' % name)
    elif _imm_is_trans(self):
        return _imm_trans_setattr(self, name, value)
    else:
        return _imm_init_setattr(self, name, value)",A persistent immutable's setattr simply does not allow attributes to be set.
"def resolve(
            self, expr, name,
            safe=DEFAULT_SAFE, tostr=DEFAULT_TOSTR, scope=DEFAULT_SCOPE,
            besteffort=DEFAULT_BESTEFFORT
    ):
        """"""Resolve an expression with possibly a dedicated expression resolvers.

        :param str name: expression resolver registered name. Default is the
            first registered expression resolver.
        :param bool safe: if True (Default), resolve in a safe context
            (without I/O).
        :param bool tostr: if True (False by default), transform the result into
            a string format.
        :param dict scope: scope execution resolution.
        :return: a string if tostr, otherwise, a python object.

        :raises: KeyError if no expression resolver has been registered or if
            name does not exist in expression resolvers.
        """"""

        resolver = None

        if name is None:
            name = self.default

        resolver = self[name]

        result = resolver(
            expr=expr,
            safe=safe, tostr=tostr, scope=scope, besteffort=besteffort
        )

        return result","Resolve an expression with possibly a dedicated expression resolvers.

        :param str name: expression resolver registered name. Default is the
            first registered expression resolver.
        :param bool safe: if True (Default), resolve in a safe context
            (without I/O).
        :param bool tostr: if True (False by default), transform the result into
            a string format.
        :param dict scope: scope execution resolution.
        :return: a string if tostr, otherwise, a python object.

        :raises: KeyError if no expression resolver has been registered or if
            name does not exist in expression resolvers."
"def load_raw_schema(data: Union[str, TextIO],
                    source_file: str=None,
                    source_file_date: str=None,
                    source_file_size: int=None,
                    base_dir: Optional[str]=None) -> SchemaDefinition:
    """""" Load and flatten SchemaDefinition from a file name, a URL or a block of text

    @param data: URL, file name or block of text
    @param source_file: Source file name for the schema
    @param source_file_date: timestamp of source file
    @param source_file_size: size of source file
    @param base_dir: Working directory of sources
    @return: Map from schema name to SchemaDefinition
    """"""
    if isinstance(data, str):
        if '\n' in data:
            return load_raw_schema((cast(TextIO, StringIO(data))))  # Not sure why typing doesn't see StringIO as TextIO
        elif '://' in data:
            # TODO: complete and test URL access
            req = Request(data)
            req.add_header(""Accept"", ""application/yaml, text/yaml;q=0.9"")
            with urlopen(req) as response:
                return load_raw_schema(response)
        else:
            fname = os.path.join(base_dir if base_dir else '', data)
            with open(fname) as f:
                return load_raw_schema(f, data, time.ctime(os.path.getmtime(fname)), os.path.getsize(fname))
    else:
        schemadefs = yaml.load(data, DupCheckYamlLoader)
        # Some schemas don't have an outermost identifier.  Construct one if necessary
        if 'name' in schemadefs:
            schemadefs = {schemadefs.pop('name'): schemadefs}
        elif 'id' in schemadefs:
            schemadefs = {schemadefs['id']: schemadefs}
        elif len(schemadefs) > 1 or not isinstance(list(schemadefs.values())[0], dict):
            schemadefs = {'Unnamed Schema': schemadefs}
        schema: SchemaDefinition = None
        for sname, sdef in {k: SchemaDefinition(name=k, **v) for k, v in schemadefs.items()}.items():
            if schema is None:
                schema = sdef
                schema.source_file = os.path.basename(source_file) if source_file else None
                schema.source_file_date = source_file_date
                schema.source_file_size = source_file_size
                schema.generation_date =  datetime.now().strftime(""%Y-%m-%d %H:%M"")
                schema.metamodel_version = metamodel_version
            else:
                merge_schemas(schema, sdef)
        return schema","Load and flatten SchemaDefinition from a file name, a URL or a block of text

    @param data: URL, file name or block of text
    @param source_file: Source file name for the schema
    @param source_file_date: timestamp of source file
    @param source_file_size: size of source file
    @param base_dir: Working directory of sources
    @return: Map from schema name to SchemaDefinition"
"def disconnect_async(self, conn_id, callback):
        """"""Asynchronously disconnect from a device that has previously been connected

        Args:
            conn_id (int): a unique identifier for this connection on the DeviceManager
                that owns this adapter.
            callback (callable): A function called as callback(conn_id, adapter_id, success, failure_reason)
            when the disconnection finishes.  Disconnection can only either succeed or timeout.
        """"""

        try:
            context = self.conns.get_context(conn_id)
        except ArgumentError:
            callback(conn_id, self.id, False, ""Could not find connection information"")
            return

        self.conns.begin_disconnection(conn_id, callback, self.get_config('default_timeout'))

        topics = context['topics']
        disconn_message = {'key': context['key'], 'client': self.name, 'type': 'command', 'operation': 'disconnect'}

        self.client.publish(topics.action, disconn_message)","Asynchronously disconnect from a device that has previously been connected

        Args:
            conn_id (int): a unique identifier for this connection on the DeviceManager
                that owns this adapter.
            callback (callable): A function called as callback(conn_id, adapter_id, success, failure_reason)
            when the disconnection finishes.  Disconnection can only either succeed or timeout."
"def start_output (self):
        """"""
        Write start of checking info as sql comment.
        """"""
        super(SQLLogger, self).start_output()
        if self.has_part(""intro""):
            self.write_intro()
            self.writeln()
            self.flush()",Write start of checking info as sql comment.
"def get_subfolder_queries(store, label_store, folders, fid, sid):
    '''Returns [unicode].

    This returns a list of queries that can be passed on to ""other""
    search engines. The list of queries is derived from the subfolder
    identified by ``fid/sid``.
    '''
    queries = []

    for cid, subid, url, stype, data in subtopics(store, folders, fid, sid):
        if stype in ('text', 'manual'):
            queries.append(data)
    return queries","Returns [unicode].

    This returns a list of queries that can be passed on to ""other""
    search engines. The list of queries is derived from the subfolder
    identified by ``fid/sid``."
"def startEdit( self ):
        """"""
        Rebuilds the pathing based on the parts.
        """"""
        self._originalText = self.text()
        self.scrollWidget().hide()
        self.setFocus()
        self.selectAll()",Rebuilds the pathing based on the parts.
"def unwire(awsclient, events, lambda_name, alias_name=ALIAS_NAME):
    """"""Unwire a list of event from an AWS Lambda function.

    'events' is a list of dictionaries, where the dict must contains the
    'schedule' of the event as string, and an optional 'name' and 'description'.

    :param awsclient:
    :param events: list of events
    :param lambda_name:
    :param alias_name:
    :return: exit_code
    """"""
    if not lambda_exists(awsclient, lambda_name):
        log.error(colored.red('The function you try to wire up doesn\'t ' +
                          'exist... Bailing out...'))
        return 1

    client_lambda = awsclient.get_client('lambda')
    lambda_function = client_lambda.get_function(FunctionName=lambda_name)
    lambda_arn = client_lambda.get_alias(FunctionName=lambda_name,
                                         Name=alias_name)['AliasArn']
    log.info('UN-wiring lambda_arn %s ' % lambda_arn)
    # TODO why load the policies here?
    '''
    policies = None
    try:
        result = client_lambda.get_policy(FunctionName=lambda_name,
                                          Qualifier=alias_name)
        policies = json.loads(result['Policy'])
    except ClientError as e:
        if e.response['Error']['Code'] == 'ResourceNotFoundException':
            log.warn(""Permission policies not found"")
        else:
            raise e
    '''

    if lambda_function is not None:
        #_unschedule_events(awsclient, events, lambda_arn)
        for event in events:
            evt_source = event['event_source']
            _remove_event_source(awsclient, evt_source, lambda_arn)
    return 0","Unwire a list of event from an AWS Lambda function.

    'events' is a list of dictionaries, where the dict must contains the
    'schedule' of the event as string, and an optional 'name' and 'description'.

    :param awsclient:
    :param events: list of events
    :param lambda_name:
    :param alias_name:
    :return: exit_code"
"def sort_url(self):
        """"""
        Return the URL to sort the linked table by this column. If the
        table is already sorted by this column, the order is reversed.

        Since there is no canonical URL for a table the current URL (via
        the HttpRequest linked to the Table instance) is reused, and any
        unrelated parameters will be included in the output.
        """"""

        prefix = (self.sort_direction == ""asc"") and ""-"" or """"
        return self.table.get_url(order_by=prefix + self.name)","Return the URL to sort the linked table by this column. If the
        table is already sorted by this column, the order is reversed.

        Since there is no canonical URL for a table the current URL (via
        the HttpRequest linked to the Table instance) is reused, and any
        unrelated parameters will be included in the output."
"def org_find_apps(org_id,
                  name=None,
                  name_mode='exact',
                  category=None,
                  all_versions=None,
                  published=None,
                  created_by=None,
                  developer=None,
                  authorized_user=None,
                  created_after=None,
                  created_before=None,
                  modified_after=None,
                  modified_before=None,
                  describe=False,
                  limit=None,
                  return_handler=False,
                  first_page_size=100,
                  **kwargs):
    """"""
    :param name: Name of the app (also see *name_mode*)
    :type name: string
    :param name_mode: Method by which to interpret the *name* field
        ""exact"": exact match,
        ""glob"": use ""*"" and ""?"" as wildcards,
        ""regexp"": interpret as a regular expression
    :type name_mode: string
    :param category: If specified, only returns apps that are in the specified category
    :type category: string
    :param all_versions: Whether to return all versions of each app or just the version tagged ""default""
    :type all_versions: boolean
    :param published: If specified, only returns results that have the specified publish status
        True for published apps,
        False for unpublished apps
    :type published: boolean
    :param created_by: If specified, only returns app versions that were created by the specified user
        (of the form ""user-USERNAME"")
    :type created_by: string
    :param developer: If specified, only returns apps for which the specified user (of the form ""user-USERNAME"")
        is a developer
    :type developer: string
    :param authorized_user: If specified, only returns apps for which the specified user (either a user ID, org ID,
        or the string ""PUBLIC"") appears in the app's list of authorized users
    :type authorized_user: string
    :param created_after: Timestamp after which each result was last created (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type created_after: int or string
    :param created_before: Timestamp before which each result was last created (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type created_before: int or string
    :param modified_after: Timestamp after which each result was last modified (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type modified_after: int or string
    :param modified_before: Timestamp before which each result was last modified (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type modified_before: int or string
    :param describe: Controls whether to also return the output of
        calling describe() on each app. Supply False to omit describe
        output, True to obtain the default describe output, or a dict to
        be supplied as the describe call input (which may be used to
        customize the set of fields that is returned)
    :type describe: bool or dict
    :param limit: The maximum number of results to be returned (if not specified, the number of results is unlimited)
    :type limit: int
    :param first_page_size: The number of results that the initial API call will return. Subsequent calls will raise
        this by multiplying by 2 up to a maximum of 1000.
    :type first_page_size: int
    :param return_handler: If True, yields results as dxpy object handlers (otherwise, yields each result as a dict
        with keys ""id"" and ""project"")
    :type return_handler: boolean
    :rtype: generator

    Returns a generator that yields all apps that match the query. It
    transparently handles paging through the result set if necessary.
    For all parameters that are omitted, the search is not restricted by
    the corresponding field.

    """"""

    query = {}
    if name is not None:
        if name_mode == 'exact':
            query['name'] = name
        elif name_mode == 'glob':
            query['name'] = {'glob': name}
        elif name_mode == 'regexp':
            query['name'] = {'regexp': name}
        else:
            raise DXError('find_apps: Unexpected value found for argument name_mode')
    if category is not None:
        query[""category""] = category
    if all_versions is not None:
        query[""allVersions""] = all_versions
    if published is not None:
        query[""published""] = published
    if created_by is not None:
        query[""createdBy""] = created_by
    if developer is not None:
        query[""developer""] = developer
    if authorized_user is not None:
        query[""authorizedUser""] = authorized_user
    if modified_after is not None or modified_before is not None:
        query[""modified""] = {}
        if modified_after is not None:
            query[""modified""][""after""] = dxpy.utils.normalize_time_input(modified_after)
        if modified_before is not None:
            query[""modified""][""before""] = dxpy.utils.normalize_time_input(modified_before)
    if created_after is not None or created_before is not None:
        query[""created""] = {}
        if created_after is not None:
            query[""created""][""after""] = dxpy.utils.normalize_time_input(created_after)
        if created_before is not None:
            query[""created""][""before""] = dxpy.utils.normalize_time_input(created_before)
    if describe is not None and describe is not False:
        query[""describe""] = describe
    if limit is not None:
        query[""limit""] = limit

    return _org_find(dxpy.api.org_find_apps, org_id, query)",":param name: Name of the app (also see *name_mode*)
    :type name: string
    :param name_mode: Method by which to interpret the *name* field
        ""exact"": exact match,
        ""glob"": use ""*"" and ""?"" as wildcards,
        ""regexp"": interpret as a regular expression
    :type name_mode: string
    :param category: If specified, only returns apps that are in the specified category
    :type category: string
    :param all_versions: Whether to return all versions of each app or just the version tagged ""default""
    :type all_versions: boolean
    :param published: If specified, only returns results that have the specified publish status
        True for published apps,
        False for unpublished apps
    :type published: boolean
    :param created_by: If specified, only returns app versions that were created by the specified user
        (of the form ""user-USERNAME"")
    :type created_by: string
    :param developer: If specified, only returns apps for which the specified user (of the form ""user-USERNAME"")
        is a developer
    :type developer: string
    :param authorized_user: If specified, only returns apps for which the specified user (either a user ID, org ID,
        or the string ""PUBLIC"") appears in the app's list of authorized users
    :type authorized_user: string
    :param created_after: Timestamp after which each result was last created (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type created_after: int or string
    :param created_before: Timestamp before which each result was last created (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type created_before: int or string
    :param modified_after: Timestamp after which each result was last modified (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type modified_after: int or string
    :param modified_before: Timestamp before which each result was last modified (see note accompanying
        :meth:`find_data_objects()` for interpretation)
    :type modified_before: int or string
    :param describe: Controls whether to also return the output of
        calling describe() on each app. Supply False to omit describe
        output, True to obtain the default describe output, or a dict to
        be supplied as the describe call input (which may be used to
        customize the set of fields that is returned)
    :type describe: bool or dict
    :param limit: The maximum number of results to be returned (if not specified, the number of results is unlimited)
    :type limit: int
    :param first_page_size: The number of results that the initial API call will return. Subsequent calls will raise
        this by multiplying by 2 up to a maximum of 1000.
    :type first_page_size: int
    :param return_handler: If True, yields results as dxpy object handlers (otherwise, yields each result as a dict
        with keys ""id"" and ""project"")
    :type return_handler: boolean
    :rtype: generator

    Returns a generator that yields all apps that match the query. It
    transparently handles paging through the result set if necessary.
    For all parameters that are omitted, the search is not restricted by
    the corresponding field."
"def reload_components_ui(self):
        """"""
        Reloads user selected Components.

        :return: Method success.
        :rtype: bool

        :note: May require user interaction.
        """"""

        selected_components = self.get_selected_components()

        self.__engine.start_processing(""Reloading Components ..."", len(selected_components))
        reload_failed_components = []
        for component in selected_components:
            if component.interface.deactivatable:
                success = self.reload_component(component.name) or False
                if not success:
                    reload_failed_components.append(component)
            else:
                self.__engine.notifications_manager.warnify(
                    ""{0} | '{1}' Component cannot be deactivated and won't be reloaded!"".format(self.__class__.__name__,
                                                                                                component.name))
            self.__engine.step_processing()
        self.__engine.stop_processing()

        if not reload_failed_components:
            return True
        else:
            raise manager.exceptions.ComponentReloadError(
                ""{0} | Exception(s) raised while reloading '{1}' Component(s)!"".format(self.__class__.__name__,
                                                                                       "", "".join(
                                                                                           (reload_failed_component.name
                                                                                            for reload_failed_component
                                                                                            in
                                                                                            reload_failed_components))))","Reloads user selected Components.

        :return: Method success.
        :rtype: bool

        :note: May require user interaction."
"def codes_get_long_array(handle, key, size):
    # type: (cffi.FFI.CData, str, int) -> T.List[int]
    """"""
    Get long array values from a key.

    :param bytes key: the keyword whose value(s) are to be extracted

    :rtype: List(int)
    """"""
    values = ffi.new('long[]', size)
    size_p = ffi.new('size_t *', size)
    _codes_get_long_array(handle, key.encode(ENC), values, size_p)
    return list(values)","Get long array values from a key.

    :param bytes key: the keyword whose value(s) are to be extracted

    :rtype: List(int)"
"def to_annot(self, annot, name=None, chan=None):
        """"""Write events to Annotations file.
        
        Parameters
        ----------
        annot : instance of Annotations
            Annotations file
        name : str
            name for the event type        
        """"""
        annot.add_events(self.events, name=name, chan=chan)","Write events to Annotations file.
        
        Parameters
        ----------
        annot : instance of Annotations
            Annotations file
        name : str
            name for the event type"
"def ports(self):
        """"""
        :return: dict
            {
                # container -> host
                ""1234"": ""2345""
            }
        """"""

        if self._ports is None:
            self._ports = {}
            if self.net_settings[""Ports""]:
                for key, value in self.net_settings[""Ports""].items():
                    cleaned_port = key.split(""/"")[0]
                    self._ports[cleaned_port] = graceful_chain_get(value, 0, ""HostPort"")
            # in case of --net=host, there's nothing in network settings, let's get it from ""Config""
            exposed_ports_section = graceful_chain_get(self.inspect_data, ""Config"", ""ExposedPorts"")
            if exposed_ports_section:
                for key, value in exposed_ports_section.items():
                    cleaned_port = key.split(""/"")[0]
                    self._ports[cleaned_port] = None  # extremely docker specific
        return self._ports",":return: dict
            {
                # container -> host
                ""1234"": ""2345""
            }"
"def put_pidfile( pidfile_path, pid ):
    """"""
    Put a PID into a pidfile
    """"""
    with open( pidfile_path, ""w"" ) as f:
        f.write(""%s"" % pid)
        os.fsync(f.fileno())

    return",Put a PID into a pidfile
"def contains_locked_file(directory: str):
    """"""
    :return: True if any of the files in the directory are in use. For example, if the dll is injected
    into the game, this will definitely return true.
    """"""
    for root, subdirs, files in os.walk(directory):
        for filename in files:
            file_path = os.path.join(root, filename)
            try:
                with open(file_path, 'a'):
                    pass
            except IOError:
                logger.debug(f""Locked file: {file_path}"")
                return True
    return False",":return: True if any of the files in the directory are in use. For example, if the dll is injected
    into the game, this will definitely return true."
"def copy_binder_files(app, exception):
    """"""Copy all Binder requirements and notebooks files.""""""
    if exception is not None:
        return

    if app.builder.name not in ['html', 'readthedocs']:
        return

    gallery_conf = app.config.sphinx_gallery_conf
    binder_conf = check_binder_conf(gallery_conf.get('binder'))

    if not len(binder_conf) > 0:
        return

    logger.info('copying binder requirements...', color='white')
    _copy_binder_reqs(app, binder_conf)
    _copy_binder_notebooks(app)",Copy all Binder requirements and notebooks files.
"def dot(a, b):
    """"""Take arrays `a` and `b` and form the dot product between the last axis
    of `a` and the first of `b`.
    """"""
    b = numpy.asarray(b)
    return numpy.dot(a, b.reshape(b.shape[0], -1)).reshape(a.shape[:-1] + b.shape[1:])","Take arrays `a` and `b` and form the dot product between the last axis
    of `a` and the first of `b`."
"def jwt_required(fn):
    """"""
    If you decorate a view with this, it will ensure that the requester has a
    valid JWT before calling the actual view.

    :param fn: The view function to decorate
    """"""
    @wraps(fn)
    def wrapper(*args, **kwargs):
        jwt_data = _decode_jwt_from_headers()
        ctx_stack.top.jwt = jwt_data
        return fn(*args, **kwargs)
    return wrapper","If you decorate a view with this, it will ensure that the requester has a
    valid JWT before calling the actual view.

    :param fn: The view function to decorate"
"def onscroll(self, event):
        """"""Action to be taken when an event is triggered.

        Event is scroll of the mouse's wheel. This leads to changing the temporal frame displayed.

        :param event: Scroll of mouse wheel
        """"""
        if event.button == 'up':
            self.ind = (self.ind + 1) % self.slices
        else:
            self.ind = (self.ind - 1) % self.slices
        self.update()","Action to be taken when an event is triggered.

        Event is scroll of the mouse's wheel. This leads to changing the temporal frame displayed.

        :param event: Scroll of mouse wheel"
"def reassign(self, user_ids, requester):
        """"""Reassign this incident to a user or list of users

        :param user_ids: A non-empty list of user ids
        :param requester: The email address of individual requesting reassign
        """"""
        path = '{0}'.format(self.collection.name)
        assignments = []
        if not user_ids:
            raise Error('Must pass at least one user id')
        for user_id in user_ids:
            ref = {
                ""assignee"": {
                    ""id"": user_id,
                    ""type"": ""user_reference""
                }
            }
            assignments.append(ref)
        data = {
            ""incidents"": [
                {
                    ""id"": self.id,
                    ""type"": ""incident_reference"",
                    ""assignments"": assignments
                }
            ]
        }
        extra_headers = {""From"": requester}
        return self.pagerduty.request('PUT', path, data=_json_dumper(data), extra_headers=extra_headers)","Reassign this incident to a user or list of users

        :param user_ids: A non-empty list of user ids
        :param requester: The email address of individual requesting reassign"
"def join_paths(fnames:FilePathList, path:PathOrStr='.')->Collection[Path]:
    ""Join `path` to every file name in `fnames`.""
    path = Path(path)
    return [join_path(o,path) for o in fnames]",Join `path` to every file name in `fnames`.
"def inspect_node(self, index):
        """"""Inspect the graph node at the given index.""""""

        if index >= len(self.graph.nodes):
            raise RPCErrorCode(6)  #FIXME: use actual error code here for UNKNOWN_ERROR status

        return create_binary_descriptor(str(self.graph.nodes[index]))",Inspect the graph node at the given index.
"def checkarity(func, args, kwargs, discount=0):
    '''Check if arguments respect a given function arity and return
    an error message if the check did not pass,
    otherwise it returns ``None``.

    :parameter func: the function.
    :parameter args: function arguments.
    :parameter kwargs: function key-valued parameters.
    :parameter discount: optional integer which discount the number of
                         positional argument to check. Default ``0``.
    '''
    spec = inspect.getargspec(func)
    self = getattr(func, '__self__', None)
    if self and spec.args:
        discount += 1
    args = list(args)
    defaults = list(spec.defaults or ())
    len_defaults = len(defaults)
    len_args = len(spec.args) - discount
    len_args_input = len(args)
    minlen = len_args - len_defaults
    totlen = len_args_input + len(kwargs)
    maxlen = len_args
    if spec.varargs or spec.keywords:
        maxlen = None
        if not minlen:
            return

    if not spec.defaults and maxlen:
        start = '""{0}"" takes'.format(func.__name__)
    else:
        if maxlen and totlen > maxlen:
            start = '""{0}"" takes at most'.format(func.__name__)
        else:
            start = '""{0}"" takes at least'.format(func.__name__)

    if totlen < minlen:
        return '{0} {1} parameters. {2} given.'.format(start, minlen, totlen)
    elif maxlen and totlen > maxlen:
        return '{0} {1} parameters. {2} given.'.format(start, maxlen, totlen)

    # Length of parameter OK, check names
    if len_args_input < len_args:
        le = minlen - len_args_input
        for arg in spec.args[discount:]:
            if args:
                args.pop(0)
            else:
                if le > 0:
                    if defaults:
                        defaults.pop(0)
                    elif arg not in kwargs:
                        return ('""{0}"" has missing ""{1}"" parameter.'
                                .format(func.__name__, arg))
                kwargs.pop(arg, None)
            le -= 1
        if kwargs and maxlen:
            s = ''
            if len(kwargs) > 1:
                s = 's'
            p = ', '.join('""{0}""'.format(p) for p in kwargs)
            return ('""{0}"" does not accept {1} parameter{2}.'
                    .format(func.__name__, p, s))
    elif len_args_input > len_args + len_defaults:
        n = len_args + len_defaults
        start = '""{0}"" takes'.format(func.__name__)
        return ('{0} {1} positional parameters. {2} given.'
                .format(start, n, len_args_input))","Check if arguments respect a given function arity and return
    an error message if the check did not pass,
    otherwise it returns ``None``.

    :parameter func: the function.
    :parameter args: function arguments.
    :parameter kwargs: function key-valued parameters.
    :parameter discount: optional integer which discount the number of
                         positional argument to check. Default ``0``."
"def expand(template, variables=None):
    """"""
    Expand a URL template string using the passed variables
    """"""
    if variables is None:
        variables = {}
    return patterns.sub(functools.partial(_replace, variables), template)",Expand a URL template string using the passed variables
"def as_dictionary(self):
        """"""Return the key as a python dictionary.""""""
        values = {
            'id': self._id,
            'type': self._type
        }

        if self._owner:
            values['owner'] = self._owner
        return values",Return the key as a python dictionary.
"def get_pwhash_bits(params):
    """""" Extract bits for password hash validation from params. """"""
    if not ""pwhash"" in params or \
            not ""nonce"" in params or \
            not ""aead"" in params or \
            not ""kh"" in params:
        raise Exception(""Missing required parameter in request (pwhash, nonce, aead or kh)"")
    pwhash = params[""pwhash""][0]
    nonce = params[""nonce""][0]
    aead = params[""aead""][0]
    key_handle = pyhsm.util.key_handle_to_int(params[""kh""][0])
    return pwhash, nonce, aead, key_handle",Extract bits for password hash validation from params.
"def display_stats(request, stats, queries):
    """"""
    Generate a HttpResponse of functions for a profiling run.

    _stats_ should contain a pstats.Stats of a hotshot session.
    _queries_ should contain a list of SQL queries.
    """"""
    sort = [
        request.REQUEST.get('sort_first', 'time'),
        request.REQUEST.get('sort_second', 'calls')
    ]
    fmt = request.REQUEST.get('format', 'print_stats')
    sort_first_buttons = RadioButtons('sort_first', sort[0], sort_categories)
    sort_second_buttons = RadioButtons('sort_second', sort[1], sort_categories)
    format_buttons = RadioButtons('format', fmt, (
        ('print_stats', 'by function'),
        ('print_callers', 'by callers'),
        ('print_callees', 'by callees')
    ))
    output = render_stats(stats, sort, fmt)
    output.reset()
    output = [html.escape(unicode(line)) for line in output.readlines()]
    response = HttpResponse(content_type='text/html; charset=utf-8')
    response.content = (stats_template % {
        'format_buttons': format_buttons,
        'sort_first_buttons': sort_first_buttons,
        'sort_second_buttons': sort_second_buttons,
        'rawqueries' : b64encode(cPickle.dumps(queries)),
        'rawstats': b64encode(pickle_stats(stats)),
        'stats': """".join(output),
        'url': request.path
    })
    return response","Generate a HttpResponse of functions for a profiling run.

    _stats_ should contain a pstats.Stats of a hotshot session.
    _queries_ should contain a list of SQL queries."
"def gen_polygon_pts(n_pts=3, radius=[1.0]):
    '''Generate points for a polygon with a number of radiuses.
This makes it easy to generate shapes with an arbitrary number of sides,
  regularly angled around the origin.
A single radius will give a simple shape such as a square, hexagon, etc.
Multiple radiuses will give complex shapes like stars, gear wheels, ratchet
  wheels, etc.
    '''
    assert isinstance(n_pts, int) and n_pts > 0
    assert isinstance(radius, list)
    l_rad = len(radius)
    assert l_rad > 0
    for i in radius:
        assert isinstance(i, float)

    return [pt_rotate((radius[i % l_rad], 0.0), [i*2*pi/n_pts]) \
            for i in range(n_pts)]","Generate points for a polygon with a number of radiuses.
This makes it easy to generate shapes with an arbitrary number of sides,
  regularly angled around the origin.
A single radius will give a simple shape such as a square, hexagon, etc.
Multiple radiuses will give complex shapes like stars, gear wheels, ratchet
  wheels, etc."
"def extract_string_pairs_in_directory(directory_path, extract_func, filter_func):
    """""" Retrieves all string pairs in the directory

    Args:
        directory_path (str): The path of the directory containing the file to extract string pairs from.
        extract_func (function): Function for extracting the localization keys and comments from the files.
            The extract function receives 2 parameters:
            - dict that the keys (a key in the dict) and comments (a value in the dict) are added to.
            - str representing file path

        filter_func (function): Function for filtering files in the directory.
            The filter function receives the file name and returns a bool representing the filter result.
            True if the file name passed the filter, False otherwise.

    Returns:
        dict: A mapping between string pairs first value (probably the key), and the second value (probably the comment).
    """"""
    result = {}
    for root, dirnames, filenames in os.walk(directory_path):
        for file_name in filenames:
            if filter_func(file_name):
                file_path = os.path.join(root, file_name)
                try:
                    extract_func(result, file_path)
                except Exception as e:
                    print ""Error in file "" + file_name
                    print e
    return result","Retrieves all string pairs in the directory

    Args:
        directory_path (str): The path of the directory containing the file to extract string pairs from.
        extract_func (function): Function for extracting the localization keys and comments from the files.
            The extract function receives 2 parameters:
            - dict that the keys (a key in the dict) and comments (a value in the dict) are added to.
            - str representing file path

        filter_func (function): Function for filtering files in the directory.
            The filter function receives the file name and returns a bool representing the filter result.
            True if the file name passed the filter, False otherwise.

    Returns:
        dict: A mapping between string pairs first value (probably the key), and the second value (probably the comment)."
"def __select(self, method, url, headers=None, cookies=None, timeout=60, verify=False, proxies=None,
                 allow_redirects=True, encoding='utf-8', params=None, form_data=None, stream=False):
        """"""
        httpgethttp get
        post, http post
        :param method: 'get' or 'post'
        :param url: Url
        :param headers: 
        :param cookies: cookies
        :param timeout: 
        :param verify: ssl
        :param proxies: 
        :param allow_redirects: 
        :param encoding: html
        :param params: 
        :param form_data: post
        :param stream: 
        :return: (html, cookie)  
        """"""
        # historyurl
        # 
        self.filter_duplicate(url) 
        r = None
        if method == 'get':
            if self.session:
                r = self.session.get(url, headers=odict(headers), cookies=cookies, timeout=timeout,
                                     verify=verify, proxies=proxies, allow_redirects=allow_redirects, params=params)
            else:
                r = requests.get(url, headers=odict(headers), cookies=cookies, timeout=timeout, verify=verify,
                                 proxies=proxies, allow_redirects=allow_redirects, params=params)
        elif method == 'post':
            if self.session:
                r = self.session.post(url, headers=odict(headers), cookies=cookies, timeout=timeout,
                                      data=form_data, verify=verify, proxies=proxies, allow_redirects=allow_redirects,
                                      params=params)
            else:
                r = requests.post(url, headers=odict(headers), cookies=cookies, timeout=timeout, data=form_data,
                                  verify=verify, proxies=proxies, allow_redirects=allow_redirects, params=params)
        else:
            raise Exception('unsupported http method')

        # http
        r.raise_for_status()
        r.encoding = encoding  # html
        if stream:
            # 
            return r
        return r.text, r.headers, dict_from_cookiejar(r.cookies), r.history, r.elapsed.microseconds / 1000000","httpgethttp get
        post, http post
        :param method: 'get' or 'post'
        :param url: Url
        :param headers: 
        :param cookies: cookies
        :param timeout: 
        :param verify: ssl
        :param proxies: 
        :param allow_redirects: 
        :param encoding: html
        :param params: 
        :param form_data: post
        :param stream: 
        :return: (html, cookie)  "
"def get(self):
        """"""Get graphics options.""""""
        return {k:v for k,v in list(self.options.items()) if k in self._allowed_graphics}",Get graphics options.
"def get_quality(self, qual_idx=None):
        """"""Get the signal qualifier, using shortcuts or combobox.""""""
        if self.annot is None:  # remove if buttons are disabled
            msg = 'No score file loaded'
            error_dialog = QErrorMessage()
            error_dialog.setWindowTitle('Error getting quality')
            error_dialog.showMessage(msg)
            error_dialog.exec()
            lg.debug(msg)
            return

        window_start = self.parent.value('window_start')
        window_length = self.parent.value('window_length')

        try:
            self.annot.set_stage_for_epoch(window_start,
                                           QUALIFIERS[qual_idx],
                                           attr='quality')

        except KeyError:
            msg = ('The start of the window does not correspond to any epoch ' +
                   'in sleep scoring file')
            error_dialog = QErrorMessage()
            error_dialog.setWindowTitle('Error getting quality')
            error_dialog.showMessage(msg)
            error_dialog.exec()
            lg.debug(msg)

        else:
            lg.debug('User staged ' + str(window_start) + ' as ' +
                     QUALIFIERS[qual_idx])

            self.set_quality_index()
            self.parent.overview.mark_quality(window_start, window_length,
                                              QUALIFIERS[qual_idx])
            self.display_stats()
            self.parent.traces.page_next()","Get the signal qualifier, using shortcuts or combobox."
"def search(self, query, category=None, orientation=None, page=1, per_page=10):
        """"""
        Get a single page from a photo search.
        Optionally limit your search to a set of categories by supplying the category IDs.

        Note: If supplying multiple category IDs,
        the resulting photos will be those that match all of the given categories,
        not ones that match any category.

        :param query [string]: Search terms.
        :param category [string]: Category ID(s) to filter search. If multiple, comma-separated. (deprecated)
        :param orientation [string]: Filter search results by photo orientation.
        Valid values are landscape, portrait, and squarish.
        :param page [integer]: Page number to retrieve. (Optional; default: 1)
        :param per_page [integer]: Number of items per page. (Optional; default: 10)
        :return: [Array]: A single page of the curated Photo list.
        :raise UnsplashError: If the given orientation is not in the default orientation values.
        """"""
        if orientation and orientation not in self.orientation_values:
            raise Exception()
        params = {
            ""query"": query,
            ""category"": category,
            ""orientation"": orientation,
            ""page"": page,
            ""per_page"": per_page
        }
        url = ""/photos/search""
        result = self._get(url, params=params)
        return PhotoModel.parse_list(result)","Get a single page from a photo search.
        Optionally limit your search to a set of categories by supplying the category IDs.

        Note: If supplying multiple category IDs,
        the resulting photos will be those that match all of the given categories,
        not ones that match any category.

        :param query [string]: Search terms.
        :param category [string]: Category ID(s) to filter search. If multiple, comma-separated. (deprecated)
        :param orientation [string]: Filter search results by photo orientation.
        Valid values are landscape, portrait, and squarish.
        :param page [integer]: Page number to retrieve. (Optional; default: 1)
        :param per_page [integer]: Number of items per page. (Optional; default: 10)
        :return: [Array]: A single page of the curated Photo list.
        :raise UnsplashError: If the given orientation is not in the default orientation values."
"def page(self, actor_sid=values.unset, event_type=values.unset,
             resource_sid=values.unset, source_ip_address=values.unset,
             start_date=values.unset, end_date=values.unset,
             page_token=values.unset, page_number=values.unset,
             page_size=values.unset):
        """"""
        Retrieve a single page of EventInstance records from the API.
        Request is executed immediately

        :param unicode actor_sid: Only include Events initiated by this Actor
        :param unicode event_type: Only include Events of this EventType
        :param unicode resource_sid: Only include Events referring to this resource
        :param unicode source_ip_address: Only include Events that originated from this IP address
        :param datetime start_date: Only show events on or after this date
        :param datetime end_date: Only show events on or before this date
        :param str page_token: PageToken provided by the API
        :param int page_number: Page Number, this value is simply for client state
        :param int page_size: Number of records to return, defaults to 50

        :returns: Page of EventInstance
        :rtype: twilio.rest.monitor.v1.event.EventPage
        """"""
        params = values.of({
            'ActorSid': actor_sid,
            'EventType': event_type,
            'ResourceSid': resource_sid,
            'SourceIpAddress': source_ip_address,
            'StartDate': serialize.iso8601_datetime(start_date),
            'EndDate': serialize.iso8601_datetime(end_date),
            'PageToken': page_token,
            'Page': page_number,
            'PageSize': page_size,
        })

        response = self._version.page(
            'GET',
            self._uri,
            params=params,
        )

        return EventPage(self._version, response, self._solution)","Retrieve a single page of EventInstance records from the API.
        Request is executed immediately

        :param unicode actor_sid: Only include Events initiated by this Actor
        :param unicode event_type: Only include Events of this EventType
        :param unicode resource_sid: Only include Events referring to this resource
        :param unicode source_ip_address: Only include Events that originated from this IP address
        :param datetime start_date: Only show events on or after this date
        :param datetime end_date: Only show events on or before this date
        :param str page_token: PageToken provided by the API
        :param int page_number: Page Number, this value is simply for client state
        :param int page_size: Number of records to return, defaults to 50

        :returns: Page of EventInstance
        :rtype: twilio.rest.monitor.v1.event.EventPage"
"def get_reaction(self, reactants, products):
        """"""
        Gets a reaction from the Materials Project.

        Args:
            reactants ([str]): List of formulas
            products ([str]): List of formulas

        Returns:
            rxn
        """"""
        return self._make_request(""/reaction"",
                                  payload={""reactants[]"": reactants,
                                           ""products[]"": products}, mp_decode=False)","Gets a reaction from the Materials Project.

        Args:
            reactants ([str]): List of formulas
            products ([str]): List of formulas

        Returns:
            rxn"
"def entrez(args):
    """"""
    %prog entrez <filename|term>

    `filename` contains a list of terms to search. Or just one term. If the
    results are small in size, e.g. ""--format=acc"", use ""--batchsize=100"" to speed
    the download.
    """"""
    p = OptionParser(entrez.__doc__)

    allowed_databases = {""fasta"": [""genome"", ""nuccore"", ""nucgss"", ""protein"", ""nucest""],
                         ""asn.1"": [""genome"", ""nuccore"", ""nucgss"", ""protein"", ""gene""],
                         ""xml"": [""genome"", ""nuccore"", ""nucgss"", ""nucest"", ""gene""],
                         ""gb"": [""genome"", ""nuccore"", ""nucgss""],
                         ""est"": [""nucest""],
                         ""gss"": [""nucgss""],
                         ""acc"": [""nuccore""],
                         }

    valid_formats = tuple(allowed_databases.keys())
    valid_databases = (""genome"", ""nuccore"", ""nucest"",
                       ""nucgss"", ""protein"", ""gene"")

    p.add_option(""--noversion"", dest=""noversion"",
                 default=False, action=""store_true"",
                 help=""Remove trailing accession versions"")
    p.add_option(""--format"", default=""fasta"", choices=valid_formats,
                 help=""download format [default: %default]"")
    p.add_option(""--database"", default=""nuccore"", choices=valid_databases,
                 help=""search database [default: %default]"")
    p.add_option(""--retmax"", default=1000000, type=""int"",
                 help=""how many results to return [default: %default]"")
    p.add_option(""--skipcheck"", default=False, action=""store_true"",
                 help=""turn off prompt to check file existence [default: %default]"")
    p.add_option(""--batchsize"", default=500, type=""int"",
                 help=""download the results in batch for speed-up [default: %default]"")
    p.set_outdir(outdir=None)
    p.add_option(""--outprefix"", default=""out"",
                 help=""output file name prefix [default: %default]"")
    p.set_email()
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(p.print_help())

    filename, = args
    if op.exists(filename):
        pf = filename.rsplit(""."", 1)[0]
        list_of_terms = [row.strip() for row in open(filename)]
        if opts.noversion:
            list_of_terms = [x.rsplit(""."", 1)[0] for x in list_of_terms]
    else:
        pf = filename
        # the filename is the search term
        list_of_terms = [filename.strip()]

    fmt = opts.format
    database = opts.database
    batchsize = opts.batchsize

    assert database in allowed_databases[fmt], \
        ""For output format '{0}', allowed databases are: {1}"".\
        format(fmt, allowed_databases[fmt])
    assert batchsize >= 1, ""batchsize must >= 1""

    if "" "" in pf:
        pf = opts.outprefix

    outfile = ""{0}.{1}"".format(pf, fmt)

    outdir = opts.outdir
    if outdir:
        mkdir(outdir)

    # If noprompt, will not check file existence
    if not outdir:
        fw = must_open(outfile, ""w"", checkexists=True,
                       skipcheck=opts.skipcheck)
        if fw is None:
            return

    seen = set()
    totalsize = 0
    for id, size, term, handle in batch_entrez(list_of_terms, retmax=opts.retmax,
                                               rettype=fmt, db=database, batchsize=batchsize,
                                               email=opts.email):
        if outdir:
            outfile = urljoin(outdir, ""{0}.{1}"".format(term, fmt))
            fw = must_open(outfile, ""w"", checkexists=True,
                           skipcheck=opts.skipcheck)
            if fw is None:
                continue

        rec = handle.read()
        if id in seen:
            logging.error(""Duplicate key ({0}) found"".format(rec))
            continue

        totalsize += size
        print(rec, file=fw)
        print(file=fw)

        seen.add(id)

    if seen:
        print(""A total of {0} {1} records downloaded."".
              format(totalsize, fmt.upper()), file=sys.stderr)

    return outfile","%prog entrez <filename|term>

    `filename` contains a list of terms to search. Or just one term. If the
    results are small in size, e.g. ""--format=acc"", use ""--batchsize=100"" to speed
    the download."
"def rotation_from_matrix(matrix):
    """"""Return rotation angle and axis from rotation matrix.

    >>> angle = (random.random() - 0.5) * (2*math.pi)
    >>> direc = numpy.random.random(3) - 0.5
    >>> point = numpy.random.random(3) - 0.5
    >>> R0 = rotation_matrix(angle, direc, point)
    >>> angle, direc, point = rotation_from_matrix(R0)
    >>> R1 = rotation_matrix(angle, direc, point)
    >>> is_same_transform(R0, R1)
    True

    """"""
    R = numpy.array(matrix, dtype=numpy.float64, copy=False)
    R33 = R[:3, :3]
    # direction: unit eigenvector of R33 corresponding to eigenvalue of 1
    w, W = numpy.linalg.eig(R33.T)
    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]
    if not len(i):
        raise ValueError(""no unit eigenvector corresponding to eigenvalue 1"")
    direction = numpy.real(W[:, i[-1]]).squeeze()
    # point: unit eigenvector of R33 corresponding to eigenvalue of 1
    w, Q = numpy.linalg.eig(R)
    i = numpy.where(abs(numpy.real(w) - 1.0) < 1e-8)[0]
    if not len(i):
        raise ValueError(""no unit eigenvector corresponding to eigenvalue 1"")
    point = numpy.real(Q[:, i[-1]]).squeeze()
    point /= point[3]
    # rotation angle depending on direction
    cosa = (numpy.trace(R33) - 1.0) / 2.0
    if abs(direction[2]) > 1e-8:
        sina = (R[1, 0] + (cosa-1.0)*direction[0]*direction[1]) / direction[2]
    elif abs(direction[1]) > 1e-8:
        sina = (R[0, 2] + (cosa-1.0)*direction[0]*direction[2]) / direction[1]
    else:
        sina = (R[2, 1] + (cosa-1.0)*direction[1]*direction[2]) / direction[0]
    angle = math.atan2(sina, cosa)
    return angle, direction, point","Return rotation angle and axis from rotation matrix.

    >>> angle = (random.random() - 0.5) * (2*math.pi)
    >>> direc = numpy.random.random(3) - 0.5
    >>> point = numpy.random.random(3) - 0.5
    >>> R0 = rotation_matrix(angle, direc, point)
    >>> angle, direc, point = rotation_from_matrix(R0)
    >>> R1 = rotation_matrix(angle, direc, point)
    >>> is_same_transform(R0, R1)
    True"
"def random_set_distribution(
    rnd: Optional[tcod.random.Random], dist: int
) -> None:
    """"""Change the distribution mode of a random number generator.

    Args:
        rnd (Optional[Random]): A Random instance, or None to use the default.
        dist (int): The distribution mode to use.  Should be DISTRIBUTION_*.
    """"""
    lib.TCOD_random_set_distribution(rnd.random_c if rnd else ffi.NULL, dist)","Change the distribution mode of a random number generator.

    Args:
        rnd (Optional[Random]): A Random instance, or None to use the default.
        dist (int): The distribution mode to use.  Should be DISTRIBUTION_*."
"def edit_config_input_edit_content_url_url(self, **kwargs):
        """"""Auto Generated Code
        """"""
        config = ET.Element(""config"")
        edit_config = ET.Element(""edit_config"")
        config = edit_config
        input = ET.SubElement(edit_config, ""input"")
        edit_content = ET.SubElement(input, ""edit-content"")
        url = ET.SubElement(edit_content, ""url"")
        url = ET.SubElement(url, ""url"")
        url.text = kwargs.pop('url')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)",Auto Generated Code
"def setOverlayRenderingPid(self, ulOverlayHandle, unPID):
        """"""
        Sets the pid that is allowed to render to this overlay (the creator pid is always allow to render),
        by default this is the pid of the process that made the overlay
        """"""

        fn = self.function_table.setOverlayRenderingPid
        result = fn(ulOverlayHandle, unPID)
        return result","Sets the pid that is allowed to render to this overlay (the creator pid is always allow to render),
        by default this is the pid of the process that made the overlay"
"def get_url_rev_options(self, url):
        # type: (str) -> Tuple[str, RevOptions]
        """"""
        Return the URL and RevOptions object to use in obtain() and in
        some cases export(), as a tuple (url, rev_options).
        """"""
        url, rev, user_pass = self.get_url_rev_and_auth(url)
        username, password = user_pass
        extra_args = self.make_rev_args(username, password)
        rev_options = self.make_rev_options(rev, extra_args=extra_args)

        return url, rev_options","Return the URL and RevOptions object to use in obtain() and in
        some cases export(), as a tuple (url, rev_options)."
"def find_sources_in_image(self, filename, hdu_index=0, outfile=None, rms=None, bkg=None, max_summits=None, innerclip=5,
                              outerclip=4, cores=None, rmsin=None, bkgin=None, beam=None, doislandflux=False,
                              nopositive=False, nonegative=False, mask=None, lat=None, imgpsf=None, blank=False,
                              docov=True, cube_index=None):
        """"""
        Run the Aegean source finder.


        Parameters
        ----------
        filename : str or HDUList
            Image filename or HDUList.

        hdu_index : int
            The index of the FITS HDU (extension).

        outfile : str
            file for printing catalog (NOT a table, just a text file of my own design)

        rms : float
            Use this rms for the entire image (will also assume that background is 0)

        max_summits : int
            Fit up to this many components to each island (extras are included but not fit)

        innerclip, outerclip : float
            The seed (inner) and flood (outer) clipping level (sigmas).

        cores : int
            Number of CPU cores to use. None means all cores.

        rmsin, bkgin : str or HDUList
            Filename or HDUList for the noise and background images.
            If either are None, then it will be calculated internally.

        beam : (major, minor, pa)
            Floats representing the synthesised beam (degrees).
            Replaces whatever is given in the FITS header.
            If the FITS header has no BMAJ/BMIN then this is required.

        doislandflux : bool
            If True then each island will also be characterized.

        nopositive, nonegative : bool
            Whether to return positive or negative sources.
            Default nopositive=False, nonegative=True.

        mask : str
            The filename of a region file created by MIMAS.
            Islands outside of this region will be ignored.

        lat : float
            The latitude of the telescope (declination of zenith).

        imgpsf : str or HDUList
             Filename or HDUList for a psf image.

        blank : bool
            Cause the output image to be blanked where islands are found.

        docov : bool
            If True then include covariance matrix in the fitting process. (default=True)

        cube_index : int
            For image cubes, cube_index determines which slice is used.

        Returns
        -------
        sources : list
            List of sources found.
        """"""

        # Tell numpy to be quiet
        np.seterr(invalid='ignore')
        if cores is not None:
            if not (cores >= 1): raise AssertionError(""cores must be one or more"")

        self.load_globals(filename, hdu_index=hdu_index, bkgin=bkgin, rmsin=rmsin, beam=beam, rms=rms, bkg=bkg, cores=cores,
                          verb=True, mask=mask, lat=lat, psf=imgpsf, blank=blank, docov=docov, cube_index=cube_index)
        global_data = self.global_data
        rmsimg = global_data.rmsimg
        data = global_data.data_pix

        self.log.info(""beam = {0:5.2f}'' x {1:5.2f}'' at {2:5.2f}deg"".format(
            global_data.beam.a * 3600, global_data.beam.b * 3600, global_data.beam.pa))
        # stop people from doing silly things.
        if outerclip > innerclip:
            outerclip = innerclip
        self.log.info(""seedclip={0}"".format(innerclip))
        self.log.info(""floodclip={0}"".format(outerclip))

        isle_num = 0

        if cores == 1:  # single-threaded, no parallel processing
            queue = []
        else:
            queue = pprocess.Queue(limit=cores, reuse=1)
            fit_parallel = queue.manage(pprocess.MakeReusable(self._fit_islands))

        island_group = []
        group_size = 20
        for i, xmin, xmax, ymin, ymax in self._gen_flood_wrap(data, rmsimg, innerclip, outerclip, domask=True):
            # ignore empty islands
            # This should now be impossible to trigger
            if np.size(i) < 1:
                self.log.warn(""Empty island detected, this should be imposisble."")
                continue
            isle_num += 1
            scalars = (innerclip, outerclip, max_summits)
            offsets = (xmin, xmax, ymin, ymax)
            island_data = IslandFittingData(isle_num, i, scalars, offsets, doislandflux)
            # If cores==1 run fitting in main process. Otherwise build up groups of islands
            # and submit to queue for subprocesses. Passing a group of islands is more
            # efficient than passing single islands to the subprocesses.
            if cores == 1:
                res = self._fit_island(island_data)
                queue.append(res)
            else:
                island_group.append(island_data)
                # If the island group is full queue it for the subprocesses to fit
                if len(island_group) >= group_size:
                    fit_parallel(island_group)
                    island_group = []

        # The last partially-filled island group also needs to be queued for fitting
        if len(island_group) > 0:
            fit_parallel(island_group)

        # Write the output to the output file
        if outfile:
            print(header.format(""{0}-({1})"".format(__version__, __date__), filename), file=outfile)
            print(OutputSource.header, file=outfile)

        sources = []
        for srcs in queue:
            if srcs:  # ignore empty lists
                for src in srcs:
                    # ignore sources that we have been told to ignore
                    if (src.peak_flux > 0 and nopositive) or (src.peak_flux < 0 and nonegative):
                        continue
                    sources.append(src)
                    if outfile:
                        print(str(src), file=outfile)
        self.sources.extend(sources)
        return sources","Run the Aegean source finder.


        Parameters
        ----------
        filename : str or HDUList
            Image filename or HDUList.

        hdu_index : int
            The index of the FITS HDU (extension).

        outfile : str
            file for printing catalog (NOT a table, just a text file of my own design)

        rms : float
            Use this rms for the entire image (will also assume that background is 0)

        max_summits : int
            Fit up to this many components to each island (extras are included but not fit)

        innerclip, outerclip : float
            The seed (inner) and flood (outer) clipping level (sigmas).

        cores : int
            Number of CPU cores to use. None means all cores.

        rmsin, bkgin : str or HDUList
            Filename or HDUList for the noise and background images.
            If either are None, then it will be calculated internally.

        beam : (major, minor, pa)
            Floats representing the synthesised beam (degrees).
            Replaces whatever is given in the FITS header.
            If the FITS header has no BMAJ/BMIN then this is required.

        doislandflux : bool
            If True then each island will also be characterized.

        nopositive, nonegative : bool
            Whether to return positive or negative sources.
            Default nopositive=False, nonegative=True.

        mask : str
            The filename of a region file created by MIMAS.
            Islands outside of this region will be ignored.

        lat : float
            The latitude of the telescope (declination of zenith).

        imgpsf : str or HDUList
             Filename or HDUList for a psf image.

        blank : bool
            Cause the output image to be blanked where islands are found.

        docov : bool
            If True then include covariance matrix in the fitting process. (default=True)

        cube_index : int
            For image cubes, cube_index determines which slice is used.

        Returns
        -------
        sources : list
            List of sources found."
"def init(calc_id='nojob', level=logging.INFO):
    """"""
    1. initialize the root logger (if not already initialized)
    2. set the format of the root handlers (if any)
    3. return a new calculation ID candidate if calc_id is 'job' or 'nojob'
       (with 'nojob' the calculation ID is not stored in the database)
    """"""
    if not logging.root.handlers:  # first time
        logging.basicConfig(level=level)
    if calc_id == 'job':  # produce a calc_id by creating a job in the db
        calc_id = dbcmd('create_job', datastore.get_datadir())
    elif calc_id == 'nojob':  # produce a calc_id without creating a job
        calc_id = datastore.get_last_calc_id() + 1
    else:
        assert isinstance(calc_id, int), calc_id
    fmt = '[%(asctime)s #{} %(levelname)s] %(message)s'.format(calc_id)
    for handler in logging.root.handlers:
        handler.setFormatter(logging.Formatter(fmt))
    return calc_id","1. initialize the root logger (if not already initialized)
    2. set the format of the root handlers (if any)
    3. return a new calculation ID candidate if calc_id is 'job' or 'nojob'
       (with 'nojob' the calculation ID is not stored in the database)"
"def resample(self, seed=None):
        """"""Resample the dataset.

        Args:
            seed (int, optional): Seed for resampling. By default no seed is
            used.
        """"""
        if seed is not None:
            gen = torch.manual_seed(seed)
        else:
            gen = torch.default_generator

        if self.replacement:
            self.perm = torch.LongTensor(len(self)).random_(
                len(self.dataset), generator=gen)
        else:
            self.perm = torch.randperm(
                len(self.dataset), generator=gen).narrow(0, 0, len(self))","Resample the dataset.

        Args:
            seed (int, optional): Seed for resampling. By default no seed is
            used."
"def finalize(self, result):
        """"""Put monkeypatches back as we found them.""""""
        sys.stderr = self._stderr.pop()
        sys.stdout = self._stdout.pop()
        pdb.set_trace = self._set_trace.pop()
        pdb.Pdb.cmdloop = self._cmdloop.pop()",Put monkeypatches back as we found them.
"def blocks(self):
        """"""
        The RDD of sub-matrix blocks
        ((blockRowIndex, blockColIndex), sub-matrix) that form this
        distributed matrix.

        >>> mat = BlockMatrix(
        ...     sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),
        ...                     ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))]), 3, 2)
        >>> blocks = mat.blocks
        >>> blocks.first()
        ((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0))

        """"""
        # We use DataFrames for serialization of sub-matrix blocks
        # from Java, so we first convert the RDD of blocks to a
        # DataFrame on the Scala/Java side. Then we map each Row in
        # the DataFrame back to a sub-matrix block on this side.
        blocks_df = callMLlibFunc(""getMatrixBlocks"", self._java_matrix_wrapper._java_model)
        blocks = blocks_df.rdd.map(lambda row: ((row[0][0], row[0][1]), row[1]))
        return blocks","The RDD of sub-matrix blocks
        ((blockRowIndex, blockColIndex), sub-matrix) that form this
        distributed matrix.

        >>> mat = BlockMatrix(
        ...     sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),
        ...                     ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))]), 3, 2)
        >>> blocks = mat.blocks
        >>> blocks.first()
        ((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0))"
"def _rotate(img, angle):
        '''
        angle [DEG]
        '''
        s = img.shape
        if angle == 0:
            return img
        else:
            M = cv2.getRotationMatrix2D((s[1] // 2,
                                         s[0] // 2), angle, 1)
            return cv2.warpAffine(img, M, (s[1], s[0]))",angle [DEG]
"def dynamic_filter(self):
        """"""Stream statuses/filter with dynamic parameters""""""

        url = 'https://stream.twitter.com/%s/statuses/filter.json' \
              % self.streamer.api_version
        self.streamer._request(url, 'POST', params=self.params)",Stream statuses/filter with dynamic parameters
"def post(self, url, body):
        """"""Perform a POST request to the given resource with the given
        body.  The ""url"" argument will be joined to the base URL this
        object was initialized with.

        :param string url: the URL resource to hit
        :param string body: the POST body for the request
        :rtype: requests.Response object""""""

        to_hit = urlparse.urljoin(self.base_url, url)
        resp = self.pool.post(to_hit, data=body, auth=self.auth)
        return resp","Perform a POST request to the given resource with the given
        body.  The ""url"" argument will be joined to the base URL this
        object was initialized with.

        :param string url: the URL resource to hit
        :param string body: the POST body for the request
        :rtype: requests.Response object"
"def append_field(self, header, value, mask=None):
        """"""
        Append a match field.

        ========= =======================================================
        Argument  Description
        ========= =======================================================
        header    match field header ID which is defined automatically in
                  ``ofproto``
        value     match field value
        mask      mask value to the match field
        ========= =======================================================

        The available ``header`` is as follows.

        ====================== ===================================
        Header ID              Description
        ====================== ===================================
        OXM_OF_IN_PORT         Switch input port
        OXM_OF_IN_PHY_PORT     Switch physical input port
        OXM_OF_METADATA        Metadata passed between tables
        OXM_OF_ETH_DST         Ethernet destination address
        OXM_OF_ETH_SRC         Ethernet source address
        OXM_OF_ETH_TYPE        Ethernet frame type
        OXM_OF_VLAN_VID        VLAN id
        OXM_OF_VLAN_PCP        VLAN priority
        OXM_OF_IP_DSCP         IP DSCP (6 bits in ToS field)
        OXM_OF_IP_ECN          IP ECN (2 bits in ToS field)
        OXM_OF_IP_PROTO        IP protocol
        OXM_OF_IPV4_SRC        IPv4 source address
        OXM_OF_IPV4_DST        IPv4 destination address
        OXM_OF_TCP_SRC         TCP source port
        OXM_OF_TCP_DST         TCP destination port
        OXM_OF_UDP_SRC         UDP source port
        OXM_OF_UDP_DST         UDP destination port
        OXM_OF_SCTP_SRC        SCTP source port
        OXM_OF_SCTP_DST        SCTP destination port
        OXM_OF_ICMPV4_TYPE     ICMP type
        OXM_OF_ICMPV4_CODE     ICMP code
        OXM_OF_ARP_OP          ARP opcode
        OXM_OF_ARP_SPA         ARP source IPv4 address
        OXM_OF_ARP_TPA         ARP target IPv4 address
        OXM_OF_ARP_SHA         ARP source hardware address
        OXM_OF_ARP_THA         ARP target hardware address
        OXM_OF_IPV6_SRC        IPv6 source address
        OXM_OF_IPV6_DST        IPv6 destination address
        OXM_OF_IPV6_FLABEL     IPv6 Flow Label
        OXM_OF_ICMPV6_TYPE     ICMPv6 type
        OXM_OF_ICMPV6_CODE     ICMPv6 code
        OXM_OF_IPV6_ND_TARGET  Target address for ND
        OXM_OF_IPV6_ND_SLL     Source link-layer for ND
        OXM_OF_IPV6_ND_TLL     Target link-layer for ND
        OXM_OF_MPLS_LABEL      MPLS label
        OXM_OF_MPLS_TC         MPLS TC
        OXM_OF_MPLS_BOS        MPLS BoS bit
        OXM_OF_PBB_ISID        PBB I-SID
        OXM_OF_TUNNEL_ID       Logical Port Metadata
        OXM_OF_IPV6_EXTHDR     IPv6 Extension Header pseudo-field
        ====================== ===================================
        """"""
        self.fields.append(OFPMatchField.make(header, value, mask))","Append a match field.

        ========= =======================================================
        Argument  Description
        ========= =======================================================
        header    match field header ID which is defined automatically in
                  ``ofproto``
        value     match field value
        mask      mask value to the match field
        ========= =======================================================

        The available ``header`` is as follows.

        ====================== ===================================
        Header ID              Description
        ====================== ===================================
        OXM_OF_IN_PORT         Switch input port
        OXM_OF_IN_PHY_PORT     Switch physical input port
        OXM_OF_METADATA        Metadata passed between tables
        OXM_OF_ETH_DST         Ethernet destination address
        OXM_OF_ETH_SRC         Ethernet source address
        OXM_OF_ETH_TYPE        Ethernet frame type
        OXM_OF_VLAN_VID        VLAN id
        OXM_OF_VLAN_PCP        VLAN priority
        OXM_OF_IP_DSCP         IP DSCP (6 bits in ToS field)
        OXM_OF_IP_ECN          IP ECN (2 bits in ToS field)
        OXM_OF_IP_PROTO        IP protocol
        OXM_OF_IPV4_SRC        IPv4 source address
        OXM_OF_IPV4_DST        IPv4 destination address
        OXM_OF_TCP_SRC         TCP source port
        OXM_OF_TCP_DST         TCP destination port
        OXM_OF_UDP_SRC         UDP source port
        OXM_OF_UDP_DST         UDP destination port
        OXM_OF_SCTP_SRC        SCTP source port
        OXM_OF_SCTP_DST        SCTP destination port
        OXM_OF_ICMPV4_TYPE     ICMP type
        OXM_OF_ICMPV4_CODE     ICMP code
        OXM_OF_ARP_OP          ARP opcode
        OXM_OF_ARP_SPA         ARP source IPv4 address
        OXM_OF_ARP_TPA         ARP target IPv4 address
        OXM_OF_ARP_SHA         ARP source hardware address
        OXM_OF_ARP_THA         ARP target hardware address
        OXM_OF_IPV6_SRC        IPv6 source address
        OXM_OF_IPV6_DST        IPv6 destination address
        OXM_OF_IPV6_FLABEL     IPv6 Flow Label
        OXM_OF_ICMPV6_TYPE     ICMPv6 type
        OXM_OF_ICMPV6_CODE     ICMPv6 code
        OXM_OF_IPV6_ND_TARGET  Target address for ND
        OXM_OF_IPV6_ND_SLL     Source link-layer for ND
        OXM_OF_IPV6_ND_TLL     Target link-layer for ND
        OXM_OF_MPLS_LABEL      MPLS label
        OXM_OF_MPLS_TC         MPLS TC
        OXM_OF_MPLS_BOS        MPLS BoS bit
        OXM_OF_PBB_ISID        PBB I-SID
        OXM_OF_TUNNEL_ID       Logical Port Metadata
        OXM_OF_IPV6_EXTHDR     IPv6 Extension Header pseudo-field
        ====================== ==================================="
"def quantity_yXL(fig, left, bottom, top, quantity=params.L_yXL, label=r'$\mathcal{L}_{yXL}$'):
                            
    '''make a bunch of image plots, each showing the spatial normalized
    connectivity of synapses'''
    
 
    layers = ['L1', 'L2/3', 'L4', 'L5', 'L6']
    ncols = len(params.y) / 4
    
    #assess vlims
    vmin = 0
    vmax = 0
    for y in params.y:
        if quantity[y].max() > vmax:
            vmax = quantity[y].max()
    
    gs = gridspec.GridSpec(4, 4, left=left, bottom=bottom, top=top)
    
    for i, y in enumerate(params.y):
        ax = fig.add_subplot(gs[i/4, i%4])

        masked_array = np.ma.array(quantity[y], mask=quantity[y]==0)
        # cmap = plt.get_cmap('hot', 20)
        # cmap.set_bad('k', 0.5)
        
        # im = ax.imshow(masked_array,
        im = ax.pcolormesh(masked_array,
                            vmin=vmin, vmax=vmax,
                            cmap=cmap,
                            #interpolation='nearest',
                            )
        ax.invert_yaxis()

        ax.axis(ax.axis('tight'))
        ax.xaxis.set_ticks_position('top')
        ax.set_xticks(np.arange(9)+0.5)
        ax.set_yticks(np.arange(5)+0.5)
        
        #if divmod(i, 4)[1] == 0:
        if i % 4 == 0:
            ax.set_yticklabels(layers, )
            ax.set_ylabel('$L$', labelpad=0.)
        else:
            ax.set_yticklabels([])
        if i < 4:
            ax.set_xlabel(r'$X$', labelpad=-1,fontsize=8)
            ax.set_xticklabels(params.X, rotation=270)
        else:
            ax.set_xticklabels([])
        ax.xaxis.set_label_position('top')
        
        ax.text(0.5, -0.13, r'$y=$'+y,
            horizontalalignment='center',
            verticalalignment='center',
            #
                transform=ax.transAxes,fontsize=5.5)
    
    #colorbar
    rect = np.array(ax.get_position().bounds)
    rect[0] += rect[2] + 0.01
    rect[1] = bottom
    rect[2] = 0.01
    rect[3] = top-bottom
    cax = fig.add_axes(rect)
    cbar = plt.colorbar(im, cax=cax)
    #cbar.set_label(label, ha='center')
    cbar.set_label(label, labelpad=0)","make a bunch of image plots, each showing the spatial normalized
    connectivity of synapses"
"def confusion_performance(mat, fn):
    """"""Apply a performance function to a confusion matrix
    
    :param mat: confusion matrix
    :type mat: square matrix
    :param function fn: performance function
    """"""
    if mat.shape[0] != mat.shape[1] or mat.shape < (2, 2):
        raise TypeError('{} is not a confusion matrix'.format(mat))
    elif mat.shape == (2, 2):
        return fn(mat[TP], mat[TN], mat[FP], mat[FN])
    res = numpy.empty(mat.shape[0])
    for i in range(len(res)):
        res[i] = fn(mat[i, i],                                   # TP
                    sum(mat) - sum(mat[:, i]) - sum(mat[i, :]),  # TN
                    sum(mat[:, i]) - mat[i, i],                  # FP
                    sum(mat[i, :]) - mat[i, i])                  # FN
    return res","Apply a performance function to a confusion matrix
    
    :param mat: confusion matrix
    :type mat: square matrix
    :param function fn: performance function"
"def unpack_hyperopt_vals(vals):
    """"""
    Unpack values from a hyperopt return dictionary where values are wrapped in a list.
    :param vals: dict
    :return: dict
        copy of the dictionary with unpacked values
    """"""
    assert isinstance(vals, dict), ""Parameter must be given as dict.""
    ret = {}
    for k, v in list(vals.items()):
        try:
            ret[k] = v[0]
        except (TypeError, IndexError):
            ret[k] = v
    return ret","Unpack values from a hyperopt return dictionary where values are wrapped in a list.
    :param vals: dict
    :return: dict
        copy of the dictionary with unpacked values"
"def to_element(self, include_namespaces=False):
        """"""Return an ElementTree Element representing this instance.

        Args:
            include_namespaces (bool, optional): If True, include xml
                namespace attributes on the root element

        Return:
            ~xml.etree.ElementTree.Element: an Element.
        """"""
        elt_attrib = {}
        if include_namespaces:
            elt_attrib.update({
                'xmlns': ""urn:schemas-upnp-org:metadata-1-0/DIDL-Lite/"",
                'xmlns:dc': ""http://purl.org/dc/elements/1.1/"",
                'xmlns:upnp': ""urn:schemas-upnp-org:metadata-1-0/upnp/"",
            })
        elt_attrib.update({
            'parentID': self.parent_id,
            'restricted': 'true' if self.restricted else 'false',
            'id': self.item_id
        })
        elt = XML.Element(self.tag, elt_attrib)

        # Add the title, which should always come first, according to the spec
        XML.SubElement(elt, 'dc:title').text = self.title

        # Add in any resources
        for resource in self.resources:
            elt.append(resource.to_element())

        # Add the rest of the metadata attributes (i.e all those listed in
        # _translation) as sub-elements of the item element.
        for key, value in self._translation.items():
            if hasattr(self, key):
                # Some attributes have a namespace of '', which means they
                # are in the default namespace. We need to handle those
                # carefully
                tag = ""%s:%s"" % value if value[0] else ""%s"" % value[1]
                XML.SubElement(elt, tag).text = (""%s"" % getattr(self, key))
        # Now add in the item class
        XML.SubElement(elt, 'upnp:class').text = self.item_class

        # And the desc element
        desc_attrib = {'id': 'cdudn', 'nameSpace':
                       'urn:schemas-rinconnetworks-com:metadata-1-0/'}
        desc_elt = XML.SubElement(elt, 'desc', desc_attrib)
        desc_elt.text = self.desc

        return elt","Return an ElementTree Element representing this instance.

        Args:
            include_namespaces (bool, optional): If True, include xml
                namespace attributes on the root element

        Return:
            ~xml.etree.ElementTree.Element: an Element."
"def _specialKeyEvent(key, upDown):
    """""" Helper method for special keys.

    Source: http://stackoverflow.com/questions/11045814/emulate-media-key-press-on-mac
    """"""
    assert upDown in ('up', 'down'), ""upDown argument must be 'up' or 'down'""

    key_code = special_key_translate_table[key]

    ev = AppKit.NSEvent.otherEventWithType_location_modifierFlags_timestamp_windowNumber_context_subtype_data1_data2_(
            Quartz.NSSystemDefined, # type
            (0,0), # location
            0xa00 if upDown == 'down' else 0xb00, # flags
            0, # timestamp
            0, # window
            0, # ctx
            8, # subtype
            (key_code << 16) | ((0xa if upDown == 'down' else 0xb) << 8), # data1
            -1 # data2
        )

    Quartz.CGEventPost(0, ev.CGEvent())","Helper method for special keys.

    Source: http://stackoverflow.com/questions/11045814/emulate-media-key-press-on-mac"
"def reverseComplement(self):
        """"""
        Reverse complement a nucleotide sequence.

        @return: The reverse complemented sequence as an instance of the
            current class.
        """"""
        quality = None if self.quality is None else self.quality[::-1]
        sequence = self.sequence.translate(self.COMPLEMENT_TABLE)[::-1]
        return self.__class__(self.id, sequence, quality)","Reverse complement a nucleotide sequence.

        @return: The reverse complemented sequence as an instance of the
            current class."
"def _get(self, url: str) -> str:
        """"""A small wrapper method which makes a quick GET request.

        Parameters
        ----------
        url : str
            The URL to get.

        Returns
        -------
        str
            The raw html of the requested page.

        Raises
        ------
        RuneConnectionError
            If the GET response status is not 200.
        """"""
        resp = self.session.get(url, headers=self.HEADERS)
        if resp.status_code is 200:
            return resp.text
        else:
            raise RuneConnectionError(resp.status_code)","A small wrapper method which makes a quick GET request.

        Parameters
        ----------
        url : str
            The URL to get.

        Returns
        -------
        str
            The raw html of the requested page.

        Raises
        ------
        RuneConnectionError
            If the GET response status is not 200."
"def hasNext(self):
        """"""
        Returns True if the cursor has a next position, False if not
        :return:
        """"""
        cursor_pos = self.cursorpos + 1

        try:
            self.cursordat[cursor_pos]
            return True
        except IndexError:
            return False","Returns True if the cursor has a next position, False if not
        :return:"
"def update(self):
        """"""Determine all MA coefficients.

        >>> from hydpy.models.arma import *
        >>> parameterstep('1d')
        >>> responses(((1., 2.), (1.,)), th_3=((1.,), (1., 2., 3.)))
        >>> derived.ma_coefs.update()
        >>> derived.ma_coefs
        ma_coefs([[1.0, nan, nan],
                  [1.0, 2.0, 3.0]])

        Note that updating parameter `ar_coefs` sets the shape of the log
        sequence |LogIn| automatically.

        >>> logs.login
        login([[nan, nan, nan],
               [nan, nan, nan]])
        """"""
        pars = self.subpars.pars
        coefs = pars.control.responses.ma_coefs
        self.shape = coefs.shape
        self(coefs)
        pars.model.sequences.logs.login.shape = self.shape","Determine all MA coefficients.

        >>> from hydpy.models.arma import *
        >>> parameterstep('1d')
        >>> responses(((1., 2.), (1.,)), th_3=((1.,), (1., 2., 3.)))
        >>> derived.ma_coefs.update()
        >>> derived.ma_coefs
        ma_coefs([[1.0, nan, nan],
                  [1.0, 2.0, 3.0]])

        Note that updating parameter `ar_coefs` sets the shape of the log
        sequence |LogIn| automatically.

        >>> logs.login
        login([[nan, nan, nan],
               [nan, nan, nan]])"
"def CopyFromStringTuple(self, time_elements_tuple):
    """"""Copies time elements from string-based time elements tuple.

    Args:
      time_elements_tuple (Optional[tuple[str, str, str, str, str, str]]):
          time elements, contains year, month, day of month, hours, minutes and
          seconds.

    Raises:
      ValueError: if the time elements tuple is invalid.
    """"""
    if len(time_elements_tuple) < 6:
      raise ValueError((
          'Invalid time elements tuple at least 6 elements required,'
          'got: {0:d}').format(len(time_elements_tuple)))

    try:
      year = int(time_elements_tuple[0], 10)
    except (TypeError, ValueError):
      raise ValueError('Invalid year value: {0!s}'.format(
          time_elements_tuple[0]))

    try:
      month = int(time_elements_tuple[1], 10)
    except (TypeError, ValueError):
      raise ValueError('Invalid month value: {0!s}'.format(
          time_elements_tuple[1]))

    try:
      day_of_month = int(time_elements_tuple[2], 10)
    except (TypeError, ValueError):
      raise ValueError('Invalid day of month value: {0!s}'.format(
          time_elements_tuple[2]))

    try:
      hours = int(time_elements_tuple[3], 10)
    except (TypeError, ValueError):
      raise ValueError('Invalid hours value: {0!s}'.format(
          time_elements_tuple[3]))

    try:
      minutes = int(time_elements_tuple[4], 10)
    except (TypeError, ValueError):
      raise ValueError('Invalid minutes value: {0!s}'.format(
          time_elements_tuple[4]))

    try:
      seconds = int(time_elements_tuple[5], 10)
    except (TypeError, ValueError):
      raise ValueError('Invalid seconds value: {0!s}'.format(
          time_elements_tuple[5]))

    self._normalized_timestamp = None
    self._number_of_seconds = self._GetNumberOfSecondsFromElements(
        year, month, day_of_month, hours, minutes, seconds)
    self._time_elements_tuple = (
        year, month, day_of_month, hours, minutes, seconds)","Copies time elements from string-based time elements tuple.

    Args:
      time_elements_tuple (Optional[tuple[str, str, str, str, str, str]]):
          time elements, contains year, month, day of month, hours, minutes and
          seconds.

    Raises:
      ValueError: if the time elements tuple is invalid."
"def resolve(self, geoid, id_only=False):
        '''
        Resolve a GeoZone given a GeoID.

        The start date is resolved from the given GeoID,
        ie. it find there is a zone valid a the geoid validity,
        resolve the `latest` alias
        or use `latest` when no validity is given.

        If `id_only` is True,
        the result will be the resolved GeoID
        instead of the resolved zone.
        '''
        level, code, validity = geoids.parse(geoid)
        qs = self(level=level, code=code)
        if id_only:
            qs = qs.only('id')
        if validity == 'latest':
            result = qs.latest()
        else:
            result = qs.valid_at(validity).first()
        return result.id if id_only and result else result","Resolve a GeoZone given a GeoID.

        The start date is resolved from the given GeoID,
        ie. it find there is a zone valid a the geoid validity,
        resolve the `latest` alias
        or use `latest` when no validity is given.

        If `id_only` is True,
        the result will be the resolved GeoID
        instead of the resolved zone."
"def add_permission(self, name):
        """"""
            Adds a permission to the backend, model permission

            :param name:
                name of the permission: 'can_add','can_edit' etc...
        """"""
        perm = self.find_permission(name)
        if perm is None:
            try:
                perm = self.permission_model()
                perm.name = name
                self.get_session.add(perm)
                self.get_session.commit()
                return perm
            except Exception as e:
                log.error(c.LOGMSG_ERR_SEC_ADD_PERMISSION.format(str(e)))
                self.get_session.rollback()
        return perm","Adds a permission to the backend, model permission

            :param name:
                name of the permission: 'can_add','can_edit' etc..."
"def WriteUInt256(self, value):
        """"""
        Write a UInt256 type to the stream.

        Args:
            value (UInt256):

        Raises:
            Exception: when `value` is not of neocore.UInt256 type.
        """"""
        if type(value) is UInt256:
            value.Serialize(self)
        else:
            raise Exception(""Cannot write value that is not UInt256"")","Write a UInt256 type to the stream.

        Args:
            value (UInt256):

        Raises:
            Exception: when `value` is not of neocore.UInt256 type."
"def get_mnemonics_from_regex(self, pattern):
        """"""
        Should probably integrate getting curves with regex, vs getting with
        aliases, even though mixing them is probably confusing. For now I can't
        think of another use case for these wildcards, so I'll just implement
        for the curve table and we can worry about a nice solution later if we
        ever come back to it.
        """"""
        regex = re.compile(pattern)
        keys = list(self.data.keys())
        return [m.group(0) for k in keys for m in [regex.search(k)] if m]","Should probably integrate getting curves with regex, vs getting with
        aliases, even though mixing them is probably confusing. For now I can't
        think of another use case for these wildcards, so I'll just implement
        for the curve table and we can worry about a nice solution later if we
        ever come back to it."
"def photo_url(self):
        """""".

        :return: url
        :rtype: str
        """"""
        if self.url is not None:
            if self.soup is not None:
                img = self.soup.find('img', class_='Avatar Avatar--l')['src']
                return img.replace('_l', '_r')
            else:
                assert (self.card is not None)
                return PROTOCOL + self.card.img['src'].replace('_xs', '_r')
        else:
            return 'http://pic1.zhimg.com/da8e974dc_r.jpg'",".

        :return: url
        :rtype: str"
"def parse_options_header(value, multiple=False):
    """"""Parse a ``Content-Type`` like header into a tuple with the content
    type and the options:

    >>> parse_options_header('text/html; charset=utf8')
    ('text/html', {'charset': 'utf8'})

    This should not be used to parse ``Cache-Control`` like headers that use
    a slightly different format.  For these headers use the
    :func:`parse_dict_header` function.

    .. versionchanged:: 0.15
        :rfc:`2231` parameter continuations are handled.

    .. versionadded:: 0.5

    :param value: the header to parse.
    :param multiple: Whether try to parse and return multiple MIME types
    :return: (mimetype, options) or (mimetype, options, mimetype, options, )
             if multiple=True
    """"""
    if not value:
        return """", {}

    result = []

    value = "","" + value.replace(""\n"", "","")
    while value:
        match = _option_header_start_mime_type.match(value)
        if not match:
            break
        result.append(match.group(1))  # mimetype
        options = {}
        # Parse options
        rest = match.group(2)
        continued_encoding = None
        while rest:
            optmatch = _option_header_piece_re.match(rest)
            if not optmatch:
                break
            option, count, encoding, language, option_value = optmatch.groups()
            # Continuations don't have to supply the encoding after the
            # first line. If we're in a continuation, track the current
            # encoding to use for subsequent lines. Reset it when the
            # continuation ends.
            if not count:
                continued_encoding = None
            else:
                if not encoding:
                    encoding = continued_encoding
                continued_encoding = encoding
            option = unquote_header_value(option)
            if option_value is not None:
                option_value = unquote_header_value(option_value, option == ""filename"")
                if encoding is not None:
                    option_value = _unquote(option_value).decode(encoding)
            if count:
                # Continuations append to the existing value. For
                # simplicity, this ignores the possibility of
                # out-of-order indices, which shouldn't happen anyway.
                options[option] = options.get(option, """") + option_value
            else:
                options[option] = option_value
            rest = rest[optmatch.end() :]
        result.append(options)
        if multiple is False:
            return tuple(result)
        value = rest

    return tuple(result) if result else ("""", {})","Parse a ``Content-Type`` like header into a tuple with the content
    type and the options:

    >>> parse_options_header('text/html; charset=utf8')
    ('text/html', {'charset': 'utf8'})

    This should not be used to parse ``Cache-Control`` like headers that use
    a slightly different format.  For these headers use the
    :func:`parse_dict_header` function.

    .. versionchanged:: 0.15
        :rfc:`2231` parameter continuations are handled.

    .. versionadded:: 0.5

    :param value: the header to parse.
    :param multiple: Whether try to parse and return multiple MIME types
    :return: (mimetype, options) or (mimetype, options, mimetype, options, )
             if multiple=True"
"def init_app(self, app, entry_point_group='invenio_webhooks.receivers'):
        """"""Flask application initialization.""""""
        self.init_config(app)
        state = _WebhooksState(app, entry_point_group=entry_point_group)
        self._state = app.extensions['invenio-webhooks'] = state",Flask application initialization.
"def _is_pickle_valid(self):
        """"""Logic to decide if the file should be processed or just needs to
        be loaded from its pickle data.
        """"""
        if not os.path.exists(self._pickle_file):
            return False
        else:
            file_mtime = os.path.getmtime(self.logfile)
            pickle_mtime = os.path.getmtime(self._pickle_file)
            if file_mtime > pickle_mtime:
                return False
        return True","Logic to decide if the file should be processed or just needs to
        be loaded from its pickle data."
"def in_scope(self, exclude_scopes=None, include_scopes=None):
    """"""Whether this scope should be included by the given inclusion and exclusion rules.

    :param Scope exclude_scopes: An optional Scope containing scope names to exclude. None (the
      default value) indicates that no filtering should be done based on exclude_scopes.
    :param Scope include_scopes: An optional Scope containing scope names to include. None (the
      default value) indicates that no filtering should be done based on include_scopes.
    :return: True if none of the input scopes are in `exclude_scopes`, and either (a) no include
      scopes are provided, or (b) at least one input scope is included in the `include_scopes` list.
    :rtype: bool
    """"""
    if include_scopes is not None and not isinstance(include_scopes, Scope):
      raise ValueError('include_scopes must be a Scope instance but was {}.'.format(
        type(include_scopes)
      ))
    if exclude_scopes is not None and not isinstance(exclude_scopes, Scope):
      raise ValueError('exclude_scopes must be a Scope instance but was {}.'.format(
        type(exclude_scopes)
      ))
    if exclude_scopes and any(s in exclude_scopes for s in self):
      return False
    if include_scopes and not any(s in include_scopes for s in self):
      return False
    return True","Whether this scope should be included by the given inclusion and exclusion rules.

    :param Scope exclude_scopes: An optional Scope containing scope names to exclude. None (the
      default value) indicates that no filtering should be done based on exclude_scopes.
    :param Scope include_scopes: An optional Scope containing scope names to include. None (the
      default value) indicates that no filtering should be done based on include_scopes.
    :return: True if none of the input scopes are in `exclude_scopes`, and either (a) no include
      scopes are provided, or (b) at least one input scope is included in the `include_scopes` list.
    :rtype: bool"
"def ektnam(n, lenout=_default_len_out):
    """"""
    Return the name of a specified, loaded table.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/ektnam_c.html

    :param n: Index of table.
    :type n: int
    :param lenout: Maximum table name length.
    :type lenout: int
    :return: Name of table.
    :rtype: str
    """"""
    n = ctypes.c_int(n)
    lenout = ctypes.c_int(lenout)
    table = stypes.stringToCharP(lenout)
    libspice.ektnam_c(n, lenout, table)
    return stypes.toPythonString(table)","Return the name of a specified, loaded table.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/ektnam_c.html

    :param n: Index of table.
    :type n: int
    :param lenout: Maximum table name length.
    :type lenout: int
    :return: Name of table.
    :rtype: str"
"def cookies(self):
        """"""
        Retrieve the cookies header from all the users who visited.
        """"""
        return (self.get_query()
                .select(PageView.ip, PageView.headers['Cookie'])
                .where(PageView.headers['Cookie'].is_null(False))
                .tuples())",Retrieve the cookies header from all the users who visited.
"def notify_observers(self, joinpoint, post=False):
        """"""Notify observers with parameter calls and information about
        pre/post call.
        """"""

        _observers = tuple(self.observers)

        for observer in _observers:
            observer.notify(joinpoint=joinpoint, post=post)","Notify observers with parameter calls and information about
        pre/post call."
"def getDialog(cls, name, parent=None):
        """"""
        Generates a dialog for this class widget and returns it.
        
        :param      parent | <QtGui.QWidget> || None
        
        :return     <QtGui.QDialog>
        """"""
        key = '_{0}__{1}_dialog'.format(cls.__name__, name)
        dlgref = getattr(cls, key, None)
        
        if dlgref is not None:
            dlg = dlgref()
            if dlg:
                return dlg
            
        if parent is None:
            parent = QApplication.activeWindow()
        
        dlg = QDialog(parent)
        
        # create widget
        widget = cls(dlg)
        dlg.__dict__['_mainwidget'] = widget
        widget.layout().setContentsMargins(0, 0, 0, 0)
        
        # create buttons
        opts    = QDialogButtonBox.Save | QDialogButtonBox.Cancel
        buttons = QDialogButtonBox(opts, Qt.Horizontal, dlg)
        
        # create layout
        layout = QVBoxLayout()
        layout.addWidget(widget)
        layout.addWidget(buttons)
        dlg.setLayout(layout)
        dlg.resize(widget.minimumSize() + QSize(15, 15))
        widget.resizeRequested.connect(dlg.adjustSize)
        
        # create connections
        buttons.accepted.connect(widget.save)
        buttons.rejected.connect(dlg.reject)
        widget.saved.connect(dlg.accept)
        widget.setFocus()
        
        dlg.adjustSize()
        if parent and parent.window():
            center = parent.window().geometry().center()
            dlg.move(center.x() - dlg.width() / 2.0,
                     center.y() - dlg.height() / 2.0)
        
        setattr(cls, key, weakref.ref(dlg))
        return dlg","Generates a dialog for this class widget and returns it.
        
        :param      parent | <QtGui.QWidget> || None
        
        :return     <QtGui.QDialog>"
"def on_batch_end(self, last_input, last_output, **kwargs):
        ""Steps through the generators then each of the critics.""
        self.G_A.zero_grad(); self.G_B.zero_grad()
        fake_A, fake_B = last_output[0].detach(), last_output[1].detach()
        real_A, real_B = last_input
        self._set_trainable(D_A=True)
        self.D_A.zero_grad()
        loss_D_A = 0.5 * (self.crit(self.D_A(real_A), True) + self.crit(self.D_A(fake_A), False))
        loss_D_A.backward()
        self.opt_D_A.step()
        self._set_trainable(D_B=True)
        self.D_B.zero_grad()
        loss_D_B = 0.5 * (self.crit(self.D_B(real_B), True) + self.crit(self.D_B(fake_B), False))
        loss_D_B.backward()
        self.opt_D_B.step()
        self._set_trainable()
        metrics = self.learn.loss_func.metrics + [loss_D_A, loss_D_B]
        for n,m in zip(self.names,metrics): self.smootheners[n].add_value(m)",Steps through the generators then each of the critics.
"def cli(env):
    """"""List firewalls.""""""

    mgr = SoftLayer.FirewallManager(env.client)
    table = formatting.Table(['firewall id',
                              'type',
                              'features',
                              'server/vlan id'])
    fwvlans = mgr.get_firewalls()
    dedicated_firewalls = [firewall for firewall in fwvlans
                           if firewall['dedicatedFirewallFlag']]

    for vlan in dedicated_firewalls:
        features = []
        if vlan['highAvailabilityFirewallFlag']:
            features.append('HA')

        if features:
            feature_list = formatting.listing(features, separator=',')
        else:
            feature_list = formatting.blank()

        table.add_row([
            'vlan:%s' % vlan['networkVlanFirewall']['id'],
            'VLAN - dedicated',
            feature_list,
            vlan['id']
        ])

    shared_vlan = [firewall for firewall in fwvlans
                   if not firewall['dedicatedFirewallFlag']]
    for vlan in shared_vlan:
        vs_firewalls = [guest
                        for guest in vlan['firewallGuestNetworkComponents']
                        if has_firewall_component(guest)]

        for firewall in vs_firewalls:
            table.add_row([
                'vs:%s' % firewall['id'],
                'Virtual Server - standard',
                '-',
                firewall['guestNetworkComponent']['guest']['id']
            ])

        server_firewalls = [server
                            for server in vlan['firewallNetworkComponents']
                            if has_firewall_component(server)]

        for firewall in server_firewalls:
            table.add_row([
                'server:%s' % firewall['id'],
                'Server - standard',
                '-',
                utils.lookup(firewall,
                             'networkComponent',
                             'downlinkComponent',
                             'hardwareId')
            ])

    env.fout(table)",List firewalls.
"def one_mask(self):
        """"""Return a mask to determine whether an array chunk has any ones.""""""
        accum = 0
        for i in range(self.data.itemsize):
            accum += (0xAA << (i << 3))
        return accum",Return a mask to determine whether an array chunk has any ones.
"def download_folder(project, destdir, folder=""/"", overwrite=False, chunksize=dxfile.DEFAULT_BUFFER_SIZE,
                    show_progress=False, **kwargs):
    '''
    :param project: Project ID to use as context for this download.
    :type project: string
    :param destdir: Local destination location
    :type destdir: string
    :param folder: Path to the remote folder to download
    :type folder: string
    :param overwrite: Overwrite existing files
    :type overwrite: boolean

    Downloads the contents of the remote *folder* of the *project* into the local directory specified by *destdir*.

    Example::

        download_folder(""project-xxxx"", ""/home/jsmith/input"", folder=""/input"")

    '''

    def ensure_local_dir(d):
        if not os.path.isdir(d):
            if os.path.exists(d):
                raise DXFileError(""Destination location '{}' already exists and is not a directory"".format(d))
            logger.debug(""Creating destination directory: '%s'"", d)
            os.makedirs(d)

    def compose_local_dir(d, remote_folder, remote_subfolder):
        suffix = remote_subfolder[1:] if remote_folder == ""/"" else remote_subfolder[len(remote_folder) + 1:]
        if os.sep != '/':
            suffix = suffix.replace('/', os.sep)
        return os.path.join(d, suffix) if suffix != """" else d

    normalized_folder = folder.strip()
    if normalized_folder != ""/"" and normalized_folder.endswith(""/""):
        normalized_folder = normalized_folder[:-1]
    if normalized_folder == """":
        raise DXFileError(""Invalid remote folder name: '{}'"".format(folder))
    normalized_dest_dir = os.path.normpath(destdir).strip()
    if normalized_dest_dir == """":
        raise DXFileError(""Invalid destination directory name: '{}'"".format(destdir))
    # Creating target directory tree
    remote_folders = list(list_subfolders(project, normalized_folder, recurse=True))
    if len(remote_folders) <= 0:
        raise DXFileError(""Remote folder '{}' not found"".format(normalized_folder))
    remote_folders.sort()
    for remote_subfolder in remote_folders:
        ensure_local_dir(compose_local_dir(normalized_dest_dir, normalized_folder, remote_subfolder))

    # Downloading files
    describe_input = dict(fields=dict(folder=True,
                                      name=True,
                                      id=True,
                                      parts=True,
                                      size=True,
                                      drive=True,
                                      md5=True))

    # A generator that returns the files one by one. We don't want to materialize it, because
    # there could be many files here.
    files_gen = dxpy.search.find_data_objects(classname='file', state='closed', project=project,
                                              folder=normalized_folder, recurse=True, describe=describe_input)
    if files_gen is None:
        # In python 3, the generator can be None, and iterating on it
        # will cause an error.
        return

    # Now it is safe, in both python 2 and 3, to iterate on the generator
    for remote_file in files_gen:
        local_filename = os.path.join(compose_local_dir(normalized_dest_dir,
                                                        normalized_folder,
                                                        remote_file['describe']['folder']),
                                      remote_file['describe']['name'])
        if os.path.exists(local_filename) and not overwrite:
            raise DXFileError(
                ""Destination file '{}' already exists but no overwrite option is provided"".format(local_filename)
            )
        logger.debug(""Downloading '%s/%s' remote file to '%s' location"",
                     ("""" if remote_file['describe']['folder'] == ""/"" else remote_file['describe']['folder']),
                     remote_file['describe']['name'],
                     local_filename)
        download_dxfile(remote_file['describe']['id'],
                        local_filename,
                        chunksize=chunksize,
                        project=project,
                        show_progress=show_progress,
                        describe_output=remote_file['describe'],
                        **kwargs)",":param project: Project ID to use as context for this download.
    :type project: string
    :param destdir: Local destination location
    :type destdir: string
    :param folder: Path to the remote folder to download
    :type folder: string
    :param overwrite: Overwrite existing files
    :type overwrite: boolean

    Downloads the contents of the remote *folder* of the *project* into the local directory specified by *destdir*.

    Example::

        download_folder(""project-xxxx"", ""/home/jsmith/input"", folder=""/input"")"
"def get_ridx(self, iloc):
        """"""List of rupture indices for the given iloc""""""
        with h5py.File(self.source_file, ""r"") as hdf5:
            return hdf5[self.idx_set[""geol""] + ""/RuptureIndex""][iloc]",List of rupture indices for the given iloc
"def get_kpoints(structure, min_distance=0, min_total_kpoints=1, 
                kppra=None, gap_distance=7, remove_symmetry=None, 
                include_gamma=""auto"", header=""simple"", incar=None):
    """"""
    Get kpoints object from JHU servlet, per Wisesa-McGill-Mueller
    methodology.  Refer to http://muellergroup.jhu.edu/K-Points.html
    and P. Wisesa, K. A. McGill, T. Mueller, Phys. Rev. B 93, 
    155109 (2016)

    Args:
        structure (Structure): structure object
        min_distance (float): The minimum allowed distance 
            between lattice points on the real-space superlattice 
        min_total_kpoints (int): The minimum allowed number of 
            total k-points in the Brillouin zone.
        kppra (float): minimum k-points per reciprocal atom.
        gap_distance (float): auto-detection threshold for
            non-periodicity (in slabs, nanowires, etc.)
        remove_symmetry (string): optional flag to control
            symmetry options, can be none, structural, 
            time_reversal, or all
        include_gamma (string or bool): whether to include
            gamma point
        header (string): ""verbose"" or ""simple"", denotes
            the verbosity of the header
        incar (Incar): incar object to upload
    """"""
    config = locals()
    config.pop(""structure"", ""incar"")

    # Generate PRECALC string
    precalc = ''.join([""{}={}\n"".format(k, v) for k, v in config.items()])
    precalc = precalc.replace('_', '').upper()
    precalc = precalc.replace('REMOVESYMMETRY', 'REMOVE_SYMMETRY')
    precalc = precalc.replace('TIMEREVERSAL', 'TIME_REVERSAL')
    url = ""http://muellergroup.jhu.edu:8080/PreCalcServer/PreCalcServlet""
    temp_dir_name = tempfile.mkdtemp()
    cwd = os.getcwd()
    os.chdir(temp_dir_name)

    precalc_file = open(""PRECALC"", 'w+')
    poscar_file = open(""POSCAR"", 'w+')
    incar_file = open(""INCAR"", 'w+')
    
    precalc_file.write(precalc)
    poscar_file.write(structure.to(""POSCAR""))
    files = [(""fileupload"", precalc_file),
             (""fileupload"", poscar_file)]
    if incar:
        incar_file.write(incar.get_string())
        files.append((""fileupload"", incar_file))
    
    precalc_file.seek(0)
    poscar_file.seek(0)
    incar_file.seek(0)
    
    r = requests.post(url, files=files)
    
    precalc_file.close()
    poscar_file.close()
    incar_file.close()
    kpoints = Kpoints.from_string(r.text)
    os.chdir(cwd)

    shutil.rmtree(temp_dir_name)

    return kpoints","Get kpoints object from JHU servlet, per Wisesa-McGill-Mueller
    methodology.  Refer to http://muellergroup.jhu.edu/K-Points.html
    and P. Wisesa, K. A. McGill, T. Mueller, Phys. Rev. B 93, 
    155109 (2016)

    Args:
        structure (Structure): structure object
        min_distance (float): The minimum allowed distance 
            between lattice points on the real-space superlattice 
        min_total_kpoints (int): The minimum allowed number of 
            total k-points in the Brillouin zone.
        kppra (float): minimum k-points per reciprocal atom.
        gap_distance (float): auto-detection threshold for
            non-periodicity (in slabs, nanowires, etc.)
        remove_symmetry (string): optional flag to control
            symmetry options, can be none, structural, 
            time_reversal, or all
        include_gamma (string or bool): whether to include
            gamma point
        header (string): ""verbose"" or ""simple"", denotes
            the verbosity of the header
        incar (Incar): incar object to upload"
"def _read_optimized_geometry(self):
        """"""
        Parses optimized XYZ coordinates. If not present, parses optimized Z-matrix.
        """"""
        header_pattern = r""\*+\s+OPTIMIZATION\s+CONVERGED\s+\*+\s+\*+\s+Coordinates \(Angstroms\)\s+ATOM\s+X\s+Y\s+Z""
        table_pattern = r""\s+\d+\s+\w+\s+([\d\-\.]+)\s+([\d\-\.]+)\s+([\d\-\.]+)""
        footer_pattern = r""\s+Z-matrix Print:""

        parsed_optimized_geometry = read_table_pattern(
            self.text, header_pattern, table_pattern, footer_pattern)
        if parsed_optimized_geometry == [] or None:
            self.data[""optimized_geometry""] = None
            header_pattern = r""^\s+\*+\s+OPTIMIZATION CONVERGED\s+\*+\s+\*+\s+Z-matrix\s+Print:\s+\$molecule\s+[\d\-]+\s+[\d\-]+\n""
            table_pattern = r""\s*(\w+)(?:\s+(\d+)\s+([\d\-\.]+)(?:\s+(\d+)\s+([\d\-\.]+)(?:\s+(\d+)\s+([\d\-\.]+))*)*)*(?:\s+0)*""
            footer_pattern = r""^\$end\n""

            self.data[""optimized_zmat""] = read_table_pattern(
                self.text, header_pattern, table_pattern, footer_pattern)
        else:
            self.data[""optimized_geometry""] = process_parsed_coords(
                parsed_optimized_geometry[0])
            if self.data.get('charge') != None:
                self.data[""molecule_from_optimized_geometry""] = Molecule(
                    species=self.data.get('species'),
                    coords=self.data.get('optimized_geometry'),
                    charge=self.data.get('charge'),
                    spin_multiplicity=self.data.get('multiplicity'))","Parses optimized XYZ coordinates. If not present, parses optimized Z-matrix."
"def extract_random_video_patch(videos, num_frames=-1):
  """"""For every video, extract a random consecutive patch of num_frames.

  Args:
    videos: 5-D Tensor, (NTHWC)
    num_frames: Integer, if -1 then the entire video is returned.
  Returns:
    video_patch: 5-D Tensor, (NTHWC) with T = num_frames.
  Raises:
    ValueError: If num_frames is greater than the number of total frames in
                the video.
  """"""
  if num_frames == -1:
    return videos
  batch_size, num_total_frames, h, w, c = common_layers.shape_list(videos)
  if num_total_frames < num_frames:
    raise ValueError(""Expected num_frames <= %d, got %d"" %
                     (num_total_frames, num_frames))

  # Randomly choose start_inds for each video.
  frame_start = tf.random_uniform(
      shape=(batch_size,), minval=0, maxval=num_total_frames - num_frames + 1,
      dtype=tf.int32)

  # [start[0], start[0] + 1, ... start[0] + num_frames - 1] + ...
  # [start[batch_size-1], ... start[batch_size-1] + num_frames - 1]
  range_inds = tf.expand_dims(tf.range(num_frames), axis=0)
  frame_inds = range_inds + tf.expand_dims(frame_start, axis=1)
  frame_inds = tf.reshape(frame_inds, [-1])

  # [0]*num_frames + [1]*num_frames + ... [batch_size-1]*num_frames
  batch_inds = tf.expand_dims(tf.range(batch_size), axis=1)
  batch_inds = tf.tile(batch_inds, [1, num_frames])
  batch_inds = tf.reshape(batch_inds, [-1])

  gather_inds = tf.stack((batch_inds, frame_inds), axis=1)
  video_patches = tf.gather_nd(videos, gather_inds)
  return tf.reshape(video_patches, (batch_size, num_frames, h, w, c))","For every video, extract a random consecutive patch of num_frames.

  Args:
    videos: 5-D Tensor, (NTHWC)
    num_frames: Integer, if -1 then the entire video is returned.
  Returns:
    video_patch: 5-D Tensor, (NTHWC) with T = num_frames.
  Raises:
    ValueError: If num_frames is greater than the number of total frames in
                the video."
"def format_active_subjects(request):
    """"""Create a string listing active subjects for this connection, suitable for
    appending to authentication error messages.""""""
    decorated_subject_list = [request.primary_subject_str + ' (primary)']
    for subject in request.all_subjects_set:
        if subject != request.primary_subject_str:
            decorated_subject_list.append(subject)
    return ', '.join(decorated_subject_list)","Create a string listing active subjects for this connection, suitable for
    appending to authentication error messages."
"def weibo_url(self):
        """""".

        :return:  unknown
        :rtype: str
        """"""
        if self.url is None:
            return None
        else:
            tmp = self.soup.find(
                'a', class_='zm-profile-header-user-weibo')
            return tmp['href'] if tmp is not None else 'unknown'",".

        :return:  unknown
        :rtype: str"
"def _resample_obspy(samples, sr, newsr, window='hanning', lowpass=True):
    # type: (np.ndarray, int, int, str, bool) -> np.ndarray
    """"""
    Resample using Fourier method. The same as resample_scipy but with
    low-pass filtering for upsampling

    """"""
    from scipy.signal import resample
    from math import ceil
    factor = sr/float(newsr)
    if newsr < sr and lowpass:
        # be sure filter still behaves good
        if factor > 16:
            logger.info(""Automatic filter design is unstable for resampling ""
                        ""factors (current sampling rate/new sampling rate) "" 
                        ""above 16. Manual resampling is necessary."")
        freq = min(sr, newsr) * 0.5 / float(factor)
        logger.debug(f""resample_obspy: lowpass {freq}"")
        samples = lowpass_cheby2(samples, freq=freq, sr=sr, maxorder=12)
    num = int(ceil(len(samples) / factor))

    return _applyMultichan(samples, 
                           lambda S: resample(S, num, window=window))","Resample using Fourier method. The same as resample_scipy but with
    low-pass filtering for upsampling"
"def patch_keepalive_run():
    """"""
    Patches the ``luigi.worker.KeepAliveThread.run`` to immediately stop the keep-alive thread when
    running within a sandbox.
    """"""
    _run = luigi.worker.KeepAliveThread.run

    def run(self):
        # do not run the keep-alive loop when sandboxed
        if os.getenv(""LAW_SANDBOX_SWITCHED"") == ""1"":
            self.stop()
        else:
            _run(self)

    luigi.worker.KeepAliveThread.run = run","Patches the ``luigi.worker.KeepAliveThread.run`` to immediately stop the keep-alive thread when
    running within a sandbox."
"def ask(question, default=True, exact=False):
    """"""Ask the question in y/n form and return True/False.

    If you don't want a default 'yes', set default to None (or to False if you
    want a default 'no').

    With exact=True, we want to get a literal 'yes' or 'no', at least
    when it does not match the default.

    """"""
    if AUTO_RESPONSE:
        if default is None:
            msg = (""The question '%s' requires a manual answer, but "" +
                   ""we're running in --no-input mode."")
            msg = msg % question
            raise RuntimeError(msg)
        logger.debug(""Auto-responding '%s' to the question below."" % (
            default and ""yes"" or ""no""))
        logger.debug(question)
        return default
    while True:
        yn = 'y/n'
        if default is True:
            yn = 'Y/n'
        if default is False:
            yn = 'y/N'
        q = question + "" (%s)? "" % yn
        answer = input(q)
        if answer:
            answer = answer
        else:
            answer = ''
        if not answer and default is not None:
            return default
        if exact and answer.lower() not in ('yes', 'no'):
            print(""Please explicitly answer yes/no in full ""
                  ""(or accept the default)"")
            continue
        if answer:
            answer = answer[0].lower()
            if answer == 'y':
                return True
            if answer == 'n':
                return False
        # We really want an answer.
        print('Please explicitly answer y/n')
        continue","Ask the question in y/n form and return True/False.

    If you don't want a default 'yes', set default to None (or to False if you
    want a default 'no').

    With exact=True, we want to get a literal 'yes' or 'no', at least
    when it does not match the default."
"def override_role_permissions(self):
        """"""Updates the role with the give datasource permissions.

          Permissions not in the request will be revoked. This endpoint should
          be available to admins only. Expects JSON in the format:
           {
            'role_name': '{role_name}',
            'database': [{
                'datasource_type': '{table|druid}',
                'name': '{database_name}',
                'schema': [{
                    'name': '{schema_name}',
                    'datasources': ['{datasource name}, {datasource name}']
                }]
            }]
        }
        """"""
        data = request.get_json(force=True)
        role_name = data['role_name']
        databases = data['database']

        db_ds_names = set()
        for dbs in databases:
            for schema in dbs['schema']:
                for ds_name in schema['datasources']:
                    fullname = utils.get_datasource_full_name(
                        dbs['name'], ds_name, schema=schema['name'])
                    db_ds_names.add(fullname)

        existing_datasources = ConnectorRegistry.get_all_datasources(db.session)
        datasources = [
            d for d in existing_datasources if d.full_name in db_ds_names]
        role = security_manager.find_role(role_name)
        # remove all permissions
        role.permissions = []
        # grant permissions to the list of datasources
        granted_perms = []
        for datasource in datasources:
            view_menu_perm = security_manager.find_permission_view_menu(
                view_menu_name=datasource.perm,
                permission_name='datasource_access')
            # prevent creating empty permissions
            if view_menu_perm and view_menu_perm.view_menu:
                role.permissions.append(view_menu_perm)
                granted_perms.append(view_menu_perm.view_menu.name)
        db.session.commit()
        return self.json_response({
            'granted': granted_perms,
            'requested': list(db_ds_names),
        }, status=201)","Updates the role with the give datasource permissions.

          Permissions not in the request will be revoked. This endpoint should
          be available to admins only. Expects JSON in the format:
           {
            'role_name': '{role_name}',
            'database': [{
                'datasource_type': '{table|druid}',
                'name': '{database_name}',
                'schema': [{
                    'name': '{schema_name}',
                    'datasources': ['{datasource name}, {datasource name}']
                }]
            }]
        }"
"def get_ips(self, instance_id):
        """"""Retrieves all IP addresses associated to a given instance.

        :return: tuple (IPs)
        """"""
        self._init_os_api()
        instance = self._load_instance(instance_id)
        try:
            ip_addrs = set([self.floating_ip])
        except AttributeError:
            ip_addrs = set([])
        for ip_addr in sum(instance.networks.values(), []):
            ip_addrs.add(ip_addr)
        log.debug(""VM `%s` has IP addresses %r"", instance_id, ip_addrs)
        return list(ip_addrs)","Retrieves all IP addresses associated to a given instance.

        :return: tuple (IPs)"
"def process():
    ''' Figure out how we were invoked '''
    invoked_as = os.path.basename(sys.argv[0])

    if invoked_as == ""psiturk"":
        launch_shell()
    elif invoked_as == ""psiturk-server"":
        launch_server()
    elif invoked_as == ""psiturk-shell"":
        launch_shell()
    elif invoked_as == ""psiturk-setup-example"":
        setup_example()
    elif invoked_as == ""psiturk-install"":
        install_from_exchange()",Figure out how we were invoked
"def exp_rmspe(pred:Tensor, targ:Tensor)->Rank0Tensor:
    ""Exp RMSE between `pred` and `targ`.""
    pred,targ = flatten_check(pred,targ)
    pred, targ = torch.exp(pred), torch.exp(targ)
    pct_var = (targ - pred)/targ
    return torch.sqrt((pct_var**2).mean())",Exp RMSE between `pred` and `targ`.
"def _determine_stream_track(self,nTrackChunks):
        """"""Determine the track of the stream in real space""""""
        #Determine how much orbital time is necessary for the progenitor's orbit to cover the stream
        if nTrackChunks is None:
            #default is floor(self._deltaAngleTrack/0.15)+1
            self._nTrackChunks= int(numpy.floor(self._deltaAngleTrack/0.15))+1
        else:
            self._nTrackChunks= nTrackChunks
        if self._nTrackChunks < 4: self._nTrackChunks= 4
        if not hasattr(self,'nInterpolatedTrackChunks'):
            self.nInterpolatedTrackChunks= 1001
        dt= self._deltaAngleTrack\
            /self._progenitor_Omega_along_dOmega
        self._trackts= numpy.linspace(0.,2*dt,2*self._nTrackChunks-1) #to be sure that we cover it
        if self._useTM:
            return self._determine_stream_track_TM()
        #Instantiate an auxiliaryTrack, which is an Orbit instance at the mean frequency of the stream, and zero angle separation wrt the progenitor; prog_stream_offset is the offset between this track and the progenitor at zero angle
        prog_stream_offset=\
            _determine_stream_track_single(self._aA,
                                           self._progenitor,
                                           0., #time = 0
                                           self._progenitor_angle,
                                           self._sigMeanSign,
                                           self._dsigomeanProgDirection,
                                           lambda x: self.meanOmega(x,use_physical=False),
                                           0.) #angle = 0
        auxiliaryTrack= Orbit(prog_stream_offset[3])
        if dt < 0.:
            self._trackts= numpy.linspace(0.,-2.*dt,2*self._nTrackChunks-1)
            #Flip velocities before integrating
            auxiliaryTrack= auxiliaryTrack.flip()
        auxiliaryTrack.integrate(self._trackts,self._pot)
        if dt < 0.:
            #Flip velocities again
            auxiliaryTrack._orb.orbit[:,1]= -auxiliaryTrack._orb.orbit[:,1]
            auxiliaryTrack._orb.orbit[:,2]= -auxiliaryTrack._orb.orbit[:,2]
            auxiliaryTrack._orb.orbit[:,4]= -auxiliaryTrack._orb.orbit[:,4]
        #Calculate the actions, frequencies, and angle for this auxiliary orbit
        acfs= self._aA.actionsFreqs(auxiliaryTrack(0.),
                                    use_physical=False)
        auxiliary_Omega= numpy.array([acfs[3],acfs[4],acfs[5]]).reshape(3\
)
        auxiliary_Omega_along_dOmega= \
            numpy.dot(auxiliary_Omega,self._dsigomeanProgDirection)
        #Now calculate the actions, frequencies, and angles + Jacobian for each chunk
        allAcfsTrack= numpy.empty((self._nTrackChunks,9))
        alljacsTrack= numpy.empty((self._nTrackChunks,6,6))
        allinvjacsTrack= numpy.empty((self._nTrackChunks,6,6))
        thetasTrack= numpy.linspace(0.,self._deltaAngleTrack,
                                    self._nTrackChunks)
        ObsTrack= numpy.empty((self._nTrackChunks,6))
        ObsTrackAA= numpy.empty((self._nTrackChunks,6))
        detdOdJps= numpy.empty((self._nTrackChunks))
        if self._multi is None:
            for ii in range(self._nTrackChunks):
                multiOut= _determine_stream_track_single(self._aA,
                                                         auxiliaryTrack,
                                                         self._trackts[ii]*numpy.fabs(self._progenitor_Omega_along_dOmega/auxiliary_Omega_along_dOmega), #this factor accounts for the difference in frequency between the progenitor and the auxiliary track
                                                         self._progenitor_angle,
                                                         self._sigMeanSign,
                                                         self._dsigomeanProgDirection,
                                                         lambda x: self.meanOmega(x,use_physical=False),
                                                         thetasTrack[ii])
                allAcfsTrack[ii,:]= multiOut[0]
                alljacsTrack[ii,:,:]= multiOut[1]
                allinvjacsTrack[ii,:,:]= multiOut[2]
                ObsTrack[ii,:]= multiOut[3]
                ObsTrackAA[ii,:]= multiOut[4]
                detdOdJps[ii]= multiOut[5]
        else:
            multiOut= multi.parallel_map(\
                (lambda x: _determine_stream_track_single(self._aA,auxiliaryTrack,
                                                          self._trackts[x]*numpy.fabs(self._progenitor_Omega_along_dOmega/auxiliary_Omega_along_dOmega),
                                                          self._progenitor_angle,
                                                          self._sigMeanSign,
                                                          self._dsigomeanProgDirection,
                                                          lambda x: self.meanOmega(x,use_physical=False),
                                                          thetasTrack[x])),
                range(self._nTrackChunks),
                numcores=numpy.amin([self._nTrackChunks,
                                     multiprocessing.cpu_count(),
                                     self._multi]))
            for ii in range(self._nTrackChunks):
                allAcfsTrack[ii,:]= multiOut[ii][0]
                alljacsTrack[ii,:,:]= multiOut[ii][1]
                allinvjacsTrack[ii,:,:]= multiOut[ii][2]
                ObsTrack[ii,:]= multiOut[ii][3]
                ObsTrackAA[ii,:]= multiOut[ii][4]
                detdOdJps[ii]= multiOut[ii][5]
        #Repeat the track calculation using the previous track, to get closer to it
        for nn in range(self.nTrackIterations):
            if self._multi is None:
                for ii in range(self._nTrackChunks):
                    multiOut= _determine_stream_track_single(self._aA,
                                                             Orbit(ObsTrack[ii,:]),
                                                             0.,
                                                             self._progenitor_angle,
                                                             self._sigMeanSign,
                                                             self._dsigomeanProgDirection,
                                                             lambda x:self.meanOmega(x,use_physical=False),
                                                             thetasTrack[ii])
                    allAcfsTrack[ii,:]= multiOut[0]
                    alljacsTrack[ii,:,:]= multiOut[1]
                    allinvjacsTrack[ii,:,:]= multiOut[2]
                    ObsTrack[ii,:]= multiOut[3]
                    ObsTrackAA[ii,:]= multiOut[4]
                    detdOdJps[ii]= multiOut[5]
            else:
                multiOut= multi.parallel_map(\
                    (lambda x: _determine_stream_track_single(self._aA,Orbit(ObsTrack[x,:]),0.,
                                                              self._progenitor_angle,
                                                              self._sigMeanSign,
                                                              self._dsigomeanProgDirection,
                                                              lambda x: self.meanOmega(x,use_physical=False),
                                                              thetasTrack[x])),
                    range(self._nTrackChunks),
                    numcores=numpy.amin([self._nTrackChunks,
                                         multiprocessing.cpu_count(),
                                         self._multi]))
                for ii in range(self._nTrackChunks):
                    allAcfsTrack[ii,:]= multiOut[ii][0]
                    alljacsTrack[ii,:,:]= multiOut[ii][1]
                    allinvjacsTrack[ii,:,:]= multiOut[ii][2]
                    ObsTrack[ii,:]= multiOut[ii][3]
                    ObsTrackAA[ii,:]= multiOut[ii][4]
                    detdOdJps[ii]= multiOut[ii][5]           
        #Store the track
        self._thetasTrack= thetasTrack
        self._ObsTrack= ObsTrack
        self._ObsTrackAA= ObsTrackAA
        self._allAcfsTrack= allAcfsTrack
        self._alljacsTrack= alljacsTrack
        self._allinvjacsTrack= allinvjacsTrack
        self._detdOdJps= detdOdJps
        self._meandetdOdJp= numpy.mean(self._detdOdJps)
        self._logmeandetdOdJp= numpy.log(self._meandetdOdJp)
        self._calc_ObsTrackXY()
        return None",Determine the track of the stream in real space
"def from_camel_case(s):
    """"""
    Convert names in camel case into underscore (_) separated names

    :param s: string in CamelCase notation
    :returns: string in under_score notation

    Example:

    >>> from_camel_case(""TableName"") # yields ""table_name""

    """"""

    def convert(match):
        return ('_' if match.groups()[0] else '') + match.group(0).lower()

    if not re.match(r'[A-Z][a-zA-Z0-9]*', s):
        raise DataJointError(
            'ClassName must be alphanumeric in CamelCase, begin with a capital letter')
    return re.sub(r'(\B[A-Z])|(\b[A-Z])', convert, s)","Convert names in camel case into underscore (_) separated names

    :param s: string in CamelCase notation
    :returns: string in under_score notation

    Example:

    >>> from_camel_case(""TableName"") # yields ""table_name"""
"def set_resource_type(self, klass):
        """"""
        set type to load and load schema
        """"""
        self.resource_type = klass
        self.schema = loaders.load_schema_raw(self.resource_type)",set type to load and load schema
"def hessian(self, x, y, Rs, theta_Rs, r_core, center_x=0, center_y=0):

        """"""
        :param x: x coordinate
        :param y: y coordinate
        :param Rs: scale radius
        :param rho0: central core density
        :param r_core: core radius
        :param center_x:
        :param center_y:
        :return:
        """"""

        if Rs < 0.0001:
            Rs = 0.0001
        x_ = x - center_x
        y_ = y - center_y
        R = np.sqrt(x_ ** 2 + y_ ** 2)

        rho0 = self._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs, r_core=r_core)

        kappa = self.density_2d(x_, y_, Rs, rho0, r_core)

        gamma1, gamma2 = self.cBurkGamma(R, Rs, rho0, r_core, x_, y_)
        f_xx = kappa + gamma1
        f_yy = kappa - gamma1
        f_xy = gamma2
        return f_xx, f_yy, f_xy",":param x: x coordinate
        :param y: y coordinate
        :param Rs: scale radius
        :param rho0: central core density
        :param r_core: core radius
        :param center_x:
        :param center_y:
        :return:"
"def SynchronizedClassMethod(*locks_attr_names, **kwargs):
    # pylint: disable=C1801
    """"""
    A synchronizer decorator for class methods. An AttributeError can be raised
    at runtime if the given lock attribute doesn't exist or if it is None.

    If a parameter ``sorted`` is found in ``kwargs`` and its value is True,
    then the list of locks names will be sorted before locking.

    :param locks_attr_names: A list of the lock(s) attribute(s) name(s) to be
                             used for synchronization
    :return: The decorator method, surrounded with the lock
    """"""
    # Filter the names (remove empty ones)
    locks_attr_names = [
        lock_name for lock_name in locks_attr_names if lock_name
    ]

    if not locks_attr_names:
        raise ValueError(""The lock names list can't be empty"")

    if ""sorted"" not in kwargs or kwargs[""sorted""]:
        # Sort the lock names if requested
        # (locking always in the same order reduces the risk of dead lock)
        locks_attr_names = list(locks_attr_names)
        locks_attr_names.sort()

    def wrapped(method):
        """"""
        The wrapping method

        :param method: The wrapped method
        :return: The wrapped method
        :raise AttributeError: The given attribute name doesn't exist
        """"""

        @functools.wraps(method)
        def synchronized(self, *args, **kwargs):
            """"""
            Calls the wrapped method with a lock
            """"""
            # Raises an AttributeError if needed
            locks = [getattr(self, attr_name) for attr_name in locks_attr_names]
            locked = collections.deque()
            i = 0

            try:
                # Lock
                for lock in locks:
                    if lock is None:
                        # No lock...
                        raise AttributeError(
                            ""Lock '{0}' can't be None in class {1}"".format(
                                locks_attr_names[i], type(self).__name__
                            )
                        )

                    # Get the lock
                    i += 1
                    lock.acquire()
                    locked.appendleft(lock)

                # Use the method
                return method(self, *args, **kwargs)

            finally:
                # Unlock what has been locked in all cases
                for lock in locked:
                    lock.release()

                locked.clear()
                del locks[:]

        return synchronized

    # Return the wrapped method
    return wrapped","A synchronizer decorator for class methods. An AttributeError can be raised
    at runtime if the given lock attribute doesn't exist or if it is None.

    If a parameter ``sorted`` is found in ``kwargs`` and its value is True,
    then the list of locks names will be sorted before locking.

    :param locks_attr_names: A list of the lock(s) attribute(s) name(s) to be
                             used for synchronization
    :return: The decorator method, surrounded with the lock"
"def omit_loglevel(self, msg) -> bool:
        """"""Determine if message is below log level.""""""
        return self.loglevels and (
            self.loglevels[0] > fontbakery.checkrunner.Status(msg)
        )",Determine if message is below log level.
"def _query_near(*, session=None, **kwargs):
    """"""Query marine database with given query string values and keys""""""
    url_endpoint = 'http://calib.org/marine/index.html'
    if session is not None:
        resp = session.get(url_endpoint, params=kwargs)
    else:
        with requests.Session() as s:
            # Need to get the index page before query. Otherwise get bad query response that seems legit.
            s.get('http://calib.org/marine/index.html')
            resp = s.get(url_endpoint, params=kwargs)
    return resp",Query marine database with given query string values and keys
"def frombed(args):
    """"""
    %prog frombed bedfile contigfasta readfasta

    Convert read placement to contig format. This is useful before running BAMBUS.
    """"""
    from jcvi.formats.fasta import Fasta
    from jcvi.formats.bed import Bed
    from jcvi.utils.cbook import fill

    p = OptionParser(frombed.__doc__)
    opts, args = p.parse_args(args)

    if len(args) != 3:
        sys.exit(not p.print_help())

    bedfile, contigfasta, readfasta = args
    prefix = bedfile.rsplit(""."", 1)[0]
    contigfile = prefix + "".contig""
    idsfile = prefix + "".ids""

    contigfasta = Fasta(contigfasta)
    readfasta = Fasta(readfasta)

    bed = Bed(bedfile)
    checksum = ""00000000 checksum.""
    fw_ids = open(idsfile, ""w"")
    fw = open(contigfile, ""w"")

    for ctg, reads in bed.sub_beds():
        ctgseq = contigfasta[ctg]
        ctgline = ""##{0} {1} {2} bases, {3}"".format(\
                ctg, len(reads), len(ctgseq), checksum)

        print(ctg, file=fw_ids)
        print(ctgline, file=fw)
        print(fill(ctgseq.seq), file=fw)

        for b in reads:
            read = b.accn
            strand = b.strand
            readseq = readfasta[read]
            rc = "" [RC]"" if strand == ""-"" else """"
            readlen = len(readseq)
            rstart, rend = 1, readlen
            if strand == ""-"":
                rstart, rend = rend, rstart

            readrange = ""{{{0} {1}}}"".format(rstart, rend)
            conrange = ""<{0} {1}>"".format(b.start, b.end)
            readline = ""#{0}(0){1} {2} bases, {3} {4} {5}"".format(\
                    read, rc, readlen, checksum, readrange, conrange)
            print(readline, file=fw)
            print(fill(readseq.seq), file=fw)

    logging.debug(""Mapped contigs written to `{0}`."".format(contigfile))
    logging.debug(""Contig IDs written to `{0}`."".format(idsfile))","%prog frombed bedfile contigfasta readfasta

    Convert read placement to contig format. This is useful before running BAMBUS."
"def get_raw_data_from_url(pdb_id, reduced=False):
    """""""" Get the msgpack unpacked data given a PDB id.

    :param pdb_id: the input PDB id
    :return the unpacked data (a dict) """"""
    url = get_url(pdb_id,reduced)
    request = urllib2.Request(url)
    request.add_header('Accept-encoding', 'gzip')
    response = urllib2.urlopen(request)
    if response.info().get('Content-Encoding') == 'gzip':
        data = ungzip_data(response.read())
    else:
        data = response.read()
    return _unpack(data)","Get the msgpack unpacked data given a PDB id.

    :param pdb_id: the input PDB id
    :return the unpacked data (a dict)"
"def sample_with_temperature(logits, temperature, sampling_keep_top_k=-1):
  """"""Either argmax or random sampling.

  Args:
    logits: a Tensor.
    temperature: a float  0.0=argmax 1.0=random
    sampling_keep_top_k: If not -1, only sample from the top k logits.
  Returns:
    a Tensor with one fewer dimension than logits.
  """"""
  if temperature == 0.0:
    # TF argmax doesn't handle >5 dimensions, so we reshape here.
    logits_shape = shape_list(logits)
    argmax = tf.argmax(tf.reshape(logits, [-1, logits_shape[-1]]), axis=1)
    return tf.reshape(argmax, logits_shape[:-1])
  else:
    assert temperature > 0.0

    if sampling_keep_top_k != -1:
      if sampling_keep_top_k <= 0:
        raise ValueError(""sampling_keep_top_k must either be -1 or positive."")

      vocab_size = shape_list(logits)[1]

      k_largest = tf.contrib.nn.nth_element(
          logits, n=sampling_keep_top_k, reverse=True)
      k_largest = tf.tile(tf.reshape(k_largest, [-1, 1]), [1, vocab_size])

      # Force every position that is not in the top k to have probability near
      # 0 by setting the logit to be very negative.
      logits = tf.where(tf.less_equal(logits, k_largest),
                        tf.ones_like(logits)*-1e6, logits)

    reshaped_logits = (
        tf.reshape(logits, [-1, shape_list(logits)[-1]]) / temperature)
    choices = tf.multinomial(reshaped_logits, 1)
    choices = tf.reshape(choices,
                         shape_list(logits)[:logits.get_shape().ndims - 1])
    return choices","Either argmax or random sampling.

  Args:
    logits: a Tensor.
    temperature: a float  0.0=argmax 1.0=random
    sampling_keep_top_k: If not -1, only sample from the top k logits.
  Returns:
    a Tensor with one fewer dimension than logits."
"def _to_list(obj):
    '''
    Convert snetinfo object to list
    '''
    ret = {}

    for attr in __attrs:
        if hasattr(obj, attr):
            ret[attr] = getattr(obj, attr)
    return ret",Convert snetinfo object to list
"def build_genome_alignment_from_directory(d_name, ref_spec, extensions=None,
                                          index_exts=None,
                                          fail_no_index=False):
  """"""
  build a genome aligment by loading all files in a directory.

  Fiel without indexes are loaded immediately; those with indexes are
  loaded on-demand. Not recursive (i.e. subdirectories are not parsed).

  :param d_name:        directory to load from.
  :param ref_spec:      which species in the alignemnt files is the reference?
  :param extensions:    list or set of acceptable extensions; treat any files
                        with these extensions as part of the alignment. If
                        None, treat any file which has an extension that is
                        NOT in index_extensions as part of the alignment.
  :param index_exts:    treat any files with these extensions as index files.
  :param fail_no_index: fail if index extensions are provided and an alignment
                        file has no index file.
  """"""
  if index_exts is None and fail_no_index:
    raise ValueError(""Failure on no index specified for loading genome "" +
                     ""alignment, but no index extensions specified"")

  blocks = []
  for fn in os.listdir(d_name):
    pth = os.path.join(d_name, fn)
    if os.path.isfile(pth):
      _, ext = os.path.splitext(pth)
      if extensions is None or ext in extensions:
        idx_path = __find_index(pth, index_exts)
        if idx_path is None and fail_no_index:
          raise PyokitIOError(""No index file for "" + fn)
        for b in genome_alignment_iterator(pth, ref_spec, idx_path):
          blocks.append(b)
  return GenomeAlignment(blocks)","build a genome aligment by loading all files in a directory.

  Fiel without indexes are loaded immediately; those with indexes are
  loaded on-demand. Not recursive (i.e. subdirectories are not parsed).

  :param d_name:        directory to load from.
  :param ref_spec:      which species in the alignemnt files is the reference?
  :param extensions:    list or set of acceptable extensions; treat any files
                        with these extensions as part of the alignment. If
                        None, treat any file which has an extension that is
                        NOT in index_extensions as part of the alignment.
  :param index_exts:    treat any files with these extensions as index files.
  :param fail_no_index: fail if index extensions are provided and an alignment
                        file has no index file."
"def _SignedVarintEncoder():
  """"""Return an encoder for a basic signed varint value (does not include
  tag).""""""

  def EncodeSignedVarint(write, value):
    if value < 0:
      value += (1 << 64)
    bits = value & 0x7f
    value >>= 7
    while value:
      write(six.int2byte(0x80|bits))
      bits = value & 0x7f
      value >>= 7
    return write(six.int2byte(bits))

  return EncodeSignedVarint","Return an encoder for a basic signed varint value (does not include
  tag)."
"def getCmd(snmpEngine, authData, transportTarget, contextData,
           *varBinds, **options):
    """"""Creates a generator to perform SNMP GET query.

    When iterator gets advanced by :py:mod:`asyncio` main loop,
    SNMP GET request is send (:RFC:`1905#section-4.2.1`).
    The iterator yields :py:class:`asyncio.Future` which gets done whenever
    response arrives or error occurs.

    Parameters
    ----------
    snmpEngine : :py:class:`~pysnmp.hlapi.SnmpEngine`
        Class instance representing SNMP engine.

    authData : :py:class:`~pysnmp.hlapi.CommunityData` or :py:class:`~pysnmp.hlapi.UsmUserData`
        Class instance representing SNMP credentials.

    transportTarget : :py:class:`~pysnmp.hlapi.asyncio.UdpTransportTarget` or :py:class:`~pysnmp.hlapi.asyncio.Udp6TransportTarget`
        Class instance representing transport type along with SNMP peer address.

    contextData : :py:class:`~pysnmp.hlapi.ContextData`
        Class instance representing SNMP ContextEngineId and ContextName values.

    \*varBinds : :py:class:`~pysnmp.smi.rfc1902.ObjectType`
        One or more class instances representing MIB variables to place
        into SNMP request.

    Other Parameters
    ----------------
    \*\*options :
        Request options:

            * `lookupMib` - load MIB and resolve response MIB variables at
              the cost of slightly reduced performance. Default is `True`.

    Yields
    ------
    errorIndication : str
        True value indicates SNMP engine error.
    errorStatus : str
        True value indicates SNMP PDU error.
    errorIndex : int
        Non-zero value refers to `varBinds[errorIndex-1]`
    varBinds : tuple
        A sequence of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class
        instances representing MIB variables returned in SNMP response.

    Raises
    ------
    PySnmpError
        Or its derivative indicating that an error occurred while
        performing SNMP operation.

    Examples
    --------
    >>> import asyncio
    >>> from pysnmp.hlapi.asyncio import *
    >>>
    >>> @asyncio.coroutine
    ... def run():
    ...     errorIndication, errorStatus, errorIndex, varBinds = yield from getCmd(
    ...         SnmpEngine(),
    ...         CommunityData('public'),
    ...         UdpTransportTarget(('demo.snmplabs.com', 161)),
    ...         ContextData(),
    ...         ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0))
    ...     )
    ...     print(errorIndication, errorStatus, errorIndex, varBinds)
    >>>
    >>> asyncio.get_event_loop().run_until_complete(run())
    (None, 0, 0, [ObjectType(ObjectIdentity(ObjectName('1.3.6.1.2.1.1.1.0')), DisplayString('SunOS zeus.snmplabs.com 4.1.3_U1 1 sun4m'))])
    >>>

    """"""

    def __cbFun(snmpEngine, sendRequestHandle,
                errorIndication, errorStatus, errorIndex,
                varBinds, cbCtx):
        lookupMib, future = cbCtx

        if future.cancelled():
            return

        try:
            varBindsUnmade = VB_PROCESSOR.unmakeVarBinds(
                snmpEngine.cache, varBinds, lookupMib)

        except Exception as e:
            future.set_exception(e)

        else:
            future.set_result(
                (errorIndication, errorStatus, errorIndex, varBindsUnmade))

    addrName, paramsName = LCD.configure(
        snmpEngine, authData, transportTarget, contextData.contextName)

    future = asyncio.Future()

    cmdgen.GetCommandGenerator().sendVarBinds(
        snmpEngine, addrName, contextData.contextEngineId,
        contextData.contextName,
        VB_PROCESSOR.makeVarBinds(snmpEngine.cache, varBinds), __cbFun,
        (options.get('lookupMib', True), future)
    )

    return future","Creates a generator to perform SNMP GET query.

    When iterator gets advanced by :py:mod:`asyncio` main loop,
    SNMP GET request is send (:RFC:`1905#section-4.2.1`).
    The iterator yields :py:class:`asyncio.Future` which gets done whenever
    response arrives or error occurs.

    Parameters
    ----------
    snmpEngine : :py:class:`~pysnmp.hlapi.SnmpEngine`
        Class instance representing SNMP engine.

    authData : :py:class:`~pysnmp.hlapi.CommunityData` or :py:class:`~pysnmp.hlapi.UsmUserData`
        Class instance representing SNMP credentials.

    transportTarget : :py:class:`~pysnmp.hlapi.asyncio.UdpTransportTarget` or :py:class:`~pysnmp.hlapi.asyncio.Udp6TransportTarget`
        Class instance representing transport type along with SNMP peer address.

    contextData : :py:class:`~pysnmp.hlapi.ContextData`
        Class instance representing SNMP ContextEngineId and ContextName values.

    \*varBinds : :py:class:`~pysnmp.smi.rfc1902.ObjectType`
        One or more class instances representing MIB variables to place
        into SNMP request.

    Other Parameters
    ----------------
    \*\*options :
        Request options:

            * `lookupMib` - load MIB and resolve response MIB variables at
              the cost of slightly reduced performance. Default is `True`.

    Yields
    ------
    errorIndication : str
        True value indicates SNMP engine error.
    errorStatus : str
        True value indicates SNMP PDU error.
    errorIndex : int
        Non-zero value refers to `varBinds[errorIndex-1]`
    varBinds : tuple
        A sequence of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class
        instances representing MIB variables returned in SNMP response.

    Raises
    ------
    PySnmpError
        Or its derivative indicating that an error occurred while
        performing SNMP operation.

    Examples
    --------
    >>> import asyncio
    >>> from pysnmp.hlapi.asyncio import *
    >>>
    >>> @asyncio.coroutine
    ... def run():
    ...     errorIndication, errorStatus, errorIndex, varBinds = yield from getCmd(
    ...         SnmpEngine(),
    ...         CommunityData('public'),
    ...         UdpTransportTarget(('demo.snmplabs.com', 161)),
    ...         ContextData(),
    ...         ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0))
    ...     )
    ...     print(errorIndication, errorStatus, errorIndex, varBinds)
    >>>
    >>> asyncio.get_event_loop().run_until_complete(run())
    (None, 0, 0, [ObjectType(ObjectIdentity(ObjectName('1.3.6.1.2.1.1.1.0')), DisplayString('SunOS zeus.snmplabs.com 4.1.3_U1 1 sun4m'))])
    >>>"
"def configure(self):
        # type: () -> None
        """"""
            Configures object based on its initialization
        """"""
        for i in vars(self):
            if i.startswith(""_""):
                continue
            val = self.__get(i, return_type=type(getattr(self, i)))
            if val is not None:
                setattr(self, i, val)",Configures object based on its initialization
"def _uncythonized_model(self, beta):
        """""" Creates the structure of the model

        Parameters
        ----------
        beta : np.array
            Contains untransformed starting values for latent variables

        Returns
        ----------
        theta : np.array
            Contains the predicted values for the time series

        Y : np.array
            Contains the length-adjusted time series (accounting for lags)

        scores : np.array
            Contains the scores for the time series
        """"""

        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])
        coefficients = np.zeros((self.X.shape[1],self.model_Y.shape[0]+1))
        coefficients[:,0] = self.initial_values
        theta = np.zeros(self.model_Y.shape[0]+1)
        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)

        # Loop over time series
        theta, self.model_scores, coefficients = gas_reg_recursion(parm, theta, self.X, coefficients, self.model_scores, self.model_Y, self.model_Y.shape[0], 
            self.family.reg_score_function, self.link, model_scale, model_shape, model_skewness, self.max_lag)

        return theta[:-1], self.model_Y, self.model_scores, coefficients","Creates the structure of the model

        Parameters
        ----------
        beta : np.array
            Contains untransformed starting values for latent variables

        Returns
        ----------
        theta : np.array
            Contains the predicted values for the time series

        Y : np.array
            Contains the length-adjusted time series (accounting for lags)

        scores : np.array
            Contains the scores for the time series"
"def ccmodmd_setcoef(k):
    """"""Set the coefficient maps for the ccmod stage. The only parameter is
    the slice index `k` and there are no return values; all inputs and
    outputs are from and to global variables.
    """"""

    # Set working coefficient maps for ccmod step and compute DFT of
    # coefficient maps Z
    mp_Zf[k] = sl.rfftn(mp_Z_Y1[k], mp_cri.Nv, mp_cri.axisN)","Set the coefficient maps for the ccmod stage. The only parameter is
    the slice index `k` and there are no return values; all inputs and
    outputs are from and to global variables."
"def skip_connections_distance(list_a, list_b):
    """"""The distance between the skip-connections of two neural networks.""""""
    distance_matrix = np.zeros((len(list_a), len(list_b)))
    for i, a in enumerate(list_a):
        for j, b in enumerate(list_b):
            distance_matrix[i][j] = skip_connection_distance(a, b)
    return distance_matrix[linear_sum_assignment(distance_matrix)].sum() + abs(
        len(list_a) - len(list_b)
    )",The distance between the skip-connections of two neural networks.
"def useragent(self, value):
        """"""gets/sets the user agent value""""""
        if value is None:
            self._useragent = ""Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0""
        elif self._useragent != value:
            self._useragent = value",gets/sets the user agent value
"def merge_truthy(*dicts):
    """"""Merge multiple dictionaries, keeping the truthy values in case of key collisions.

    Accepts any number of dictionaries, or any other object that returns a 2-tuple of
    key and value pairs when its `.items()` method is called.

    If a key exists in multiple dictionaries passed to this function, the values from the latter
    dictionary is kept. If the value of the latter dictionary does not evaluate to True, then
    the value of the previous dictionary is kept.

    >>> merge_truthy({'a': 1, 'c': 4}, {'a': None, 'b': 2}, {'b': 3})
    {'a': 1, 'b': 3, 'c': 4}
    """"""
    merged = {}
    for d in dicts:
        for k, v in d.items():
            merged[k] = v or merged.get(k, v)
    return merged","Merge multiple dictionaries, keeping the truthy values in case of key collisions.

    Accepts any number of dictionaries, or any other object that returns a 2-tuple of
    key and value pairs when its `.items()` method is called.

    If a key exists in multiple dictionaries passed to this function, the values from the latter
    dictionary is kept. If the value of the latter dictionary does not evaluate to True, then
    the value of the previous dictionary is kept.

    >>> merge_truthy({'a': 1, 'c': 4}, {'a': None, 'b': 2}, {'b': 3})
    {'a': 1, 'b': 3, 'c': 4}"
"def create_anisophaplot(plotman, x, y, z, alpha, options):
    '''Plot the data of the tomodir in one overview plot.
    '''
    sizex, sizez = getfigsize(plotman)
    # create figure
    f, ax = plt.subplots(2, 3, figsize=(3 * sizex, 2 * sizez))
    if options.title is not None:
        plt.suptitle(options.title, fontsize=18)
        plt.subplots_adjust(wspace=1, top=0.8)
    # plot phase
    cidx = plotman.parman.add_data(x)
    cidy = plotman.parman.add_data(y)
    cidz = plotman.parman.add_data(z)
    cidxy = plotman.parman.add_data(np.subtract(x, y))
    cidyz = plotman.parman.add_data(np.subtract(y, z))
    cidzx = plotman.parman.add_data(np.subtract(z, x))
    plot_pha(cidx, ax[0, 0], plotman, 'x', alpha,
             options.pha_vmin, options.pha_vmax,
             options.xmin, options.xmax, options.zmin, options.zmax,
             options.unit, options.pha_cbtiks, options.no_elecs,
             )
    plot_pha(cidy, ax[0, 1], plotman, 'y', alpha,
             options.pha_vmin, options.pha_vmax,
             options.xmin, options.xmax, options.zmin, options.zmax,
             options.unit, options.pha_cbtiks, options.no_elecs,
             )
    plot_pha(cidz, ax[0, 2], plotman, 'z', alpha,
             options.pha_vmin, options.pha_vmax,
             options.xmin, options.xmax, options.zmin, options.zmax,
             options.unit, options.pha_cbtiks, options.no_elecs,
             )
    plot_ratio(cidxy, ax[1, 0], plotman, 'x-y', alpha,
               options.rat_vmin, options.rat_vmax,
               options.xmin, options.xmax, options.zmin, options.zmax,
               options.unit, options.mag_cbtiks, options.no_elecs,
               )
    plot_ratio(cidyz, ax[1, 1], plotman, 'y-z', alpha,
               options.rat_vmin, options.rat_vmax,
               options.xmin, options.xmax, options.zmin, options.zmax,
               options.unit, options.mag_cbtiks, options.no_elecs,
               )
    plot_ratio(cidzx, ax[1, 2], plotman, 'z-x', alpha,
               options.rat_vmin, options.rat_vmax,
               options.xmin, options.xmax, options.zmin, options.zmax,
               options.unit, options.mag_cbtiks, options.no_elecs,
               )
    f.tight_layout()
    f.savefig('pha_aniso.png', dpi=300)
    return f, ax",Plot the data of the tomodir in one overview plot.
"def u_grade_ipix(ipix, nside_in, nside_out, nest=False):
    """"""
    Return the indices of sub-pixels (resolution nside_subpix) within
    the super-pixel(s) (resolution nside_superpix).
    
    Parameters:
    -----------
    ipix      : index of the input superpixel(s)
    nside_in  : nside of the input superpixel
    nside_out : nside of the desired subpixels

    Returns:
    --------
    ipix_out : subpixels for each superpixel
    """"""

    if nside_in==nside_out: return ipix
    if not (nside_in < nside_out): 
        raise ValueError(""nside_in must be less than nside_out"")

    if nest: nest_ipix = ipix
    else:    nest_ipix = hp.ring2nest(nside_in, ipix)

    factor = (nside_out//nside_in)**2
    if np.isscalar(ipix):
        nest_ipix_out = factor*nest_ipix + np.arange(factor)
    else:
        nest_ipix_out = factor*np.asarray(nest_ipix)[:,np.newaxis]+np.arange(factor)

    if nest: return nest_ipix_out
    else:    return hp.nest2ring(nside_out, nest_ipix_out)","Return the indices of sub-pixels (resolution nside_subpix) within
    the super-pixel(s) (resolution nside_superpix).
    
    Parameters:
    -----------
    ipix      : index of the input superpixel(s)
    nside_in  : nside of the input superpixel
    nside_out : nside of the desired subpixels

    Returns:
    --------
    ipix_out : subpixels for each superpixel"
"def _set_country(self, c):
        """"""
        callback if we used Tor's GETINFO ip-to-country
        """"""

        self.location.countrycode = c.split()[0].split('=')[1].strip().upper()",callback if we used Tor's GETINFO ip-to-country
"def extract_file_args(subparsers):
    """"""Add the command line options for the extract_file operation""""""
    extract_parser = subparsers.add_parser('extract_file',
                                           help='Extract a single secret from'
                                           'Vault to a local file')
    extract_parser.add_argument('vault_path',
                                help='Full path (including key) to secret')
    extract_parser.add_argument('destination',
                                help='Location of destination file')
    base_args(extract_parser)",Add the command line options for the extract_file operation
"def bind_device_location(self, poi_id, device_id=None, uuid=None,
                             major=None, minor=None):
        """"""
        
        
        http://mp.weixin.qq.com/wiki/15/b9e012f917e3484b7ed02771156411f3.html

        :param poi_id: ID
        :param device_id: UUIDmajorminor
        :param uuid: UUID
        :param major: major
        :param minor: minor
        :return:  JSON 
        """"""
        data = optionaldict()
        data['poi_id'] = poi_id
        data['device_identifier'] = {
            'device_id': device_id,
            'uuid': uuid,
            'major': major,
            'minor': minor
        }
        return self._post(
            'shakearound/device/bindlocation',
            data=data
        )","
        
        http://mp.weixin.qq.com/wiki/15/b9e012f917e3484b7ed02771156411f3.html

        :param poi_id: ID
        :param device_id: UUIDmajorminor
        :param uuid: UUID
        :param major: major
        :param minor: minor
        :return:  JSON "
"def venn(args):
    """"""
    %prog venn *.benchmark

    Display benchmark results as Venn diagram.
    """"""
    from matplotlib_venn import venn2

    p = OptionParser(venn.__doc__)
    opts, args, iopts = p.set_image_options(args, figsize=""9x9"")

    if len(args) < 1:
        sys.exit(not p.print_help())

    bcs = args
    fig = plt.figure(1, (iopts.w, iopts.h))
    root = fig.add_axes([0, 0, 1, 1])

    pad = .02
    ystart = 1
    ywidth = 1. / len(bcs)
    tags = (""Bowers"", ""YGOB"", ""Schnable"")
    for bc, tag in zip(bcs, tags):
        fp = open(bc)
        data = []
        for row in fp:
            prog, pcounts, tcounts, shared = row.split()
            pcounts = int(pcounts)
            tcounts = int(tcounts)
            shared = int(shared)
            data.append((prog, pcounts, tcounts, shared))
        xstart = 0
        xwidth = 1. / len(data)
        for prog, pcounts, tcounts, shared in data:
            a, b, c = pcounts - shared, tcounts - shared, shared
            ax = fig.add_axes([xstart + pad, ystart - ywidth + pad,
                               xwidth - 2 * pad, ywidth - 2 * pad])
            venn2(subsets=(a, b, c), set_labels=(prog, tag), ax=ax)
            message = ""Sn={0} Pu={1}"".\
                format(percentage(shared, tcounts, precision=0, mode=-1),
                       percentage(shared, pcounts, precision=0, mode=-1))
            print(message, file=sys.stderr)
            ax.text(.5, .92, latex(message), ha=""center"", va=""center"",
                    transform=ax.transAxes, color='b')
            ax.set_axis_off()
            xstart += xwidth
        ystart -= ywidth

    panel_labels(root, ((.04, .96, ""A""), (.04, .96 - ywidth, ""B""),
                  (.04, .96 - 2 * ywidth, ""C"")))
    panel_labels(root, ((.5, .98, ""A. thaliana duplicates""),
                        (.5, .98 - ywidth, ""14 Yeast genomes""),
                        (.5, .98 - 2 * ywidth, ""4 Grass genomes"")))
    normalize_axes(root)
    savefig(""venn.pdf"", dpi=opts.dpi)","%prog venn *.benchmark

    Display benchmark results as Venn diagram."
"def extract_single_dist_for_current_platform(self, reqs, dist_key):
    """"""Resolve a specific distribution from a set of requirements matching the current platform.

    :param list reqs: A list of :class:`PythonRequirement` to resolve.
    :param str dist_key: The value of `distribution.key` to match for a `distribution` from the
                         resolved requirements.
    :return: The single :class:`pkg_resources.Distribution` matching `dist_key`.
    :raises: :class:`self.SingleDistExtractionError` if no dists or multiple dists matched the given
             `dist_key`.
    """"""
    distributions = self._resolve_distributions_by_platform(reqs, platforms=['current'])
    try:
      matched_dist = assert_single_element(list(
        dist
        for _, dists in distributions.items()
        for dist in dists
        if dist.key == dist_key
      ))
    except (StopIteration, ValueError) as e:
      raise self.SingleDistExtractionError(
        ""Exactly one dist was expected to match name {} in requirements {}: {}""
        .format(dist_key, reqs, e))
    return matched_dist","Resolve a specific distribution from a set of requirements matching the current platform.

    :param list reqs: A list of :class:`PythonRequirement` to resolve.
    :param str dist_key: The value of `distribution.key` to match for a `distribution` from the
                         resolved requirements.
    :return: The single :class:`pkg_resources.Distribution` matching `dist_key`.
    :raises: :class:`self.SingleDistExtractionError` if no dists or multiple dists matched the given
             `dist_key`."
"def insert_query(**kw):
    """"""
    Insert a query name for a node_id.
    `name`
    `node_id`

    Adds the name to the Query table if not already there. Sets the query field
    in Node table.
    """"""
    with current_app.app_context():
        result = db.execute(text(fetch_query_string('select_query_where_name.sql')), **kw).fetchall()
        if result:
            kw['query_id'] = result[0]['id']
        else:
            result = db.execute(text(fetch_query_string('insert_query.sql')), **kw)
            kw['query_id'] = result.lastrowid
            if (not kw['query_id']):
                result = result.fetchall()
                kw['query_id']  = result[0]['id']
        db.execute(text(fetch_query_string('insert_query_node.sql')), **kw)","Insert a query name for a node_id.
    `name`
    `node_id`

    Adds the name to the Query table if not already there. Sets the query field
    in Node table."
"def get_path(*args, module=a99):
    """"""Returns full path to specified module

    Args:
      *args: are added at the end of module path with os.path.join()
      module: Python module, defaults to a99

    Returns: path string

    >>> get_path()
    """"""

    p = os.path.abspath(os.path.join(os.path.split(module.__file__)[0], *args))
    return p","Returns full path to specified module

    Args:
      *args: are added at the end of module path with os.path.join()
      module: Python module, defaults to a99

    Returns: path string

    >>> get_path()"
"def addcols(self, desc, dminfo={}, addtoparent=True):
        """"""Add one or more columns.

        Columns can always be added to a normal table.
        They can also be added to a reference table and optionally to its
        parent table.

        `desc`
          contains a description of the column(s) to be added. It can be given
          in three ways:

          - a dict created by :func:`maketabdesc`. In this way multiple
            columns can be added.
          - a dict created by :func:`makescacoldesc`, :func:`makearrcoldesc`,
            or :func:`makecoldesc`. In this way a single column can be added.
          - a dict created by :func:`getcoldesc`. The key 'name' containing
            the column name has to be defined in such a dict.

        `dminfo`
          can be used to provide detailed data manager info to tell how the
          column(s) have to be stored. The dminfo of an existing column can be
          obtained using method :func:`getdminfo`.
        `addtoparent`
          defines if the column should also be added to the parent table in
          case the current table is a reference table (result of selection).
          If True, it will be added to the parent if it does not exist yet.

        For example, add a column using the same data manager type as another
        column::

          coldmi = t.getdminfo('colarrtsm')     # get dminfo of existing column
          coldmi[""NAME""] = 'tsm2'               # give it a unique name
          t.addcols (maketabdesc(makearrcoldesc(""colarrtsm2"",0., ndim=2)),
                     coldmi)

        """"""
        tdesc = desc
        # Create a tabdesc if only a coldesc is given.
        if 'name' in desc:
            import casacore.tables.tableutil as pt
            if len(desc) == 2 and 'desc' in desc:
                # Given as output from makecoldesc
                tdesc = pt.maketabdesc(desc)
            elif 'valueType' in desc:
                # Given as output of getcoldesc (with a name field added)
                cd = pt.makecoldesc(desc['name'], desc)
                tdesc = pt.maketabdesc(cd)
        self._addcols(tdesc, dminfo, addtoparent)
        self._makerow()","Add one or more columns.

        Columns can always be added to a normal table.
        They can also be added to a reference table and optionally to its
        parent table.

        `desc`
          contains a description of the column(s) to be added. It can be given
          in three ways:

          - a dict created by :func:`maketabdesc`. In this way multiple
            columns can be added.
          - a dict created by :func:`makescacoldesc`, :func:`makearrcoldesc`,
            or :func:`makecoldesc`. In this way a single column can be added.
          - a dict created by :func:`getcoldesc`. The key 'name' containing
            the column name has to be defined in such a dict.

        `dminfo`
          can be used to provide detailed data manager info to tell how the
          column(s) have to be stored. The dminfo of an existing column can be
          obtained using method :func:`getdminfo`.
        `addtoparent`
          defines if the column should also be added to the parent table in
          case the current table is a reference table (result of selection).
          If True, it will be added to the parent if it does not exist yet.

        For example, add a column using the same data manager type as another
        column::

          coldmi = t.getdminfo('colarrtsm')     # get dminfo of existing column
          coldmi[""NAME""] = 'tsm2'               # give it a unique name
          t.addcols (maketabdesc(makearrcoldesc(""colarrtsm2"",0., ndim=2)),
                     coldmi)"
"def compute(self):
        """"""
            Compute and return one solution. This method checks whether the
            hard part of the formula is satisfiable, i.e. an MCS can be
            extracted. If the formula is satisfiable, the model computed by the
            SAT call is used as an *over-approximation* of the MCS in the
            method :func:`_compute` invoked here, which implements the LBX
            algorithm.

            An MCS is reported as a list of integers, each representing a soft
            clause index (the smallest index is ``1``).

            :rtype: list(int)
        """"""

        self.setd = []
        self.satc = [False for cl in self.soft]  # satisfied clauses
        self.solution = None
        self.bb_assumps = []  # backbone assumptions
        self.ss_assumps = []  # satisfied soft clause assumptions

        if self.oracle.solve():
            # hard part is satisfiable => there is a solution
            self._filter_satisfied(update_setd=True)
            self._compute()

            self.solution = list(map(lambda i: i + 1, filter(lambda i: not self.satc[i], range(len(self.soft)))))

        return self.solution","Compute and return one solution. This method checks whether the
            hard part of the formula is satisfiable, i.e. an MCS can be
            extracted. If the formula is satisfiable, the model computed by the
            SAT call is used as an *over-approximation* of the MCS in the
            method :func:`_compute` invoked here, which implements the LBX
            algorithm.

            An MCS is reported as a list of integers, each representing a soft
            clause index (the smallest index is ``1``).

            :rtype: list(int)"
"def handle_request(request, validator_map, **kwargs):
    """"""Validate the request against the swagger spec and return a dict with
    all parameter values available in the request, casted to the expected
    python type.

    :param request: a :class:`PyramidSwaggerRequest` to validate
    :param validator_map: a :class:`pyramid_swagger.load_schema.ValidatorMap`
        used to validate the request
    :returns: a :class:`dict` of request data for each parameter in the swagger
        spec
    :raises: RequestValidationError when the request is not valid for the
        swagger spec
    """"""
    request_data = {}
    validation_pairs = []

    for validator, values in [
        (validator_map.query, request.query),
        (validator_map.path, request.path),
        (validator_map.form, request.form),
        (validator_map.headers, request.headers),
    ]:
        values = cast_params(validator.schema, values)
        validation_pairs.append((validator, values))
        request_data.update(values)

    # Body is a special case because the key for the request_data comes
    # from the name in the schema, instead of keys in the values
    if validator_map.body.schema:
        param_name = validator_map.body.schema['name']
        validation_pairs.append((validator_map.body, request.body))
        request_data[param_name] = request.body

    validate_request(validation_pairs)

    return request_data","Validate the request against the swagger spec and return a dict with
    all parameter values available in the request, casted to the expected
    python type.

    :param request: a :class:`PyramidSwaggerRequest` to validate
    :param validator_map: a :class:`pyramid_swagger.load_schema.ValidatorMap`
        used to validate the request
    :returns: a :class:`dict` of request data for each parameter in the swagger
        spec
    :raises: RequestValidationError when the request is not valid for the
        swagger spec"
"def _setup_chassis(self):
        """"""
        Sets up the router with the corresponding chassis
        (create slots and insert default adapters).
        """"""

        self._create_slots(2)
        self._slots[0] = self.integrated_adapters[self._chassis]()","Sets up the router with the corresponding chassis
        (create slots and insert default adapters)."
"def tacacs_server_host_retries(self, **kwargs):
        """"""Auto Generated Code
        """"""
        config = ET.Element(""config"")
        tacacs_server = ET.SubElement(config, ""tacacs-server"", xmlns=""urn:brocade.com:mgmt:brocade-aaa"")
        host = ET.SubElement(tacacs_server, ""host"")
        hostname_key = ET.SubElement(host, ""hostname"")
        hostname_key.text = kwargs.pop('hostname')
        retries = ET.SubElement(host, ""retries"")
        retries.text = kwargs.pop('retries')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)",Auto Generated Code
"def get_num_chunks(length, chunksize):
    r""""""
    Returns the number of chunks that a list will be split into given a
    chunksize.

    Args:
        length (int):
        chunksize (int):

    Returns:
        int: n_chunks

    CommandLine:
        python -m utool.util_progress --exec-get_num_chunks:0

    Example0:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_progress import *  # NOQA
        >>> length = 2000
        >>> chunksize = 256
        >>> n_chunks = get_num_chunks(length, chunksize)
        >>> result = ('n_chunks = %s' % (six.text_type(n_chunks),))
        >>> print(result)
        n_chunks = 8
    """"""
    n_chunks = int(math.ceil(length / chunksize))
    return n_chunks","r""""""
    Returns the number of chunks that a list will be split into given a
    chunksize.

    Args:
        length (int):
        chunksize (int):

    Returns:
        int: n_chunks

    CommandLine:
        python -m utool.util_progress --exec-get_num_chunks:0

    Example0:
        >>> # ENABLE_DOCTEST
        >>> from utool.util_progress import *  # NOQA
        >>> length = 2000
        >>> chunksize = 256
        >>> n_chunks = get_num_chunks(length, chunksize)
        >>> result = ('n_chunks = %s' % (six.text_type(n_chunks),))
        >>> print(result)
        n_chunks = 8"
"def wrap(self, text):
        '''Wraps the text object to width, breaking at whitespaces. Runs of
        whitespace characters are preserved, provided they do not fall at a
        line boundary. The implementation is based on that of textwrap from the
        standard library, but we can cope with StringWithFormatting objects.

        :returns: a list of string-like objects.
        '''
        result = []
        chunks = self._chunk(text)
        while chunks:
            self._lstrip(chunks)
            current_line = []
            current_line_length = 0
            current_chunk_length = 0
            while chunks:
                current_chunk_length = len(chunks[0])
                if current_line_length + current_chunk_length <= self.width:
                    current_line.append(chunks.pop(0))
                    current_line_length += current_chunk_length
                else:
                    # Line is full
                    break
            # Handle case where chunk is bigger than an entire line
            if current_chunk_length > self.width:
                space_left = self.width - current_line_length
                current_line.append(chunks[0][:space_left])
                chunks[0] = chunks[0][space_left:]
            self._rstrip(current_line)
            if current_line:
                result.append(reduce(
                    lambda x, y: x + y, current_line[1:], current_line[0]))
            else:
                # FIXME: should this line go? Removing it makes at least simple
                # cases like wrap('    ', 10) actually behave like
                # textwrap.wrap...
                result.append('')
        return result","Wraps the text object to width, breaking at whitespaces. Runs of
        whitespace characters are preserved, provided they do not fall at a
        line boundary. The implementation is based on that of textwrap from the
        standard library, but we can cope with StringWithFormatting objects.

        :returns: a list of string-like objects."
"def nanargmax(values, axis=None, skipna=True, mask=None):
    """"""
    Parameters
    ----------
    values : ndarray
    axis: int, optional
    skipna : bool, default True
    mask : ndarray[bool], optional
        nan-mask if known

    Returns
    --------
    result : int
        The index of max value in specified axis or -1 in the NA case

    Examples
    --------
    >>> import pandas.core.nanops as nanops
    >>> s = pd.Series([1, 2, 3, np.nan, 4])
    >>> nanops.nanargmax(s)
    4
    """"""
    values, mask, dtype, _, _ = _get_values(
        values, skipna, fill_value_typ='-inf', mask=mask)
    result = values.argmax(axis)
    result = _maybe_arg_null_out(result, axis, mask, skipna)
    return result","Parameters
    ----------
    values : ndarray
    axis: int, optional
    skipna : bool, default True
    mask : ndarray[bool], optional
        nan-mask if known

    Returns
    --------
    result : int
        The index of max value in specified axis or -1 in the NA case

    Examples
    --------
    >>> import pandas.core.nanops as nanops
    >>> s = pd.Series([1, 2, 3, np.nan, 4])
    >>> nanops.nanargmax(s)
    4"
"def run(self, d, x):
        """"""
        This function filters multiple samples in a row.

        **Args:**

        * `d` : desired value (1 dimensional array)

        * `x` : input matrix (2-dimensional array). Rows are samples,
          columns are input arrays.

        **Returns:**

        * `y` : output value (1 dimensional array).
          The size corresponds with the desired value.

        * `e` : filter error for every sample (1 dimensional array).
          The size corresponds with the desired value.

        * `w` : history of all weights (2 dimensional array).
          Every row is set of the weights for given sample.
        """"""
        # measure the data and check if the dimmension agree
        N = len(x)
        if not len(d) == N:
            raise ValueError('The length of vector d and matrix x must agree.')  
        self.n = len(x[0])
        # prepare data
        try:    
            x = np.array(x)
            d = np.array(d)
        except:
            raise ValueError('Impossible to convert x or d to a numpy array')
        # create empty arrays
        y = np.zeros(N)
        e = np.zeros(N)
        self.w_history = np.zeros((N,self.n))
        # adaptation loop
        for k in range(N):
            self.w_history[k,:] = self.w
            y[k] = np.dot(self.w, x[k])
            e[k] = d[k] - y[k]
            nu = self.mu / (self.eps + np.dot(x[k], x[k]))
            dw = nu * x[k] * e[k]**3
            self.w += dw
        return y, e, self.w_history","This function filters multiple samples in a row.

        **Args:**

        * `d` : desired value (1 dimensional array)

        * `x` : input matrix (2-dimensional array). Rows are samples,
          columns are input arrays.

        **Returns:**

        * `y` : output value (1 dimensional array).
          The size corresponds with the desired value.

        * `e` : filter error for every sample (1 dimensional array).
          The size corresponds with the desired value.

        * `w` : history of all weights (2 dimensional array).
          Every row is set of the weights for given sample."
"def current_api_key():
    """"""Determines the API key for the current request.

    Returns:
        The ApiKey instance.
    """"""
    if app.config.get('IGNORE_AUTH'):
        return models.ApiKey(
            id='anonymous_superuser',
            secret='',
            superuser=True)

    ops = _get_api_key_ops()
    api_key = ops.get()
    logging.debug('Authenticated as API key=%r', api_key.id)

    return api_key","Determines the API key for the current request.

    Returns:
        The ApiKey instance."
"def new_file(self):
        """"""
        Creates a new editor file.

        :return: File name.
        :rtype: unicode
        """"""

        file = self.get_untitled_file_name()
        LOGGER.debug(""> Creating '{0}' file."".format(file))
        self.set_file(file, is_modified=False, is_untitled=True)
        self.__set_document_signals()
        return file","Creates a new editor file.

        :return: File name.
        :rtype: unicode"
"def _read_geojson_features(data, features=None, prefix=""""):
        """"""Return a dict of features keyed by ID.""""""
        if features is None:
            features = collections.OrderedDict()
        for i, feature in enumerate(data['features']):
            key = feature.get('id', prefix + str(i))
            feature_type = feature['geometry']['type']
            if feature_type == 'FeatureCollection':
                _read_geojson_features(feature, features, prefix + '.' + key)
            elif feature_type == 'Point':
                value = Circle._convert_point(feature)
            elif feature_type in ['Polygon', 'MultiPolygon']:
                value = Region(feature)
            else:
                # TODO Support all http://geojson.org/geojson-spec.html#geometry-objects
                value = None
            features[key] = value
        return features",Return a dict of features keyed by ID.
"def handle_offchain_secretreveal(
        initiator_state: InitiatorTransferState,
        state_change: ReceiveSecretReveal,
        channel_state: NettingChannelState,
        pseudo_random_generator: random.Random,
) -> TransitionResult[InitiatorTransferState]:
    """""" Once the next hop proves it knows the secret, the initiator can unlock
    the mediated transfer.

    This will validate the secret, and if valid a new balance proof is sent to
    the next hop with the current lock removed from the merkle tree and the
    transferred amount updated.
    """"""
    iteration: TransitionResult[InitiatorTransferState]
    valid_reveal = is_valid_secret_reveal(
        state_change=state_change,
        transfer_secrethash=initiator_state.transfer_description.secrethash,
        secret=state_change.secret,
    )
    sent_by_partner = state_change.sender == channel_state.partner_state.address
    is_channel_open = channel.get_status(channel_state) == CHANNEL_STATE_OPENED

    if valid_reveal and is_channel_open and sent_by_partner:
        events = events_for_unlock_lock(
            initiator_state=initiator_state,
            channel_state=channel_state,
            secret=state_change.secret,
            secrethash=state_change.secrethash,
            pseudo_random_generator=pseudo_random_generator,
        )
        iteration = TransitionResult(None, events)
    else:
        events = list()
        iteration = TransitionResult(initiator_state, events)

    return iteration","Once the next hop proves it knows the secret, the initiator can unlock
    the mediated transfer.

    This will validate the secret, and if valid a new balance proof is sent to
    the next hop with the current lock removed from the merkle tree and the
    transferred amount updated."
"def on_default_shell_changed(self, combo):
        """"""Changes the activity of default_shell in dconf
        """"""
        citer = combo.get_active_iter()
        if not citer:
            return
        shell = combo.get_model().get_value(citer, 0)
        # we unset the value (restore to default) when user chooses to use
        # user shell as guake shell interpreter.
        if shell == USER_SHELL_VALUE:
            self.settings.general.reset('default-shell')
        else:
            self.settings.general.set_string('default-shell', shell)",Changes the activity of default_shell in dconf
"async def close_authenticator_async(self):
        """"""Close the CBS auth channel and session asynchronously.""""""
        _logger.info(""Shutting down CBS session on connection: %r."", self._connection.container_id)
        try:
            self._cbs_auth.destroy()
            _logger.info(""Auth closed, destroying session on connection: %r."", self._connection.container_id)
            await self._session.destroy_async()
        finally:
            _logger.info(""Finished shutting down CBS session on connection: %r."", self._connection.container_id)",Close the CBS auth channel and session asynchronously.
"def getOrCreate(cls, sc):
        """"""
        Get the existing SQLContext or create a new one with given SparkContext.

        :param sc: SparkContext
        """"""
        if cls._instantiatedContext is None:
            jsqlContext = sc._jvm.SQLContext.getOrCreate(sc._jsc.sc())
            sparkSession = SparkSession(sc, jsqlContext.sparkSession())
            cls(sc, sparkSession, jsqlContext)
        return cls._instantiatedContext","Get the existing SQLContext or create a new one with given SparkContext.

        :param sc: SparkContext"
"def list_(host=None, quiet=False, path=None):
    '''
    List defined containers (running, stopped, and frozen) for the named
    (or all) host(s).

    path
        path to the container parent
        default: /var/lib/lxc (system default)

        .. versionadded:: 2015.8.0

    .. code-block:: bash

        salt-run lxc.list [host=minion_id]
    '''
    it = _list_iter(host, path=path)
    ret = {}
    for chunk in it:
        ret.update(chunk)
        if not quiet:
            __jid_event__.fire_event(
                {'data': chunk, 'outputter': 'lxc_list'}, 'progress')
    return ret","List defined containers (running, stopped, and frozen) for the named
    (or all) host(s).

    path
        path to the container parent
        default: /var/lib/lxc (system default)

        .. versionadded:: 2015.8.0

    .. code-block:: bash

        salt-run lxc.list [host=minion_id]"
"def run_file(self, input_file, results):
        """"""
        use foctor_core library do get requests
        :param input_file: the file name of the list of test urls
                        format: 
                            1, www.facebook.com
                            2, www.google.com
                            ...
        :param results: the object to save the responses from server
        """"""

        capture_path = self.cur_path

        display_mode = 0  # 0 is virtural display(Xvfb mode)

        site_list = []
        file_name, file_contents = input_file
        result = {""file_name"": file_name}
        file_metadata = {}
        file_comments = []
        run_start_time = time.time()
        index = 1

        csvreader = csv.reader(file_contents, delimiter=',', quotechar='""')
        for row in csvreader:
            """"""
                First few lines are expected to be comments in key: value
                format. The first line after that could be our column header
                row, starting with ""url"", and the rest are data rows.
                This is a sample input file we're trying to parse:

                # comment: Global List,,,,,
                # date: 03-17-2015,,,,,
                # version: 1,,,,,
                # description: This is the global list. Last updated in 2012.,,,,
                url,country,category,description,rationale,provider
                http://8thstreetlatinas.com,glo,PORN,,,PRIV
                http://abpr2.railfan.net,glo,MISC,Pictures of trains,,PRIV

                """"""

            # parse file comments, if it looks like ""key : value"",
            # parse it as a key-value pair. otherwise, just
            # store it as a raw comment.
            if row[0][0] == '#':
                row = row[0][1:].strip()
                if len(row.split(':')) > 1:
                    key, value = row.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    file_metadata[key] = value
                else:
                    file_comments.append(row)
                continue

            # detect the header row and store it
            # it is usually the first row and starts with ""url,""
            if row[0].strip().lower() == ""url"":
                index_row = row
                continue

            url = row[0].strip()
            if url is None:
                continue

            meta = row[1:]
            site_list.append([index, url])
            index += 1

        driver, display = fc.do_crawl(sites=site_list, driver=self.driver, display=self.display,
                                      capture_path=capture_path, callback=self.wrap_results,
                                      external=results, fd=os.path.join(capture_path, ""har/""),
                                      files_count=len(os.listdir(os.path.join(capture_path, ""har/""))))
        fc.teardown_driver(driver, display, display_mode)

        driver.quit()","use foctor_core library do get requests
        :param input_file: the file name of the list of test urls
                        format: 
                            1, www.facebook.com
                            2, www.google.com
                            ...
        :param results: the object to save the responses from server"
"def _get_sa_at_1180(self, C, imt, sites, rup, dists):
        """"""
        Compute and return mean imt value for rock conditions
        (vs30 = 1100 m/s)
        """"""
        # reference vs30 = 1180 m/s
        vs30_1180 = np.ones_like(sites.vs30) * 1180.
        # reference shaking intensity = 0
        ref_iml = np.zeros_like(sites.vs30)
        # fake Z1.0 - Since negative it will be replaced by the default Z1.0
        # for the corresponding region
        fake_z1pt0 = np.ones_like(sites.vs30) * -1
        return (self._get_basic_term(C, rup, dists) +
                self._get_faulting_style_term(C, rup) +
                self._get_site_response_term(C, imt, vs30_1180, ref_iml) +
                self._get_hanging_wall_term(C, dists, rup) +
                self._get_top_of_rupture_depth_term(C, imt, rup) +
                self._get_soil_depth_term(C, fake_z1pt0, vs30_1180) +
                self._get_regional_term(C, imt, vs30_1180, dists.rrup)
                )","Compute and return mean imt value for rock conditions
        (vs30 = 1100 m/s)"
"def det_point_position(self, mparam, dparam):
        """"""Return the detector point at ``(mparam, dparam)``.

        The position is computed as follows::

            pos = refpoint(mparam) +
                  rotation_matrix(mparam).dot(detector.surface(dparam))

        In other words, the motion parameter ``mparam`` is used to move the
        detector reference point, and the detector parameter ``dparam``
        defines an intrinsic shift that is added to the reference point.

        Parameters
        ----------
        mparam : `array-like` or sequence
            Motion parameter(s) at which to evaluate. If
            ``motion_params.ndim >= 2``, a sequence of that length must be
            provided.
        dparam : `array-like` or sequence
            Detector parameter(s) at which to evaluate. If
            ``det_params.ndim >= 2``, a sequence of that length must be
            provided.

        Returns
        -------
        pos : `numpy.ndarray`
            Vector(s) pointing from the origin to the detector point.
            The shape of the returned array is obtained from the
            (broadcast) shapes of ``mparam`` and ``dparam``, and
            broadcasting is supported within both parameters and between
            them. The precise definition of the shape is
            ``broadcast(bcast_mparam, bcast_dparam).shape + (ndim,)``,
            where ``bcast_mparam`` is

            - ``mparam`` if `motion_params` is 1D,
            - ``broadcast(*mparam)`` otherwise,

            and ``bcast_dparam`` defined analogously.

        Examples
        --------
        The method works with single parameter values, in which case
        a single vector is returned:

        >>> apart = odl.uniform_partition(0, np.pi, 10)
        >>> dpart = odl.uniform_partition(-1, 1, 20)
        >>> geom = odl.tomo.Parallel2dGeometry(apart, dpart)
        >>> geom.det_point_position(0, 0)  # (0, 1) + 0 * (1, 0)
        array([ 0.,  1.])
        >>> geom.det_point_position(0, 1)  # (0, 1) + 1 * (1, 0)
        array([ 1.,  1.])
        >>> pt = geom.det_point_position(np.pi / 2, 0)  # (-1, 0) + 0 * (0, 1)
        >>> np.allclose(pt, [-1, 0])
        True
        >>> pt = geom.det_point_position(np.pi / 2, 1)  # (-1, 0) + 1 * (0, 1)
        >>> np.allclose(pt, [-1, 1])
        True

        Both variables support vectorized calls, i.e., stacks of
        parameters can be provided. The order of axes in the output (left
        of the ``ndim`` axis for the vector dimension) corresponds to the
        order of arguments:

        >>> geom.det_point_position(0, [-1, 0, 0.5, 1])
        array([[-1. ,  1. ],
               [ 0. ,  1. ],
               [ 0.5,  1. ],
               [ 1. ,  1. ]])
        >>> pts = geom.det_point_position([0, np.pi / 2, np.pi], 0)
        >>> np.allclose(pts, [[0, 1],
        ...                   [-1, 0],
        ...                   [0, -1]])
        True
        >>> # Providing 3 pairs of parameters, resulting in 3 vectors
        >>> pts = geom.det_point_position([0, np.pi / 2, np.pi],
        ...                               [-1, 0, 1])
        >>> pts[0]  # Corresponds to angle = 0, dparam = -1
        array([-1.,  1.])
        >>> pts.shape
        (3, 2)
        >>> # Pairs of parameters arranged in arrays of same size
        >>> geom.det_point_position(np.zeros((4, 5)), np.zeros((4, 5))).shape
        (4, 5, 2)
        >>> # ""Outer product"" type evaluation using broadcasting
        >>> geom.det_point_position(np.zeros((4, 1)), np.zeros((1, 5))).shape
        (4, 5, 2)

        More complicated 3D geometry with 2 angle variables and 2 detector
        variables:

        >>> apart = odl.uniform_partition([0, 0], [np.pi, 2 * np.pi],
        ...                               (10, 20))
        >>> dpart = odl.uniform_partition([-1, -1], [1, 1], (20, 20))
        >>> geom = odl.tomo.Parallel3dEulerGeometry(apart, dpart)
        >>> # 2 values for each variable, resulting in 2 vectors
        >>> angles = ([0, np.pi / 2], [0, np.pi])
        >>> dparams = ([-1, 0], [-1, 0])
        >>> pts = geom.det_point_position(angles, dparams)
        >>> pts[0]  # Corresponds to angle = (0, 0), dparam = (-1, -1)
        array([-1.,  1., -1.])
        >>> pts.shape
        (2, 3)
        >>> # 4 x 5 parameters for both
        >>> angles = dparams = (np.zeros((4, 5)), np.zeros((4, 5)))
        >>> geom.det_point_position(angles, dparams).shape
        (4, 5, 3)
        >>> # Broadcasting angles to shape (4, 5, 1, 1)
        >>> angles = (np.zeros((4, 1, 1, 1)), np.zeros((1, 5, 1, 1)))
        >>> # Broadcasting dparams to shape (1, 1, 6, 7)
        >>> dparams = (np.zeros((1, 1, 6, 1)), np.zeros((1, 1, 1, 7)))
        >>> # Total broadcast parameter shape is (4, 5, 6, 7)
        >>> geom.det_point_position(angles, dparams).shape
        (4, 5, 6, 7, 3)
        """"""
        # Always call the downstream methods with vectorized arguments
        # to be able to reliably manipulate the final axes of the result
        if self.motion_params.ndim == 1:
            squeeze_mparam = (np.shape(mparam) == ())
            mparam = np.array(mparam, dtype=float, copy=False, ndmin=1)
            matrix = self.rotation_matrix(mparam)  # shape (m, ndim, ndim)
        else:
            squeeze_mparam = (np.broadcast(*mparam).shape == ())
            mparam = tuple(np.array(a, dtype=float, copy=False, ndmin=1)
                           for a in mparam)
            matrix = self.rotation_matrix(mparam)  # shape (m, ndim, ndim)

        if self.det_params.ndim == 1:
            squeeze_dparam = (np.shape(dparam) == ())
            dparam = np.array(dparam, dtype=float, copy=False, ndmin=1)
        else:
            squeeze_dparam = (np.broadcast(*dparam).shape == ())
            dparam = tuple(np.array(p, dtype=float, copy=False, ndmin=1)
                           for p in dparam)

        surf = self.detector.surface(dparam)  # shape (d, ndim)

        # Perform matrix-vector multiplication along the last axis of both
        # `matrix` and `surf` while ""zipping"" all axes that do not
        # participate in the matrix-vector product. In other words, the axes
        # are labelled
        # [0, 1, ..., r-1, r, r+1] for `matrix` and
        # [0, 1, ..., r-1, r+1] for `surf`, and the output axes are set to
        # [0, 1, ..., r-1, r]. This automatically supports broadcasting
        # along the axes 0, ..., r-1.
        matrix_axes = list(range(matrix.ndim))
        surf_axes = list(range(matrix.ndim - 2)) + [matrix_axes[-1]]
        out_axes = list(range(matrix.ndim - 1))
        det_part = np.einsum(matrix, matrix_axes, surf, surf_axes, out_axes)

        refpt = self.det_refpoint(mparam)
        det_pt_pos = refpt + det_part
        if squeeze_mparam and squeeze_dparam:
            det_pt_pos = det_pt_pos.squeeze()

        return det_pt_pos","Return the detector point at ``(mparam, dparam)``.

        The position is computed as follows::

            pos = refpoint(mparam) +
                  rotation_matrix(mparam).dot(detector.surface(dparam))

        In other words, the motion parameter ``mparam`` is used to move the
        detector reference point, and the detector parameter ``dparam``
        defines an intrinsic shift that is added to the reference point.

        Parameters
        ----------
        mparam : `array-like` or sequence
            Motion parameter(s) at which to evaluate. If
            ``motion_params.ndim >= 2``, a sequence of that length must be
            provided.
        dparam : `array-like` or sequence
            Detector parameter(s) at which to evaluate. If
            ``det_params.ndim >= 2``, a sequence of that length must be
            provided.

        Returns
        -------
        pos : `numpy.ndarray`
            Vector(s) pointing from the origin to the detector point.
            The shape of the returned array is obtained from the
            (broadcast) shapes of ``mparam`` and ``dparam``, and
            broadcasting is supported within both parameters and between
            them. The precise definition of the shape is
            ``broadcast(bcast_mparam, bcast_dparam).shape + (ndim,)``,
            where ``bcast_mparam`` is

            - ``mparam`` if `motion_params` is 1D,
            - ``broadcast(*mparam)`` otherwise,

            and ``bcast_dparam`` defined analogously.

        Examples
        --------
        The method works with single parameter values, in which case
        a single vector is returned:

        >>> apart = odl.uniform_partition(0, np.pi, 10)
        >>> dpart = odl.uniform_partition(-1, 1, 20)
        >>> geom = odl.tomo.Parallel2dGeometry(apart, dpart)
        >>> geom.det_point_position(0, 0)  # (0, 1) + 0 * (1, 0)
        array([ 0.,  1.])
        >>> geom.det_point_position(0, 1)  # (0, 1) + 1 * (1, 0)
        array([ 1.,  1.])
        >>> pt = geom.det_point_position(np.pi / 2, 0)  # (-1, 0) + 0 * (0, 1)
        >>> np.allclose(pt, [-1, 0])
        True
        >>> pt = geom.det_point_position(np.pi / 2, 1)  # (-1, 0) + 1 * (0, 1)
        >>> np.allclose(pt, [-1, 1])
        True

        Both variables support vectorized calls, i.e., stacks of
        parameters can be provided. The order of axes in the output (left
        of the ``ndim`` axis for the vector dimension) corresponds to the
        order of arguments:

        >>> geom.det_point_position(0, [-1, 0, 0.5, 1])
        array([[-1. ,  1. ],
               [ 0. ,  1. ],
               [ 0.5,  1. ],
               [ 1. ,  1. ]])
        >>> pts = geom.det_point_position([0, np.pi / 2, np.pi], 0)
        >>> np.allclose(pts, [[0, 1],
        ...                   [-1, 0],
        ...                   [0, -1]])
        True
        >>> # Providing 3 pairs of parameters, resulting in 3 vectors
        >>> pts = geom.det_point_position([0, np.pi / 2, np.pi],
        ...                               [-1, 0, 1])
        >>> pts[0]  # Corresponds to angle = 0, dparam = -1
        array([-1.,  1.])
        >>> pts.shape
        (3, 2)
        >>> # Pairs of parameters arranged in arrays of same size
        >>> geom.det_point_position(np.zeros((4, 5)), np.zeros((4, 5))).shape
        (4, 5, 2)
        >>> # ""Outer product"" type evaluation using broadcasting
        >>> geom.det_point_position(np.zeros((4, 1)), np.zeros((1, 5))).shape
        (4, 5, 2)

        More complicated 3D geometry with 2 angle variables and 2 detector
        variables:

        >>> apart = odl.uniform_partition([0, 0], [np.pi, 2 * np.pi],
        ...                               (10, 20))
        >>> dpart = odl.uniform_partition([-1, -1], [1, 1], (20, 20))
        >>> geom = odl.tomo.Parallel3dEulerGeometry(apart, dpart)
        >>> # 2 values for each variable, resulting in 2 vectors
        >>> angles = ([0, np.pi / 2], [0, np.pi])
        >>> dparams = ([-1, 0], [-1, 0])
        >>> pts = geom.det_point_position(angles, dparams)
        >>> pts[0]  # Corresponds to angle = (0, 0), dparam = (-1, -1)
        array([-1.,  1., -1.])
        >>> pts.shape
        (2, 3)
        >>> # 4 x 5 parameters for both
        >>> angles = dparams = (np.zeros((4, 5)), np.zeros((4, 5)))
        >>> geom.det_point_position(angles, dparams).shape
        (4, 5, 3)
        >>> # Broadcasting angles to shape (4, 5, 1, 1)
        >>> angles = (np.zeros((4, 1, 1, 1)), np.zeros((1, 5, 1, 1)))
        >>> # Broadcasting dparams to shape (1, 1, 6, 7)
        >>> dparams = (np.zeros((1, 1, 6, 1)), np.zeros((1, 1, 1, 7)))
        >>> # Total broadcast parameter shape is (4, 5, 6, 7)
        >>> geom.det_point_position(angles, dparams).shape
        (4, 5, 6, 7, 3)"
"def target_lines(self):
    """"""The formatted target_type(...) lines for this target.

    This is just a convenience method for extracting and re-injecting the changed
    `dependency_lines` into the target text.
    """"""
    target_lines = self._target_source_lines[:]
    deps_begin, deps_end = self._dependencies_interval
    target_lines[deps_begin:deps_end] = self.dependency_lines()
    return target_lines","The formatted target_type(...) lines for this target.

    This is just a convenience method for extracting and re-injecting the changed
    `dependency_lines` into the target text."
"def recid_fetcher(record_uuid, data):
    """"""Fetch a record's identifiers.

    :param record_uuid: The record UUID.
    :param data: The record metadata.
    :returns: A :data:`invenio_pidstore.fetchers.FetchedPID` instance.
    """"""
    pid_field = current_app.config['PIDSTORE_RECID_FIELD']
    return FetchedPID(
        provider=RecordIdProvider,
        pid_type=RecordIdProvider.pid_type,
        pid_value=str(data[pid_field]),
    )","Fetch a record's identifiers.

    :param record_uuid: The record UUID.
    :param data: The record metadata.
    :returns: A :data:`invenio_pidstore.fetchers.FetchedPID` instance."
"def get_users(self, channel=None):
        """"""get list of users and channel access information (helper)

        :param channel: number [1:7]

        :return:
            name: (str)
            uid: (int)
            channel: (int)
            access:
                callback (bool)
                link_auth (bool)
                ipmi_msg (bool)
                privilege_level: (str)[callback, user, operatorm administrator,
                                       proprietary, no_access]
        """"""
        if channel is None:
            channel = self.get_network_channel()
        names = {}
        max_ids = self.get_channel_max_user_count(channel)
        for uid in range(1, max_ids + 1):
            name = self.get_user_name(uid=uid)
            if name is not None:
                names[uid] = self.get_user(uid=uid, channel=channel)
        return names","get list of users and channel access information (helper)

        :param channel: number [1:7]

        :return:
            name: (str)
            uid: (int)
            channel: (int)
            access:
                callback (bool)
                link_auth (bool)
                ipmi_msg (bool)
                privilege_level: (str)[callback, user, operatorm administrator,
                                       proprietary, no_access]"
"def add(self, elt):
        """"""Generic function to add objects into the scheduler daemon internal lists::
        Brok -> self.broks
        Check -> self.checks
        Notification -> self.actions
        EventHandler -> self.actions

        For an ExternalCommand, tries to resolve the command

        :param elt: element to add
        :type elt:
        :return: None
        """"""
        if elt is None:
            return
        logger.debug(""Adding: %s / %s"", elt.my_type, elt.__dict__)
        fun = self.__add_actions.get(elt.__class__, None)
        if fun:
            fun(self, elt)
        else:
            logger.warning(""self.add(): Unmanaged object class: %s (object=%r)"", elt.__class__, elt)","Generic function to add objects into the scheduler daemon internal lists::
        Brok -> self.broks
        Check -> self.checks
        Notification -> self.actions
        EventHandler -> self.actions

        For an ExternalCommand, tries to resolve the command

        :param elt: element to add
        :type elt:
        :return: None"
"def rm_docs(self):
        """"""Remove converted docs.""""""
        for filename in self.created:
            if os.path.exists(filename):
                os.unlink(filename)",Remove converted docs.
"def cut_nodes(graph):
    """"""
    Return the cut-nodes of the given graph.
    
    A cut node, or articulation point, is a node of a graph whose removal increases the number of
    connected components in the graph.
    
    @type  graph: graph, hypergraph
    @param graph: Graph.
        
    @rtype:  list
    @return: List of cut-nodes.
    """"""
    recursionlimit = getrecursionlimit()
    setrecursionlimit(max(len(graph.nodes())*2,recursionlimit))
    
    # Dispatch if we have a hypergraph
    if 'hypergraph' == graph.__class__.__name__:
        return _cut_hypernodes(graph)
        
    pre = {}    # Pre-ordering
    low = {}    # Lowest pre[] reachable from this node going down the spanning tree + one backedge
    reply = {}
    spanning_tree = {}
    pre[None] = 0
    
    # Create spanning trees, calculate pre[], low[]
    for each in graph:
        if (each not in pre):
            spanning_tree[each] = None
            _cut_dfs(graph, spanning_tree, pre, low, [], each)

    # Find cuts
    for each in graph:
        # If node is not a root
        if (spanning_tree[each] is not None):
            for other in graph[each]:
                # If there is no back-edge from descendent to a ancestral of each
                if (low[other] >= pre[each] and spanning_tree[other] == each):
                    reply[each] = 1
        # If node is a root
        else:
            children = 0
            for other in graph:
                if (spanning_tree[other] == each):
                    children = children + 1
            # root is cut-vertex iff it has two or more children
            if (children >= 2):
                reply[each] = 1

    setrecursionlimit(recursionlimit)
    return list(reply.keys())","Return the cut-nodes of the given graph.
    
    A cut node, or articulation point, is a node of a graph whose removal increases the number of
    connected components in the graph.
    
    @type  graph: graph, hypergraph
    @param graph: Graph.
        
    @rtype:  list
    @return: List of cut-nodes."
"def _parsePronunciation(pronunciationStr):
    '''
    Parses the pronunciation string
    
    Returns the list of syllables and a list of primary and
    secondary stress locations
    '''
    retList = []
    for syllableTxt in pronunciationStr.split(""#""):
        if syllableTxt == """":
            continue
        syllableList = [x.split() for x in syllableTxt.split(' . ')]
        
        # Find stress
        stressedSyllableList = []
        stressedPhoneList = []
        for i, syllable in enumerate(syllableList):
            for j, phone in enumerate(syllable):
                if u"""" in phone:
                    stressedSyllableList.insert(0, i)
                    stressedPhoneList.insert(0, j)
                    break
                elif u'' in phone:
                    stressedSyllableList.append(i)
                    stressedPhoneList.append(j)
        
        retList.append((syllableList, stressedSyllableList, stressedPhoneList))
    
    return retList","Parses the pronunciation string
    
    Returns the list of syllables and a list of primary and
    secondary stress locations"
"def register_applications(self, applications, models=None, backends=None):
        '''A higher level registration functions for group of models located
on application modules.
It uses the :func:`model_iterator` function to iterate
through all :class:`Model` models available in ``applications``
and register them using the :func:`register` low level method.

:parameter applications: A String or a list of strings representing
    python dotted paths where models are implemented.
:parameter models: Optional list of models to include. If not provided
    all models found in *applications* will be included.
:parameter backends: optional dictionary which map a model or an
    application to a backend :ref:`connection string <connection-string>`.
:rtype: A list of registered :class:`Model`.

For example::


    mapper.register_application_models('mylib.myapp')
    mapper.register_application_models(['mylib.myapp', 'another.path'])
    mapper.register_application_models(pythonmodule)
    mapper.register_application_models(['mylib.myapp',pythonmodule])

'''
        return list(self._register_applications(applications, models,
                                                backends))","A higher level registration functions for group of models located
on application modules.
It uses the :func:`model_iterator` function to iterate
through all :class:`Model` models available in ``applications``
and register them using the :func:`register` low level method.

:parameter applications: A String or a list of strings representing
    python dotted paths where models are implemented.
:parameter models: Optional list of models to include. If not provided
    all models found in *applications* will be included.
:parameter backends: optional dictionary which map a model or an
    application to a backend :ref:`connection string <connection-string>`.
:rtype: A list of registered :class:`Model`.

For example::


    mapper.register_application_models('mylib.myapp')
    mapper.register_application_models(['mylib.myapp', 'another.path'])
    mapper.register_application_models(pythonmodule)
    mapper.register_application_models(['mylib.myapp',pythonmodule])"
"def deriv(self, p):
        """"""
        Derivative of the power transform

        Parameters
        ----------
        p : array-like
            Mean parameters

        Returns
        --------
        g'(p) : array
            Derivative of power transform of `p`

        Notes
        -----
        g'(`p`) = `power` * `p`**(`power` - 1)
        """"""
        return self.power * np.power(p, self.power - 1)","Derivative of the power transform

        Parameters
        ----------
        p : array-like
            Mean parameters

        Returns
        --------
        g'(p) : array
            Derivative of power transform of `p`

        Notes
        -----
        g'(`p`) = `power` * `p`**(`power` - 1)"
"def load_model_using_search_path(
            self, filename, model, search_path, is_main_model=False,
            encoding='utf8', add_to_local_models=True):
        """"""
        add a new model to all relevant objects

        Args:
            filename: models to be loaded
            model: model holding the loaded models in its _tx_model_repository
                   field (may be None).
            search_path: list of search directories.

        Returns:
            the loaded model
        """"""
        if (model):
            self.update_model_in_repo_based_on_filename(model)
        for the_path in search_path:
            full_filename = join(the_path, filename)
            # print(full_filename)
            if exists(full_filename):
                the_metamodel = \
                    MetaModelProvider.get_metamodel(model, full_filename)
                return self.load_model(the_metamodel,
                                       full_filename,
                                       is_main_model,
                                       encoding=encoding,
                                       add_to_local_models=add_to_local_models)

        raise IOError(
            errno.ENOENT, os.strerror(errno.ENOENT), filename)","add a new model to all relevant objects

        Args:
            filename: models to be loaded
            model: model holding the loaded models in its _tx_model_repository
                   field (may be None).
            search_path: list of search directories.

        Returns:
            the loaded model"
"def MapByteStream(
      self, byte_stream, byte_offset=0, context=None, **unused_kwargs):
    """"""Maps the data type on a byte stream.

    Args:
      byte_stream (bytes): byte stream.
      byte_offset (Optional[int]): offset into the byte stream where to start.
      context (Optional[DataTypeMapContext]): data type map context.

    Returns:
      uuid.UUID: mapped value.

    Raises:
      MappingError: if the data type definition cannot be mapped on
          the byte stream.
    """"""
    data_type_size = self._data_type_definition.GetByteSize()
    self._CheckByteStreamSize(byte_stream, byte_offset, data_type_size)

    try:
      if self._byte_order == definitions.BYTE_ORDER_BIG_ENDIAN:
        mapped_value = uuid.UUID(
            bytes=byte_stream[byte_offset:byte_offset + 16])
      elif self._byte_order == definitions.BYTE_ORDER_LITTLE_ENDIAN:
        mapped_value = uuid.UUID(
            bytes_le=byte_stream[byte_offset:byte_offset + 16])

    except Exception as exception:
      error_string = (
          'Unable to read: {0:s} from byte stream at offset: {1:d} '
          'with error: {2!s}').format(
              self._data_type_definition.name, byte_offset, exception)
      raise errors.MappingError(error_string)

    if context:
      context.byte_size = data_type_size

    return mapped_value","Maps the data type on a byte stream.

    Args:
      byte_stream (bytes): byte stream.
      byte_offset (Optional[int]): offset into the byte stream where to start.
      context (Optional[DataTypeMapContext]): data type map context.

    Returns:
      uuid.UUID: mapped value.

    Raises:
      MappingError: if the data type definition cannot be mapped on
          the byte stream."
"def count_posts(self, tag=None, user_id=None, include_draft=False):
        """"""
        Returns the total number of posts for the give filter

        :param tag: Filter by a specific tag
        :type tag: str
        :param user_id: Filter by a specific user
        :type user_id: str
        :param include_draft: Whether to include posts marked as draft or not
        :type include_draft: bool
        :return: The number of posts for the given filter.
        """"""
        result = 0
        with self._engine.begin() as conn:
            try:
                count_statement = sqla.select([sqla.func.count()]). \
                    select_from(self._post_table)
                sql_filter = self._get_filter(tag, user_id, include_draft,
                                              conn)
                count_statement = count_statement.where(sql_filter)
                result = conn.execute(count_statement).scalar()
            except Exception as e:
                self._logger.exception(str(e))
                result = 0
        return result","Returns the total number of posts for the give filter

        :param tag: Filter by a specific tag
        :type tag: str
        :param user_id: Filter by a specific user
        :type user_id: str
        :param include_draft: Whether to include posts marked as draft or not
        :type include_draft: bool
        :return: The number of posts for the given filter."
"def add_class(self, cls, include_bases=True):
        """"""Add the specified class (which should be a class object, _not_ a
        string). By default all base classes for which is_backup_class returns
        True will also be added. `include_bases=False` may be spcified to
        suppress this behavior. The total number of classes added is returned.
        Note that if is_backup_class does not return True for the class object
        passed in, 0 will be returned. If you specify include_bases=False, then
        the maximum value that can be returned is 1.""""""
        if not is_backup_class(cls):
            return 0

        added = 0

        cls_name = backup_name(cls)
        if cls_name not in self.classes:
            self.classes[cls_name] = cls
            self.log(""Added class for backup: %s"", cls_name)
            added = 1

        if include_bases:
            for candidate_cls in getmro(cls):
                if is_backup_class(cls):
                    # Note that we don't keep recursing on base classes
                    added += self.add_class(candidate_cls, include_bases=False)

        return added","Add the specified class (which should be a class object, _not_ a
        string). By default all base classes for which is_backup_class returns
        True will also be added. `include_bases=False` may be spcified to
        suppress this behavior. The total number of classes added is returned.
        Note that if is_backup_class does not return True for the class object
        passed in, 0 will be returned. If you specify include_bases=False, then
        the maximum value that can be returned is 1."
"def request(self, method, path_append=None, headers=None, **kwargs):
        """"""Create a custom request

        :param method: HTTP method to use
        :param path_append: (optional) relative to :attr:`api_path`
        :param headers: (optional) Dictionary of headers to add or override
        :param kwargs: kwargs to pass along to :class:`requests.Request`
        :return:
            - :class:`Response` object
        """"""

        return self._request.custom(method, path_append=path_append, headers=headers, **kwargs)","Create a custom request

        :param method: HTTP method to use
        :param path_append: (optional) relative to :attr:`api_path`
        :param headers: (optional) Dictionary of headers to add or override
        :param kwargs: kwargs to pass along to :class:`requests.Request`
        :return:
            - :class:`Response` object"
"def _auto_insert_check(self):
        """"""Automatically insert tasks asynchronously.
        Depending on batch_size, insert or wait until next call.
        """"""

        if not self.batch_size:
            return

        if len(self._tasks) >= self.batch_size:
            self._handle_tasks()","Automatically insert tasks asynchronously.
        Depending on batch_size, insert or wait until next call."
"def is_private(self):
        """"""Test if this address is allocated for private networks.

        Returns:
            A boolean, True if the address is reserved per RFC 1918.

        """"""
        private_10 = IPv4Network('10.0.0.0/8')
        private_172 = IPv4Network('172.16.0.0/12')
        private_192 = IPv4Network('192.168.0.0/16')
        return (self in private_10 or
                self in private_172 or
                self in private_192)","Test if this address is allocated for private networks.

        Returns:
            A boolean, True if the address is reserved per RFC 1918."
"def client_create(self, label=None):
        """"""
        Creates a new LongviewClient, optionally with a given label.

        :param label: The label for the new client.  If None, a default label based
            on the new client's ID will be used.

        :returns: A new LongviewClient

        :raises ApiError: If a non-200 status code is returned
        :raises UnexpectedResponseError: If the returned data from the api does
            not look as expected.
        """"""
        result = self.client.post('/longview/clients', data={
            ""label"": label
        })

        if not 'id' in result:
            raise UnexpectedResponseError('Unexpected response when creating Longivew '
                'Client!', json=result)

        c = LongviewClient(self.client, result['id'], result)
        return c","Creates a new LongviewClient, optionally with a given label.

        :param label: The label for the new client.  If None, a default label based
            on the new client's ID will be used.

        :returns: A new LongviewClient

        :raises ApiError: If a non-200 status code is returned
        :raises UnexpectedResponseError: If the returned data from the api does
            not look as expected."
"def update(self, object, globalId=None, application=None, relationship=None):
        """"""Update a RemoteLink. 'object' is required.

        For definitions of the allowable fields for 'object' and the keyword arguments 'globalId', 'application' and
        'relationship', see https://developer.atlassian.com/display/JIRADEV/JIRA+REST+API+for+Remote+Issue+Links.

        :param object: the link details to add (see the above link for details)
        :param globalId: unique ID for the link (see the above link for details)
        :param application: application information for the link (see the above link for details)
        :param relationship: relationship description for the link (see the above link for details)
        """"""
        data = {
            'object': object}
        if globalId is not None:
            data['globalId'] = globalId
        if application is not None:
            data['application'] = application
        if relationship is not None:
            data['relationship'] = relationship

        super(RemoteLink, self).update(**data)","Update a RemoteLink. 'object' is required.

        For definitions of the allowable fields for 'object' and the keyword arguments 'globalId', 'application' and
        'relationship', see https://developer.atlassian.com/display/JIRADEV/JIRA+REST+API+for+Remote+Issue+Links.

        :param object: the link details to add (see the above link for details)
        :param globalId: unique ID for the link (see the above link for details)
        :param application: application information for the link (see the above link for details)
        :param relationship: relationship description for the link (see the above link for details)"
"def plot_day_clm(df_var, fig=None, ax=None, **kwargs):
    """"""Short summary.

    Parameters
    ----------
    df_var : pd.DataFrame
        DataFrame containing variables to plot with datetime as index

    Returns
    -------
    MPL.figure
        figure showing median lines and IQR in shadings

    """"""
    if fig is None and ax is None:
        fig, ax = plt.subplots()
    elif fig is None:
        fig = ax.get_figure()
    elif ax is None:
        ax = fig.gca()
    # plt.clf()
    # group by hour and minute
    grp_sdf_var = df_var.groupby(
        [df_var.index.hour.rename('hr'),
         df_var.index.minute.rename('min')])
    # get index
    idx = [pd.datetime(2014, 1, 1, h, m)
           for h, m in sorted(grp_sdf_var.groups.keys())]
    idx = pd.date_range(idx[0], idx[-1], periods=len(idx))
    idx = mdates.date2num(idx)

    # calculate quartiles
    quar_sel_pos_clm = grp_sdf_var.quantile(
        [.75, .5, .25]).unstack().set_index(idx)
    # fig, ax = plt.subplots(1)

    for var in quar_sel_pos_clm.columns.levels[0]:
        df_x = quar_sel_pos_clm.loc[:, var]
        y0 = df_x[0.5]
        y1, y2 = df_x[0.75], df_x[0.25]
        y0.plot(ax=ax, label=var).fill_between(
            quar_sel_pos_clm.index, y1, y2, alpha=0.3)
    # add legend
    ax.legend(title='variable')
    # adjust xticks formar
    ax.xaxis.set_major_locator(mdates.HourLocator(byhour=np.arange(0, 23, 3)))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))

    return fig, ax","Short summary.

    Parameters
    ----------
    df_var : pd.DataFrame
        DataFrame containing variables to plot with datetime as index

    Returns
    -------
    MPL.figure
        figure showing median lines and IQR in shadings"
"def set_solenoid(self, solenoid):
        """"""Set the solenoid config.

        :param solenoid: Value to set the solenoid
        :type solenoid: bool
        :returns: None
        :raises: InvalidInput
        """"""
        if type(solenoid) != bool:
            raise InvalidInput(""Solenoid value must be bool"")
        self._config['solenoid'] = bool2int(solenoid)
        self._q.put(self._config)","Set the solenoid config.

        :param solenoid: Value to set the solenoid
        :type solenoid: bool
        :returns: None
        :raises: InvalidInput"
"def get_input_grads(self, merge_multi_context=True):
        """"""Gets the gradients with respect to the inputs of the module.

        If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
        is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
        elements are `NDArray`.

        Parameters
        ----------
        merge_multi_context : bool
            Default is ``True``. In the case when data-parallelism is used, the outputs
            will be collected from multiple devices. A ``True`` value indicate that we
            should merge the collected results so that they look like from a single
            executor.

        Returns
        -------
        list of NDArray or list of list of NDArray
              Input gradients
        """"""
        assert self.binded and self.params_initialized and self.inputs_need_grad
        return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)","Gets the gradients with respect to the inputs of the module.

        If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
        is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
        elements are `NDArray`.

        Parameters
        ----------
        merge_multi_context : bool
            Default is ``True``. In the case when data-parallelism is used, the outputs
            will be collected from multiple devices. A ``True`` value indicate that we
            should merge the collected results so that they look like from a single
            executor.

        Returns
        -------
        list of NDArray or list of list of NDArray
              Input gradients"
"def get_note(self, noteid):

        """"""Fetch a single note

        :param folderid: The UUID of the note
        """"""

        if self.standard_grant_type is not ""authorization_code"":
            raise DeviantartError(""Authentication through Authorization Code (Grant Type) is required in order to connect to this endpoint."")

        response = self._req('/notes/{}'.format(noteid))

        return response","Fetch a single note

        :param folderid: The UUID of the note"
"def update_mapping_meta(self, doc_type, values, indices=None):
        """"""
        Update mapping meta
        :param doc_type: a doc type or a list of doctypes
        :param values: the dict of meta
        :param indices: a list of indices
        :return:
        """"""
        indices = self._validate_indices(indices)
        for index in indices:
            mapping = self.mappings.get_doctype(index, doc_type)
            if mapping is None:
                continue
            meta = mapping.get_meta()
            meta.update(values)
            mapping = {doc_type: {""_meta"": meta}}
            self.indices.put_mapping(doc_type=doc_type, mapping=mapping, indices=indices)","Update mapping meta
        :param doc_type: a doc type or a list of doctypes
        :param values: the dict of meta
        :param indices: a list of indices
        :return:"
"def rescale(x, min=0., max=1., axis=0):
    """"""
    Rescale values to fit a certain range [min, max]
    """"""
    def innerfn(x, min, max):
        return np.interp(x, [np.min(x), np.max(x)], [min, max])

    if isinstance(x, pd.DataFrame):
        return x.apply(innerfn, axis=axis, args=(min, max,))
    else:
        return pd.Series(innerfn(x, min, max), index=x.index)","Rescale values to fit a certain range [min, max]"
"def view(self, photo, options=None, **kwds):
        """"""
        Endpoint: /photo/<id>[/<options>]/view.json

        Requests all properties of a photo.
        Can be used to obtain URLs for the photo at a particular size,
          by using the ""returnSizes"" parameter.
        Returns the requested photo object.
        The options parameter can be used to pass in additional options.
        Eg: options={""token"": <token_data>}
        """"""
        option_string = self._build_option_string(options)
        result = self._client.get(""/photo/%s%s/view.json"" %
                                  (self._extract_id(photo), option_string),
                                  **kwds)[""result""]
        return Photo(self._client, result)","Endpoint: /photo/<id>[/<options>]/view.json

        Requests all properties of a photo.
        Can be used to obtain URLs for the photo at a particular size,
          by using the ""returnSizes"" parameter.
        Returns the requested photo object.
        The options parameter can be used to pass in additional options.
        Eg: options={""token"": <token_data>}"
"def load(self, env=None):
        """""" Load a section values of given environment.
        If nothing to specified, use environmental variable.
        If unknown environment was specified, warn it on logger.

        :param env: environment key to load in a coercive manner
        :type env: string
        :rtype: dict
        """"""
        self._load()
        e = env or \
            os.environ.get(RUNNING_MODE_ENVKEY, DEFAULT_RUNNING_MODE)
        if e in self.config:
            return self.config[e]
        logging.warn(""Environment '%s' was not found."", e)","Load a section values of given environment.
        If nothing to specified, use environmental variable.
        If unknown environment was specified, warn it on logger.

        :param env: environment key to load in a coercive manner
        :type env: string
        :rtype: dict"
"def export_obj(vertices, triangles, filename):
    """"""
    Exports a mesh in the (.obj) format.
    """"""
    
    with open(filename, 'w') as fh:
        
        for v in vertices:
            fh.write(""v {} {} {}\n"".format(*v))
            
        for f in triangles:
            fh.write(""f {} {} {}\n"".format(*(f + 1)))",Exports a mesh in the (.obj) format.
"def setup_context_menu(self):
        """"""Setup shell context menu""""""
        self.menu = QMenu(self)
        self.cut_action = create_action(self, _(""Cut""),
                                        shortcut=keybinding('Cut'),
                                        icon=ima.icon('editcut'),
                                        triggered=self.cut)
        self.copy_action = create_action(self, _(""Copy""),
                                         shortcut=keybinding('Copy'),
                                         icon=ima.icon('editcopy'),
                                         triggered=self.copy)
        paste_action = create_action(self, _(""Paste""),
                                     shortcut=keybinding('Paste'),
                                     icon=ima.icon('editpaste'),
                                     triggered=self.paste)
        save_action = create_action(self, _(""Save history log...""),
                                    icon=ima.icon('filesave'),
                                    tip=_(""Save current history log (i.e. all ""
                                          ""inputs and outputs) in a text file""),
                                    triggered=self.save_historylog)
        self.delete_action = create_action(self, _(""Delete""),
                                    shortcut=keybinding('Delete'),
                                    icon=ima.icon('editdelete'),
                                    triggered=self.delete)
        selectall_action = create_action(self, _(""Select All""),
                                    shortcut=keybinding('SelectAll'),
                                    icon=ima.icon('selectall'),
                                    triggered=self.selectAll)
        add_actions(self.menu, (self.cut_action, self.copy_action,
                                paste_action, self.delete_action, None,
                                selectall_action, None, save_action) )",Setup shell context menu
"def create_in_hdx(self):
        # type: () -> None
        """"""Check if resource exists in HDX and if so, update it, otherwise create it

        Returns:
            None
        """"""
        self.check_required_fields()
        id = self.data.get('id')
        if id and self._load_from_hdx('resource', id):
            logger.warning('%s exists. Updating %s' % ('resource', id))
            if self.file_to_upload and 'url' in self.data:
                del self.data['url']
            self._merge_hdx_update('resource', 'id', self.file_to_upload)
        else:
            self._save_to_hdx('create', 'name', self.file_to_upload)","Check if resource exists in HDX and if so, update it, otherwise create it

        Returns:
            None"
"def GetNSAndWsdlname(self, tag):
      """""" Map prefix:name tag into ns, name """"""
      idx = tag.find("":"")
      if idx >= 0:
         prefix, name = tag[:idx], tag[idx + 1:]
      else:
         prefix, name = None, tag
      # Map prefix to ns
      ns = self._GetNamespaceFromPrefix(prefix)
      return ns, name","Map prefix:name tag into ns, name"
"def next(self):

        """"""Next that shows progress in statusbar for each <freq> cells""""""

        self.progress_status()

        # Check abortes state and raise StopIteration if aborted
        if self.aborted:
            statustext = _(""File loading aborted."")
            post_command_event(self.main_window, self.main_window.StatusBarMsg,
                               text=statustext)
            raise StopIteration

        return self.parent_cls.next(self)",Next that shows progress in statusbar for each <freq> cells
"def get_lin_constraint(self, name):
        """""" Returns the constraint set with the given name.
        """"""
        for c in self.lin_constraints:
            if c.name == name:
                return c
        else:
            raise ValueError",Returns the constraint set with the given name.
"def load(
    inputobj,
    c=""gold"",
    alpha=None,
    wire=False,
    bc=None,
    texture=None,
    smoothing=None,
    threshold=None,
    connectivity=False,
):
    """"""
    Returns a ``vtkActor`` from reading a file, directory or ``vtkPolyData``.

    :param c: color in RGB format, hex, symbol or name
    :param alpha:   transparency (0=invisible)
    :param wire:    show surface as wireframe
    :param bc:      backface color of internal surface
    :param texture: any png/jpg file can be used as texture

    For volumetric data (tiff, slc, vti files):

    :param smoothing:    gaussian filter to smooth vtkImageData
    :param threshold:    value to draw the isosurface
    :param connectivity: if True only keeps the largest portion of the polydata
    """"""
    if alpha is None:
        alpha = 1

    if isinstance(inputobj, vtk.vtkPolyData):
        a = Actor(inputobj, c, alpha, wire, bc, texture)
        if inputobj and inputobj.GetNumberOfPoints() == 0:
            colors.printc(""~lightning Warning: actor has zero points."", c=5)
        return a

    acts = []
    if isinstance(inputobj, list):
        flist = inputobj
    else:
        import glob
        flist = sorted(glob.glob(inputobj))

    for fod in flist:
        if os.path.isfile(fod):
            if fod.endswith("".vtm""):
                acts += loadMultiBlockData(fod, unpack=True)
            else:
                a = _loadFile(fod, c, alpha, wire, bc, texture,
                              smoothing, threshold, connectivity)
                acts.append(a)
        elif os.path.isdir(fod):
            acts = _loadDir(fod, c, alpha, wire, bc, texture,
                            smoothing, threshold, connectivity)
    if not len(acts):
        colors.printc(""~times Error in load(): cannot find"", inputobj, c=1)
        return None

    if len(acts) == 1:
        return acts[0]
    else:
        return acts","Returns a ``vtkActor`` from reading a file, directory or ``vtkPolyData``.

    :param c: color in RGB format, hex, symbol or name
    :param alpha:   transparency (0=invisible)
    :param wire:    show surface as wireframe
    :param bc:      backface color of internal surface
    :param texture: any png/jpg file can be used as texture

    For volumetric data (tiff, slc, vti files):

    :param smoothing:    gaussian filter to smooth vtkImageData
    :param threshold:    value to draw the isosurface
    :param connectivity: if True only keeps the largest portion of the polydata"
"def chunk(keywords, lines):
    """"""
    Divide a file into chunks between
    key words in the list
    """"""
    chunks = dict()
    chunk = []
      
    # Create an empty dictionary using all the keywords
    for keyword in keywords:
        chunks[keyword] = []
    
    # Populate dictionary with lists of chunks associated
    # with the keywords in the list   
    for line in lines:
        if line.strip():
            token = line.split()[0]
            if token in keywords:
                chunk = [line]   
                chunks[token].append(chunk)   
            else:
                chunk.append(line)

    return chunks","Divide a file into chunks between
    key words in the list"
"def coordinates(value):
    """"""
    Convert a non-empty string into a list of lon-lat coordinates.

    >>> coordinates('')
    Traceback (most recent call last):
    ...
    ValueError: Empty list of coordinates: ''
    >>> coordinates('1.1 1.2')
    [(1.1, 1.2, 0.0)]
    >>> coordinates('1.1 1.2, 2.2 2.3')
    [(1.1, 1.2, 0.0), (2.2, 2.3, 0.0)]
    >>> coordinates('1.1 1.2 -0.4, 2.2 2.3 -0.5')
    [(1.1, 1.2, -0.4), (2.2, 2.3, -0.5)]
    >>> coordinates('0 0 0, 0 0 -1')
    Traceback (most recent call last):
    ...
    ValueError: Found overlapping site #2,  0 0 -1
    """"""
    if not value.strip():
        raise ValueError('Empty list of coordinates: %r' % value)
    points = []
    pointset = set()
    for i, line in enumerate(value.split(','), 1):
        pnt = point(line)
        if pnt[:2] in pointset:
            raise ValueError(""Found overlapping site #%d, %s"" % (i, line))
        pointset.add(pnt[:2])
        points.append(pnt)
    return points","Convert a non-empty string into a list of lon-lat coordinates.

    >>> coordinates('')
    Traceback (most recent call last):
    ...
    ValueError: Empty list of coordinates: ''
    >>> coordinates('1.1 1.2')
    [(1.1, 1.2, 0.0)]
    >>> coordinates('1.1 1.2, 2.2 2.3')
    [(1.1, 1.2, 0.0), (2.2, 2.3, 0.0)]
    >>> coordinates('1.1 1.2 -0.4, 2.2 2.3 -0.5')
    [(1.1, 1.2, -0.4), (2.2, 2.3, -0.5)]
    >>> coordinates('0 0 0, 0 0 -1')
    Traceback (most recent call last):
    ...
    ValueError: Found overlapping site #2,  0 0 -1"
"def _snake_to_camel_case(value):
    """"""Convert snake case string to camel case.""""""
    words = value.split(""_"")
    return words[0] + """".join(map(str.capitalize, words[1:]))",Convert snake case string to camel case.
"def view(self, rec):
        '''
        view the post.
        '''
        out_json = {
            'uid': rec.uid,
            'time_update': rec.time_update,
            'title': rec.title,
            'cnt_html': tornado.escape.xhtml_unescape(rec.cnt_html),

        }
        self.write(json.dumps(out_json))",view the post.
"def shorten_comment(line, max_line_length, last_comment=False):
    """"""Return trimmed or split long comment line.

    If there are no comments immediately following it, do a text wrap.
    Doing this wrapping on all comments in general would lead to jagged
    comment text.

    """"""
    assert len(line) > max_line_length
    line = line.rstrip()

    # PEP 8 recommends 72 characters for comment text.
    indentation = _get_indentation(line) + '# '
    max_line_length = min(max_line_length,
                          len(indentation) + 72)

    MIN_CHARACTER_REPEAT = 5
    if (
        len(line) - len(line.rstrip(line[-1])) >= MIN_CHARACTER_REPEAT and
        not line[-1].isalnum()
    ):
        # Trim comments that end with things like ---------
        return line[:max_line_length] + '\n'
    elif last_comment and re.match(r'\s*#+\s*\w+', line):
        split_lines = textwrap.wrap(line.lstrip(' \t#'),
                                    initial_indent=indentation,
                                    subsequent_indent=indentation,
                                    width=max_line_length,
                                    break_long_words=False,
                                    break_on_hyphens=False)
        return '\n'.join(split_lines) + '\n'

    return line + '\n'","Return trimmed or split long comment line.

    If there are no comments immediately following it, do a text wrap.
    Doing this wrapping on all comments in general would lead to jagged
    comment text."
"def create_order(self, pair, side, price, quantity, private_key, use_native_token=True, order_type=""limit"",
                     otc_address=None):
        """"""
        Function to create an order for the trade pair and details requested.
        Execution of this function is as follows::

            create_order(pair=""SWTH_NEO"", side=""buy"", price=0.0002, quantity=100, private_key=kp,
                         use_native_token=True, order_type=""limit"")

        The expected return result for this function is as follows::

            {
                'id': '4e6a59fd-d750-4332-aaf0-f2babfa8ad67',
                'blockchain': 'neo',
                'contract_hash': 'a195c1549e7da61b8da315765a790ac7e7633b82',
                'address': 'fea2b883725ef2d194c9060f606cd0a0468a2c59',
                'side': 'buy',
                'offer_asset_id': 'c56f33fc6ecfcd0c225c4ab356fee59390af8560be0e930faebe74a6daff7c9b',
                'want_asset_id': 'ab38352559b8b203bde5fddfa0b07d8b2525e132',
                'offer_amount': '2000000',
                'want_amount': '10000000000',
                'transfer_amount': '0',
                'priority_gas_amount': '0',
                'use_native_token': True,
                'native_fee_transfer_amount': 0,
                'deposit_txn': None,
                'created_at': '2018-08-05T10:38:37.714Z',
                'status': 'pending',
                'fills': [],
                'makes': [
                    {
                        'id': 'e30a7fdf-779c-4623-8f92-8a961450d843',
                        'offer_hash': None,
                        'available_amount': None,
                        'offer_asset_id': 'c56f33fc6ecfcd0c225c4ab356fee59390af8560be0e930faebe74a6daff7c9b',
                        'offer_amount': '2000000',
                        'want_asset_id': 'ab38352559b8b203bde5fddfa0b07d8b2525e132',
                        'want_amount': '10000000000',
                        'filled_amount': None,
                        'txn': {
                            'offerHash': 'b45ddfb97ade5e0363d9e707dac9ad1c530448db263e86494225a0025006f968',
                            'hash': '5c4cb1e73b9f2e608b6e768e0654649a4d15e08a7fe63fc536c454fa563a2f0f',
                            'sha256': 'f0b70640627947584a2976edeb055a124ae85594db76453532b893c05618e6ca',
                            'invoke': {
                                'scriptHash': 'a195c1549e7da61b8da315765a790ac7e7633b82',
                                'operation': 'makeOffer',
                                'args': [
                                    '592c8a46a0d06c600f06c994d1f25e7283b8a2fe',
                                    '9b7cffdaa674beae0f930ebe6085af9093e5fe56b34a5c220ccdcf6efc336fc5',
                                    2000000,
                                    '32e125258b7db0a0dffde5bd03b2b859253538ab',
                                    10000000000,
                                    '65333061376664662d373739632d343632332d386639322d386139363134353064383433'
                                ]
                            },
                            'type': 209,
                            'version': 1,
                            'attributes': [
                                {
                                    'usage': 32,
                                    'data': '592c8a46a0d06c600f06c994d1f25e7283b8a2fe'
                                }
                            ],
                            'inputs': [
                                {
                                    'prevHash': '0fcfd792a9d20a7795255d1d3d3927f5968b9953e80d16ffd222656edf8fedbc',
                                    'prevIndex': 0
                                }, {
                                    'prevHash': 'c858e4d2af1e1525fa974fb2b1678caca1f81a5056513f922789594939ff713d',
                                    'prevIndex': 35
                                }
                            ],
                            'outputs': [
                                {
                                    'assetId': '602c79718b16e442de58778e148d0b1084e3b2dffd5de6b7b16cee7969282de7',
                                    'scriptHash': 'e707714512577b42f9a011f8b870625429f93573',
                                    'value': 1e-08
                                }
                            ],
                            'scripts': [],
                            'script': '....',
                            'gas': 0
                        },
                        'cancel_txn': None,
                        'price': '0.0002',
                        'status': 'pending',
                        'created_at': '2018-08-05T10:38:37.731Z',
                        'transaction_hash': '5c4cb1e73b9f2e608b6e768e0654649a4d15e08a7fe63fc536c454fa563a2f0f',
                        'trades': []
                    }
                ]
            }

        :param pair: The trading pair this order is being submitted for.
        :type pair: str
        :param side: The side of the trade being submitted i.e. buy or sell
        :type side: str
        :param price: The price target for this trade.
        :type price: float
        :param quantity: The amount of the asset being exchanged in the trade.
        :type quantity: float
        :param private_key: The Private Key (ETH) or KeyPair (NEO) for the wallet being used to sign deposit message.
        :type private_key: KeyPair or str
        :param use_native_token: Flag to indicate whether or not to pay fees with the Switcheo native token.
        :type use_native_token: bool
        :param order_type: The type of order being submitted, currently this can only be a limit order.
        :type order_type: str
        :param otc_address: The address to trade with for Over the Counter exchanges.
        :type otc_address: str
        :return: Dictionary of order details to specify which parts of the trade will be filled (taker) or open (maker)
        """"""
        if side.lower() not in [""buy"", ""sell""]:
            raise ValueError(""Allowed trade types are buy or sell, you entered {}"".format(side.lower()))
        if order_type.lower() not in [""limit"", ""market"", ""otc""]:
            raise ValueError(""Allowed order type is limit, you entered {}"".format(order_type.lower()))
        if order_type.lower() == ""otc"" and otc_address is None:
            raise ValueError(""OTC Address is required when trade type is otc (over the counter)."")
        order_params = {
            ""blockchain"": self.blockchain,
            ""pair"": pair,
            ""side"": side,
            ""price"": '{:.8f}'.format(price) if order_type.lower() != ""market"" else None,
            ""quantity"": str(self.blockchain_amount[self.blockchain](quantity)),
            ""use_native_tokens"": use_native_token,
            ""order_type"": order_type,
            ""timestamp"": get_epoch_milliseconds(),
            ""contract_hash"": self.contract_hash
        }
        api_params = self.sign_create_order_function[self.blockchain](order_params, private_key)
        return self.request.post(path='/orders', json_data=api_params)","Function to create an order for the trade pair and details requested.
        Execution of this function is as follows::

            create_order(pair=""SWTH_NEO"", side=""buy"", price=0.0002, quantity=100, private_key=kp,
                         use_native_token=True, order_type=""limit"")

        The expected return result for this function is as follows::

            {
                'id': '4e6a59fd-d750-4332-aaf0-f2babfa8ad67',
                'blockchain': 'neo',
                'contract_hash': 'a195c1549e7da61b8da315765a790ac7e7633b82',
                'address': 'fea2b883725ef2d194c9060f606cd0a0468a2c59',
                'side': 'buy',
                'offer_asset_id': 'c56f33fc6ecfcd0c225c4ab356fee59390af8560be0e930faebe74a6daff7c9b',
                'want_asset_id': 'ab38352559b8b203bde5fddfa0b07d8b2525e132',
                'offer_amount': '2000000',
                'want_amount': '10000000000',
                'transfer_amount': '0',
                'priority_gas_amount': '0',
                'use_native_token': True,
                'native_fee_transfer_amount': 0,
                'deposit_txn': None,
                'created_at': '2018-08-05T10:38:37.714Z',
                'status': 'pending',
                'fills': [],
                'makes': [
                    {
                        'id': 'e30a7fdf-779c-4623-8f92-8a961450d843',
                        'offer_hash': None,
                        'available_amount': None,
                        'offer_asset_id': 'c56f33fc6ecfcd0c225c4ab356fee59390af8560be0e930faebe74a6daff7c9b',
                        'offer_amount': '2000000',
                        'want_asset_id': 'ab38352559b8b203bde5fddfa0b07d8b2525e132',
                        'want_amount': '10000000000',
                        'filled_amount': None,
                        'txn': {
                            'offerHash': 'b45ddfb97ade5e0363d9e707dac9ad1c530448db263e86494225a0025006f968',
                            'hash': '5c4cb1e73b9f2e608b6e768e0654649a4d15e08a7fe63fc536c454fa563a2f0f',
                            'sha256': 'f0b70640627947584a2976edeb055a124ae85594db76453532b893c05618e6ca',
                            'invoke': {
                                'scriptHash': 'a195c1549e7da61b8da315765a790ac7e7633b82',
                                'operation': 'makeOffer',
                                'args': [
                                    '592c8a46a0d06c600f06c994d1f25e7283b8a2fe',
                                    '9b7cffdaa674beae0f930ebe6085af9093e5fe56b34a5c220ccdcf6efc336fc5',
                                    2000000,
                                    '32e125258b7db0a0dffde5bd03b2b859253538ab',
                                    10000000000,
                                    '65333061376664662d373739632d343632332d386639322d386139363134353064383433'
                                ]
                            },
                            'type': 209,
                            'version': 1,
                            'attributes': [
                                {
                                    'usage': 32,
                                    'data': '592c8a46a0d06c600f06c994d1f25e7283b8a2fe'
                                }
                            ],
                            'inputs': [
                                {
                                    'prevHash': '0fcfd792a9d20a7795255d1d3d3927f5968b9953e80d16ffd222656edf8fedbc',
                                    'prevIndex': 0
                                }, {
                                    'prevHash': 'c858e4d2af1e1525fa974fb2b1678caca1f81a5056513f922789594939ff713d',
                                    'prevIndex': 35
                                }
                            ],
                            'outputs': [
                                {
                                    'assetId': '602c79718b16e442de58778e148d0b1084e3b2dffd5de6b7b16cee7969282de7',
                                    'scriptHash': 'e707714512577b42f9a011f8b870625429f93573',
                                    'value': 1e-08
                                }
                            ],
                            'scripts': [],
                            'script': '....',
                            'gas': 0
                        },
                        'cancel_txn': None,
                        'price': '0.0002',
                        'status': 'pending',
                        'created_at': '2018-08-05T10:38:37.731Z',
                        'transaction_hash': '5c4cb1e73b9f2e608b6e768e0654649a4d15e08a7fe63fc536c454fa563a2f0f',
                        'trades': []
                    }
                ]
            }

        :param pair: The trading pair this order is being submitted for.
        :type pair: str
        :param side: The side of the trade being submitted i.e. buy or sell
        :type side: str
        :param price: The price target for this trade.
        :type price: float
        :param quantity: The amount of the asset being exchanged in the trade.
        :type quantity: float
        :param private_key: The Private Key (ETH) or KeyPair (NEO) for the wallet being used to sign deposit message.
        :type private_key: KeyPair or str
        :param use_native_token: Flag to indicate whether or not to pay fees with the Switcheo native token.
        :type use_native_token: bool
        :param order_type: The type of order being submitted, currently this can only be a limit order.
        :type order_type: str
        :param otc_address: The address to trade with for Over the Counter exchanges.
        :type otc_address: str
        :return: Dictionary of order details to specify which parts of the trade will be filled (taker) or open (maker)"
"def comparator(objective):
    """"""
    Higher order function creating a compare function for objectives.

    Args:
        objective (cipy.algorithms.core.Objective): The objective to create a
            compare for.

    Returns:
        callable: Function accepting two objectives to compare.

    Examples:
        >>> a = Minimum(0.1)
        >>> b = Minimum(0.2)
        >>> compare = comparator(a)
        >>> comparison = compare(a, b) # False
    """"""

    if isinstance(objective, Minimum):
        return lambda l, r: l < r
    else:
        return lambda l, r: l > r","Higher order function creating a compare function for objectives.

    Args:
        objective (cipy.algorithms.core.Objective): The objective to create a
            compare for.

    Returns:
        callable: Function accepting two objectives to compare.

    Examples:
        >>> a = Minimum(0.1)
        >>> b = Minimum(0.2)
        >>> compare = comparator(a)
        >>> comparison = compare(a, b) # False"
"def get_project_activity_metrics(self, project, from_date, aggregation_type):
        """"""GetProjectActivityMetrics.
        [Preview API]
        :param str project: Project ID or project name
        :param datetime from_date:
        :param str aggregation_type:
        :rtype: :class:`<ProjectActivityMetrics> <azure.devops.v5_0.project_analysis.models.ProjectActivityMetrics>`
        """"""
        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        query_parameters = {}
        if from_date is not None:
            query_parameters['fromDate'] = self._serialize.query('from_date', from_date, 'iso-8601')
        if aggregation_type is not None:
            query_parameters['aggregationType'] = self._serialize.query('aggregation_type', aggregation_type, 'str')
        response = self._send(http_method='GET',
                              location_id='e40ae584-9ea6-4f06-a7c7-6284651b466b',
                              version='5.0-preview.1',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('ProjectActivityMetrics', response)","GetProjectActivityMetrics.
        [Preview API]
        :param str project: Project ID or project name
        :param datetime from_date:
        :param str aggregation_type:
        :rtype: :class:`<ProjectActivityMetrics> <azure.devops.v5_0.project_analysis.models.ProjectActivityMetrics>`"
"def specific_analysis_question(hazard, exposure):
    """"""Return a translated hardcoded analysis question a given hazard/exposure.

    :param hazard: The hazard definition.
    :type hazard: safe.definition.hazard

    :param exposure: The exposure definition.
    :type hazard: safe.definition.exposure

    :return: The analysis question or None if it's not hardcoded.
    :rtype: basestring
    """"""
    lang = locale()
    for item in ITEMS:
        if item['hazard'] == hazard and item['exposure'] == exposure:
            analysis_questions = item.get('analysis_question', None)
            if not analysis_questions:
                return None
            return analysis_questions.get(lang, None)
    return None","Return a translated hardcoded analysis question a given hazard/exposure.

    :param hazard: The hazard definition.
    :type hazard: safe.definition.hazard

    :param exposure: The exposure definition.
    :type hazard: safe.definition.exposure

    :return: The analysis question or None if it's not hardcoded.
    :rtype: basestring"
"def get_merge_requests(self):
        ""http://doc.gitlab.com/ce/api/merge_requests.html""
        g = self.gitlab
        merges = self.get(g['url'] + ""/projects/"" +
                          g['repo'] + ""/merge_requests"",
                          {'private_token': g['token'],
                           'state': 'all'}, cache=False)
        return dict([(str(merge['id']), merge) for merge in merges])",http://doc.gitlab.com/ce/api/merge_requests.html
"def _wait_for_job(linode_id, job_id, timeout=300, quiet=True):
    '''
    Wait for a Job to return.

    linode_id
        The ID of the Linode to wait on. Required.

    job_id
        The ID of the job to wait for.

    timeout
        The amount of time to wait for a status to update.

    quiet
        Log status updates to debug logs when True. Otherwise, logs to info.
    '''
    interval = 5
    iterations = int(timeout / interval)

    for i in range(0, iterations):
        jobs_result = _query('linode',
                             'job.list',
                             args={'LinodeID': linode_id})['DATA']
        if jobs_result[0]['JOBID'] == job_id and jobs_result[0]['HOST_SUCCESS'] == 1:
            return True

        time.sleep(interval)
        log.log(
            logging.INFO if not quiet else logging.DEBUG,
            'Still waiting on Job %s for Linode %s.', job_id, linode_id
        )
    return False","Wait for a Job to return.

    linode_id
        The ID of the Linode to wait on. Required.

    job_id
        The ID of the job to wait for.

    timeout
        The amount of time to wait for a status to update.

    quiet
        Log status updates to debug logs when True. Otherwise, logs to info."
"def hilbert_phase(ts):
    """"""Phase of the analytic signal, using the Hilbert transform""""""
    output = np.angle(signal.hilbert(signal.detrend(ts, axis=0), axis=0))
    return Timeseries(output, ts.tspan, labels=ts.labels)","Phase of the analytic signal, using the Hilbert transform"
"def currentAbove(requestContext, seriesList, n):
    """"""
    Takes one metric or a wildcard seriesList followed by an integer N.
    Out of all metrics passed, draws only the metrics whose value is above N
    at the end of the time period specified.

    Example::

        &target=currentAbove(server*.instance*.threads.busy,50)

    Draws the servers with more than 50 busy threads.

    """"""
    results = []
    for series in seriesList:
        val = safeLast(series)
        if val is not None and val >= n:
            results.append(series)
    return results","Takes one metric or a wildcard seriesList followed by an integer N.
    Out of all metrics passed, draws only the metrics whose value is above N
    at the end of the time period specified.

    Example::

        &target=currentAbove(server*.instance*.threads.busy,50)

    Draws the servers with more than 50 busy threads."
"def __command(self, ttype, tvalue):
        """"""Command parsing method

        Entry point for command parsing. Here is expected behaviour:
         * Handle command beginning if detected,
         * Call the appropriate sub-method (specified by __cstate) to
           handle the body,
         * Handle command ending or block opening if detected.

        Syntax:
            identifier arguments ("";"" / block)

        :param ttype: current token type
        :param tvalue: current token value
        :return: False if an error is encountered, True otherwise
        """"""
        if self.__cstate is None:
            if ttype == ""right_cbracket"":
                self.__up()
                self.__opened_blocks -= 1
                self.__cstate = None
                return True

            if ttype != ""identifier"":
                return False
            command = get_command_instance(
                tvalue.decode(""ascii""), self.__curcommand)
            if command.get_type() == ""test"":
                raise ParseError(
                    ""%s may not appear as a first command"" % command.name)
            if command.get_type() == ""control"" and command.accept_children \
               and command.has_arguments():
                self.__set_expected(""identifier"")
            if self.__curcommand is not None:
                if not self.__curcommand.addchild(command):
                    raise ParseError(""%s unexpected after a %s"" %
                                     (tvalue, self.__curcommand.name))
            self.__curcommand = command
            self.__cstate = self.__arguments

            return True

        if self.__cstate(ttype, tvalue):
            return True

        if ttype == ""left_cbracket"":
            self.__opened_blocks += 1
            self.__cstate = None
            return True

        if ttype == ""semicolon"":
            self.__cstate = None
            if not self.__check_command_completion(testsemicolon=False):
                return False
            self.__curcommand.complete_cb()
            self.__up()
            return True
        return False","Command parsing method

        Entry point for command parsing. Here is expected behaviour:
         * Handle command beginning if detected,
         * Call the appropriate sub-method (specified by __cstate) to
           handle the body,
         * Handle command ending or block opening if detected.

        Syntax:
            identifier arguments ("";"" / block)

        :param ttype: current token type
        :param tvalue: current token value
        :return: False if an error is encountered, True otherwise"
"def get_log_entries_by_search(self, log_entry_query, log_entry_search):
        """"""Pass through to provider LogEntrySearchSession.get_log_entries_by_search""""""
        # Implemented from azosid template for -
        # osid.resource.ResourceSearchSession.get_resources_by_search_template
        if not self._can('search'):
            raise PermissionDenied()
        return self._provider_session.get_log_entries_by_search(log_entry_query, log_entry_search)",Pass through to provider LogEntrySearchSession.get_log_entries_by_search
"def _ParseCachedEntry2003(self, value_data, cached_entry_offset):
    """"""Parses a Windows 2003 cached entry.

    Args:
      value_data (bytes): value data.
      cached_entry_offset (int): offset of the first cached entry data
          relative to the start of the value data.

    Returns:
      AppCompatCacheCachedEntry: cached entry.

    Raises:
      ParseError: if the value data could not be parsed.
    """"""

    try:
      cached_entry = self._ReadStructureFromByteStream(
          value_data[cached_entry_offset:], cached_entry_offset,
          self._cached_entry_data_type_map)
    except (ValueError, errors.ParseError) as exception:
      raise errors.ParseError(
          'Unable to parse cached entry value with error: {0!s}'.format(
              exception))

    path_size = cached_entry.path_size
    maximum_path_size = cached_entry.maximum_path_size
    path_offset = cached_entry.path_offset

    if path_offset > 0 and path_size > 0:
      path_size += path_offset
      maximum_path_size += path_offset

      try:
        path = value_data[path_offset:path_size].decode('utf-16-le')
      except UnicodeDecodeError:
        raise errors.ParseError('Unable to decode cached entry path to string')

    cached_entry_object = AppCompatCacheCachedEntry()
    cached_entry_object.cached_entry_size = (
        self._cached_entry_data_type_map.GetByteSize())
    cached_entry_object.file_size = getattr(cached_entry, 'file_size', None)
    cached_entry_object.last_modification_time = (
        cached_entry.last_modification_time)
    cached_entry_object.path = path

    return cached_entry_object","Parses a Windows 2003 cached entry.

    Args:
      value_data (bytes): value data.
      cached_entry_offset (int): offset of the first cached entry data
          relative to the start of the value data.

    Returns:
      AppCompatCacheCachedEntry: cached entry.

    Raises:
      ParseError: if the value data could not be parsed."
"def search(self, filters):
        """"""Search Five9 given a filter.

        Args:
            filters (dict): A dictionary of search strings, keyed by the name
                of the field to search.

        Returns:
            Environment: An environment representing the recordset.
        """"""
        records = self.__model__.search(self.__five9__, filters)
        return self.__class__(
            self.__five9__, self.__model__, records,
        )","Search Five9 given a filter.

        Args:
            filters (dict): A dictionary of search strings, keyed by the name
                of the field to search.

        Returns:
            Environment: An environment representing the recordset."
"def temp_dir(folder=None, delete=True):
    # type: (Optional[str], bool) -> str
    """"""Get a temporary directory optionally with folder appended (and created if it doesn't exist)

    Args:
        folder (Optional[str]): Folder to create in temporary folder. Defaults to None.
        delete (bool): Whether to delete folder on exiting with statement

    Returns:
        str: A temporary directory
    """"""
    tempdir = get_temp_dir()
    if folder:
        tempdir = join(tempdir, folder)
    if not exists(tempdir):
        makedirs(tempdir)
    try:
        yield tempdir
    finally:
        if delete:
            rmtree(tempdir)","Get a temporary directory optionally with folder appended (and created if it doesn't exist)

    Args:
        folder (Optional[str]): Folder to create in temporary folder. Defaults to None.
        delete (bool): Whether to delete folder on exiting with statement

    Returns:
        str: A temporary directory"
"def __writepid(self, pid):
        """"""
        HoverFly fails to launch if it's already running on
        the same ports. So we have to keep track of them using
        temp files with the proxy port and admin port, containing
        the processe's PID. 
        """"""
        import tempfile
        d = tempfile.gettempdir()
        name = os.path.join(d, ""hoverpy.%i.%i""%(self._proxyPort, self._adminPort))
        with open(name, 'w') as f:
            f.write(str(pid))
            logging.debug(""writing to %s""%name)","HoverFly fails to launch if it's already running on
        the same ports. So we have to keep track of them using
        temp files with the proxy port and admin port, containing
        the processe's PID."
"def show_keypair(kwargs=None, call=None):
    '''
    Show the details of an SSH keypair
    '''
    if call != 'function':
        log.error(
            'The show_keypair function must be called with -f or --function.'
        )
        return False

    if not kwargs:
        kwargs = {}

    if 'keyname' not in kwargs:
        log.error('A keyname is required.')
        return False

    keypairs = list_keypairs(call='function')
    keyid = keypairs[kwargs['keyname']]['id']
    log.debug('Key ID is %s', keyid)

    details = query(method='account/keys', command=keyid)

    return details",Show the details of an SSH keypair
"def docs(root_url, path):
    """"""Generate URL for path in the Taskcluster docs.""""""
    root_url = root_url.rstrip('/')
    path = path.lstrip('/')
    if root_url == OLD_ROOT_URL:
        return 'https://docs.taskcluster.net/{}'.format(path)
    else:
        return '{}/docs/{}'.format(root_url, path)",Generate URL for path in the Taskcluster docs.
"def axis_titles(self, x=None, y=None):
        """"""Apply axis titles to the figure.

        This is a convenience method for manually modifying the ""Axes"" mark.

        Parameters
        ----------
        x: string, default 'null'
            X-axis title
        y: string, default 'null'
            Y-axis title

        Example
        -------
        >>>vis.axis_titles(y=""Data 1"", x=""Data 2"")

        """"""
        keys = self.axes.get_keys()

        if keys:
            for key in keys:
                if key == 'x':
                    self.axes[key].title = x
                elif key == 'y':
                    self.axes[key].title = y
        else:
            self.axes.extend([Axis(type='x', title=x),
                              Axis(type='y', title=y)])
        return self","Apply axis titles to the figure.

        This is a convenience method for manually modifying the ""Axes"" mark.

        Parameters
        ----------
        x: string, default 'null'
            X-axis title
        y: string, default 'null'
            Y-axis title

        Example
        -------
        >>>vis.axis_titles(y=""Data 1"", x=""Data 2"")"
"def rename(self, from_, to):
        """"""
        Rename a table on the schema.
        """"""
        blueprint = self._create_blueprint(from_)

        blueprint.rename(to)

        self._build(blueprint)",Rename a table on the schema.
"def fill_price_worse_than_limit_price(fill_price, order):
    """"""
    Checks whether the fill price is worse than the order's limit price.

    Parameters
    ----------
    fill_price: float
        The price to check.

    order: zipline.finance.order.Order
        The order whose limit price to check.

    Returns
    -------
    bool: Whether the fill price is above the limit price (for a buy) or below
    the limit price (for a sell).
    """"""
    if order.limit:
        # this is tricky! if an order with a limit price has reached
        # the limit price, we will try to fill the order. do not fill
        # these shares if the impacted price is worse than the limit
        # price. return early to avoid creating the transaction.

        # buy order is worse if the impacted price is greater than
        # the limit price. sell order is worse if the impacted price
        # is less than the limit price
        if (order.direction > 0 and fill_price > order.limit) or \
                (order.direction < 0 and fill_price < order.limit):
            return True

    return False","Checks whether the fill price is worse than the order's limit price.

    Parameters
    ----------
    fill_price: float
        The price to check.

    order: zipline.finance.order.Order
        The order whose limit price to check.

    Returns
    -------
    bool: Whether the fill price is above the limit price (for a buy) or below
    the limit price (for a sell)."
"def is_muted(what):
    """"""
    Checks if a logged event is to be muted for debugging purposes.

    Also goes through the solo list - only items in there will be logged!

    :param what:
    :return:
    """"""

    state = False

    for item in solo:
        if item not in what:
            state = True
        else:
            state = False
            break

    for item in mute:
        if item in what:
            state = True
            break

    return state","Checks if a logged event is to be muted for debugging purposes.

    Also goes through the solo list - only items in there will be logged!

    :param what:
    :return:"
"def logpdf(x, shape, loc=0.0, scale=1.0, skewness=1.0):
    """"""
    Log PDF for the Skew-t distribution

    Parameters
    ----------
    x : np.array
        random variables

    shape : float
        The degrees of freedom for the skew-t distribution

    loc : np.array
        The location parameter for the skew-t distribution

    scale : float
        The scale of the distribution

    skewness : float
        Skewness parameter (if 1, no skewness, if > 1, +ve skew, if < 1, -ve skew)

    """"""
    m1 = (np.sqrt(shape)*sp.gamma((shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(shape/2.0))
    loc = loc + (skewness - (1.0/skewness))*scale*m1
    result = np.zeros(x.shape[0])
    result[x-loc<0] = np.log(2.0) - np.log(skewness + 1.0/skewness) + ss.t.logpdf(x=skewness*x[(x-loc) < 0], loc=loc[(x-loc) < 0]*skewness,df=shape, scale=scale[(x-loc) < 0])
    result[x-loc>=0] = np.log(2.0) - np.log(skewness + 1.0/skewness) + ss.t.logpdf(x=x[(x-loc) >= 0]/skewness, loc=loc[(x-loc) >= 0]/skewness,df=shape, scale=scale[(x-loc) >= 0])
    return result","Log PDF for the Skew-t distribution

    Parameters
    ----------
    x : np.array
        random variables

    shape : float
        The degrees of freedom for the skew-t distribution

    loc : np.array
        The location parameter for the skew-t distribution

    scale : float
        The scale of the distribution

    skewness : float
        Skewness parameter (if 1, no skewness, if > 1, +ve skew, if < 1, -ve skew)"
"def wait_for_notebook_to_execute(self):
        """"""Wait for notebook to finish executing before continuing.""""""
        while True:
            r = requests.get('http://{0}/api/notebook/job/{1}'.format(
                             self.zeppelin_url, self.notebook_id))

            if r.status_code == 200:
                try:
                    data = r.json()['body']
                    if all(paragraph['status'] in ['FINISHED', 'ERROR'] for paragraph in data):
                        break
                    time.sleep(5)
                    continue
                except KeyError as e:
                    print(e)
                    print(r.json())

            elif r.status_code == 500:
                print('Notebook is still busy executing. Checking again in 60 seconds...')
                time.sleep(60)
                continue

            else:
                print('ERROR: Unexpected return code: {}'.format(r.status_code))
                sys.exit(1)",Wait for notebook to finish executing before continuing.
"def _prepare_for_submission(self, tempfolder, inputdict):
        """"""
        Create input files.

            :param tempfolder: aiida.common.folders.Folder subclass where
                the plugin should put all its files.
            :param inputdict: dictionary of the input nodes as they would
                be returned by get_inputs_dict
        """"""
        parameters, code, structure, surface_sample = \
                self._validate_inputs(inputdict)

        # Prepare CalcInfo to be returned to aiida
        calcinfo = CalcInfo()
        calcinfo.uuid = self.uuid
        calcinfo.local_copy_list = [
            [structure.get_file_abs_path(), structure.filename],
            [surface_sample.get_file_abs_path(), surface_sample.filename],
        ]
        calcinfo.remote_copy_list = []
        calcinfo.retrieve_list = parameters.output_files

        codeinfo = CodeInfo()
        # will call ./code.py in.json out.json
        codeinfo.cmdline_params = parameters.cmdline_params(
            structure_file_name=structure.filename,
            surface_sample_file_name=surface_sample.filename)
        codeinfo.code_uuid = code.uuid
        calcinfo.codes_info = [codeinfo]

        return calcinfo","Create input files.

            :param tempfolder: aiida.common.folders.Folder subclass where
                the plugin should put all its files.
            :param inputdict: dictionary of the input nodes as they would
                be returned by get_inputs_dict"
"def _create_repo(self, args):
        '''
        Scan a directory and create an SPM-METADATA file which describes
        all of the SPM files in that directory.
        '''
        if len(args) < 2:
            raise SPMInvocationError('A path to a directory must be specified')

        if args[1] == '.':
            repo_path = os.getcwdu()
        else:
            repo_path = args[1]

        old_files = []
        repo_metadata = {}
        for (dirpath, dirnames, filenames) in salt.utils.path.os_walk(repo_path):
            for spm_file in filenames:
                if not spm_file.endswith('.spm'):
                    continue
                spm_path = '{0}/{1}'.format(repo_path, spm_file)
                if not tarfile.is_tarfile(spm_path):
                    continue
                comps = spm_file.split('-')
                spm_name = '-'.join(comps[:-2])
                spm_fh = tarfile.open(spm_path, 'r:bz2')
                formula_handle = spm_fh.extractfile('{0}/FORMULA'.format(spm_name))
                formula_conf = salt.utils.yaml.safe_load(formula_handle.read())

                use_formula = True
                if spm_name in repo_metadata:
                    # This package is already in the repo; use the latest
                    cur_info = repo_metadata[spm_name]['info']
                    new_info = formula_conf
                    if int(new_info['version']) == int(cur_info['version']):
                        # Version is the same, check release
                        if int(new_info['release']) < int(cur_info['release']):
                            # This is an old release; don't use it
                            use_formula = False
                    elif int(new_info['version']) < int(cur_info['version']):
                        # This is an old version; don't use it
                        use_formula = False

                    if use_formula is True:
                        # Ignore/archive/delete the old version
                        log.debug(
                            '%s %s-%s had been added, but %s-%s will replace it',
                            spm_name, cur_info['version'], cur_info['release'],
                            new_info['version'], new_info['release']
                        )
                        old_files.append(repo_metadata[spm_name]['filename'])
                    else:
                        # Ignore/archive/delete the new version
                        log.debug(
                            '%s %s-%s has been found, but is older than %s-%s',
                            spm_name, new_info['version'], new_info['release'],
                            cur_info['version'], cur_info['release']
                        )
                        old_files.append(spm_file)

                if use_formula is True:
                    log.debug(
                        'adding %s-%s-%s to the repo',
                        formula_conf['name'], formula_conf['version'],
                        formula_conf['release']
                    )
                    repo_metadata[spm_name] = {
                        'info': formula_conf.copy(),
                    }
                    repo_metadata[spm_name]['filename'] = spm_file

        metadata_filename = '{0}/SPM-METADATA'.format(repo_path)
        with salt.utils.files.fopen(metadata_filename, 'w') as mfh:
            salt.utils.yaml.safe_dump(
                repo_metadata,
                mfh,
                indent=4,
                canonical=False,
                default_flow_style=False,
            )

        log.debug('Wrote %s', metadata_filename)

        for file_ in old_files:
            if self.opts['spm_repo_dups'] == 'ignore':
                # ignore old packages, but still only add the latest
                log.debug('%s will be left in the directory', file_)
            elif self.opts['spm_repo_dups'] == 'archive':
                # spm_repo_archive_path is where old packages are moved
                if not os.path.exists('./archive'):
                    try:
                        os.makedirs('./archive')
                        log.debug('%s has been archived', file_)
                    except IOError:
                        log.error('Unable to create archive directory')
                try:
                    shutil.move(file_, './archive')
                except (IOError, OSError):
                    log.error('Unable to archive %s', file_)
            elif self.opts['spm_repo_dups'] == 'delete':
                # delete old packages from the repo
                try:
                    os.remove(file_)
                    log.debug('%s has been deleted', file_)
                except IOError:
                    log.error('Unable to delete %s', file_)
                except OSError:
                    # The file has already been deleted
                    pass","Scan a directory and create an SPM-METADATA file which describes
        all of the SPM files in that directory."
"def is_all_field_none(self):
        """"""
        :rtype: bool
        """"""

        if self._id_ is not None:
            return False

        if self._created is not None:
            return False

        if self._updated is not None:
            return False

        if self._year is not None:
            return False

        if self._alias_user is not None:
            return False

        return True",:rtype: bool
"def render_template(cmd_derived_from_alias, pos_args_table):
    """"""
    Render cmd_derived_from_alias as a Jinja template with pos_args_table as the arguments.

    Args:
        cmd_derived_from_alias: The string to be injected with positional arguemnts.
        pos_args_table: The dictionary used to rendered.

    Returns:
        A processed string with positional arguments injected.
    """"""
    try:
        cmd_derived_from_alias = normalize_placeholders(cmd_derived_from_alias, inject_quotes=True)
        template = jinja.Template(cmd_derived_from_alias)

        # Shlex.split allows us to split a string by spaces while preserving quoted substrings
        # (positional arguments in this case)
        rendered = shlex.split(template.render(pos_args_table))

        # Manually check if there is any runtime error (such as index out of range)
        # since Jinja template engine only checks for compile time error.
        # Only check for runtime errors if there is an empty string in rendered.
        if '' in rendered:
            check_runtime_errors(cmd_derived_from_alias, pos_args_table)

        return rendered
    except Exception as exception:
        # Exception raised from runtime error
        if isinstance(exception, CLIError):
            raise

        # The template has some sort of compile time errors
        split_exception_message = str(exception).split()

        # Check if the error message provides the index of the erroneous character
        error_index = split_exception_message[-1]
        if error_index.isdigit():
            split_exception_message.insert(-1, 'index')
            error_msg = RENDER_TEMPLATE_ERROR.format(' '.join(split_exception_message), cmd_derived_from_alias)

            # Calculate where to put an arrow (^) char so that it is exactly below the erroneous character
            # e.g. ... ""{{a.split('|)}}""
            #                       ^
            error_msg += '\n{}^'.format(' ' * (len(error_msg) - len(cmd_derived_from_alias) + int(error_index) - 1))
        else:
            exception_str = str(exception).replace('""{{', '}}').replace('}}""', '}}')
            error_msg = RENDER_TEMPLATE_ERROR.format(cmd_derived_from_alias, exception_str)

        raise CLIError(error_msg)","Render cmd_derived_from_alias as a Jinja template with pos_args_table as the arguments.

    Args:
        cmd_derived_from_alias: The string to be injected with positional arguemnts.
        pos_args_table: The dictionary used to rendered.

    Returns:
        A processed string with positional arguments injected."
"def UpdateHunt(hunt_id, client_limit=None, client_rate=None, duration=None):
  """"""Updates a hunt (it must be paused to be updated).""""""

  hunt_obj = data_store.REL_DB.ReadHuntObject(hunt_id)
  if hunt_obj.hunt_state != hunt_obj.HuntState.PAUSED:
    raise OnlyPausedHuntCanBeModifiedError(hunt_obj)

  data_store.REL_DB.UpdateHuntObject(
      hunt_id,
      client_limit=client_limit,
      client_rate=client_rate,
      duration=duration)
  return data_store.REL_DB.ReadHuntObject(hunt_id)",Updates a hunt (it must be paused to be updated).
"def from_buffer(string, config_path=None):
    '''
    Detects MIME type of the buffered content
    :param string: buffered content whose type needs to be detected
    :return:
    '''
    status, response = callServer('put', ServerEndpoint, '/detect/stream', string,
                                  {'Accept': 'text/plain'}, False, config_path=config_path)
    return response","Detects MIME type of the buffered content
    :param string: buffered content whose type needs to be detected
    :return:"
"def set_index(self, keys):
        """"""Set the index of the DataFrame to be the keys columns.

        Note this means that the old index is removed.

        Parameters
        ----------
        keys : str or list of str
            Which column(s) to set as the index.

        Returns
        -------
        DataFrame
            DataFrame with the index set to the column(s) corresponding to the keys.

        """"""
        if isinstance(keys, str):
            column = self._data[keys]
            new_index = Index(column.values, column.dtype, column.name)

            new_data = OrderedDict((sr.name, Series(sr.values, new_index, sr.dtype, sr.name))
                                   for sr in self._iter())
            del new_data[keys]

            return DataFrame(new_data, new_index)
        elif isinstance(keys, list):
            check_inner_types(keys, str)

            new_index_data = []
            for column_name in keys:
                column = self._data[column_name]
                new_index_data.append(Index(column.values, column.dtype, column.name))
            new_index = MultiIndex(new_index_data, keys)

            new_data = OrderedDict((sr.name, Series(sr.values, new_index, sr.dtype, sr.name))
                                   for sr in self._iter())
            for column_name in keys:
                del new_data[column_name]

            return DataFrame(new_data, new_index)
        else:
            raise TypeError('Expected a string or a list of strings')","Set the index of the DataFrame to be the keys columns.

        Note this means that the old index is removed.

        Parameters
        ----------
        keys : str or list of str
            Which column(s) to set as the index.

        Returns
        -------
        DataFrame
            DataFrame with the index set to the column(s) corresponding to the keys."
"def follow_redirections(self, request):
        """"""Follow all redirections of http response.""""""
        log.debug(LOG_CHECK, ""follow all redirections"")
        if self.is_redirect():
            # run connection plugins for old connection
            self.aggregate.plugin_manager.run_connection_plugins(self)
        response = None
        for response in self.get_redirects(request):
            newurl = response.url
            log.debug(LOG_CHECK, ""Redirected to %r"", newurl)
            self.aliases.append(newurl)
            # XXX on redirect errors this is not printed
            self.add_info(_(""Redirected to `%(url)s'."") % {'url': newurl})
            # Reset extern and recalculate
            self.extern = None
            self.set_extern(newurl)
            self.urlparts = strformat.url_unicode_split(newurl)
            self.build_url_parts()
            self.url_connection = response
            self.headers = response.headers
            self.url = urlutil.urlunsplit(self.urlparts)
            self.scheme = self.urlparts[0].lower()
            self._add_ssl_info()
            self._add_response_info()
            if self.is_redirect():
                # run connection plugins for old connection
                self.aggregate.plugin_manager.run_connection_plugins(self)",Follow all redirections of http response.
"def load_average(self):
        """"""
        Returns the current load average.
        """"""
        with io.open(self.load_average_file, 'r') as f:
            file_columns = f.readline().strip().split()
            return float(file_columns[self._load_average_file_column])",Returns the current load average.
"def destroy(self, names, cached=False):
        '''
        Destroy the named VMs
        '''
        processed = {}
        names = set(names)
        matching = self.get_running_by_names(names, cached=cached)
        vms_to_destroy = set()
        parallel_data = []
        for alias, drivers in six.iteritems(matching):
            for driver, vms in six.iteritems(drivers):
                for name in vms:
                    if name in names:
                        vms_to_destroy.add((alias, driver, name))
                        if self.opts['parallel']:
                            parallel_data.append({
                                'opts': self.opts,
                                'name': name,
                                'alias': alias,
                                'driver': driver,
                            })

        # destroying in parallel
        if self.opts['parallel'] and parallel_data:
            # set the pool size based on configuration or default to
            # the number of machines we're destroying
            if 'pool_size' in self.opts:
                pool_size = self.opts['pool_size']
            else:
                pool_size = len(parallel_data)
            log.info('Destroying in parallel mode; '
                     'Cloud pool size: %s', pool_size)

            # kick off the parallel destroy
            output_multip = enter_mainloop(
                _destroy_multiprocessing, parallel_data, pool_size=pool_size)

            # massage the multiprocessing output a bit
            ret_multip = {}
            for obj in output_multip:
                ret_multip.update(obj)

            # build up a data structure similar to what the non-parallel
            # destroy uses
            for obj in parallel_data:
                alias = obj['alias']
                driver = obj['driver']
                name = obj['name']
                if alias not in processed:
                    processed[alias] = {}
                if driver not in processed[alias]:
                    processed[alias][driver] = {}
                processed[alias][driver][name] = ret_multip[name]
                if name in names:
                    names.remove(name)

        # not destroying in parallel
        else:
            log.info('Destroying in non-parallel mode.')
            for alias, driver, name in vms_to_destroy:
                fun = '{0}.destroy'.format(driver)
                with salt.utils.context.func_globals_inject(
                    self.clouds[fun],
                    __active_provider_name__=':'.join([alias, driver])
                ):
                    ret = self.clouds[fun](name)
                if alias not in processed:
                    processed[alias] = {}
                if driver not in processed[alias]:
                    processed[alias][driver] = {}
                processed[alias][driver][name] = ret
                if name in names:
                    names.remove(name)

        # now the processed data structure contains the output from either
        # the parallel or non-parallel destroy and we should finish up
        # with removing minion keys if necessary
        for alias, driver, name in vms_to_destroy:
            ret = processed[alias][driver][name]
            if not ret:
                continue

            vm_ = {
                'name': name,
                'profile': None,
                'provider': ':'.join([alias, driver]),
                'driver': driver
            }
            minion_dict = salt.config.get_cloud_config_value(
                'minion', vm_, self.opts, default={}
            )
            key_file = os.path.join(
                self.opts['pki_dir'], 'minions', minion_dict.get('id', name)
            )
            globbed_key_file = glob.glob('{0}.*'.format(key_file))

            if not os.path.isfile(key_file) and not globbed_key_file:
                # There's no such key file!? It might have been renamed
                if isinstance(ret, dict) and 'newname' in ret:
                    salt.utils.cloud.remove_key(
                        self.opts['pki_dir'], ret['newname']
                    )
                continue

            if os.path.isfile(key_file) and not globbed_key_file:
                # Single key entry. Remove it!
                salt.utils.cloud.remove_key(self.opts['pki_dir'], os.path.basename(key_file))
                continue

            # Since we have globbed matches, there are probably some keys for which their minion
            # configuration has append_domain set.
            if not os.path.isfile(key_file) and globbed_key_file and len(globbed_key_file) == 1:
                # Single entry, let's remove it!
                salt.utils.cloud.remove_key(
                    self.opts['pki_dir'],
                    os.path.basename(globbed_key_file[0])
                )
                continue

            # Since we can't get the profile or map entry used to create
            # the VM, we can't also get the append_domain setting.
            # And if we reached this point, we have several minion keys
            # who's name starts with the machine name we're deleting.
            # We need to ask one by one!?
            print(
                'There are several minion keys who\'s name starts '
                'with \'{0}\'. We need to ask you which one should be '
                'deleted:'.format(
                    name
                )
            )
            while True:
                for idx, filename in enumerate(globbed_key_file):
                    print(' {0}: {1}'.format(
                        idx, os.path.basename(filename)
                    ))
                selection = input(
                    'Which minion key should be deleted(number)? '
                )
                try:
                    selection = int(selection)
                except ValueError:
                    print(
                        '\'{0}\' is not a valid selection.'.format(selection)
                    )

                try:
                    filename = os.path.basename(
                        globbed_key_file.pop(selection)
                    )
                except Exception:
                    continue

                delete = input(
                    'Delete \'{0}\'? [Y/n]? '.format(filename)
                )
                if delete == '' or delete.lower().startswith('y'):
                    salt.utils.cloud.remove_key(
                        self.opts['pki_dir'], filename
                    )
                    print('Deleted \'{0}\''.format(filename))
                    break

                print('Did not delete \'{0}\''.format(filename))
                break

        if names and not processed:
            # These machines were asked to be destroyed but could not be found
            raise SaltCloudSystemExit(
                'The following VM\'s were not found: {0}'.format(
                    ', '.join(names)
                )
            )

        elif names and processed:
            processed['Not Found'] = names

        elif not processed:
            raise SaltCloudSystemExit('No machines were destroyed!')

        return processed",Destroy the named VMs
"def compute_canonical_key_ids(self, search_amplifier=100):
        """"""
        A canonical key id is the lowest integer key id that maps to
        a particular shard. The mapping to canonical key ids depends on the
        number of shards.

        Returns a dictionary mapping from shard number to canonical key id.

        This method will throw an exception if it fails to compute all of
        the canonical key ids.
        """"""
        canonical_keys = {}
        num_shards = self.num_shards()
        # Guarantees enough to find all keys without running forever
        num_iterations = (num_shards**2) * search_amplifier
        for key_id in range(1, num_iterations):
            shard_num = self.get_shard_num_by_key(str(key_id))
            if shard_num in canonical_keys:
                continue
            canonical_keys[shard_num] = str(key_id)
            if len(canonical_keys) == num_shards:
                break

        if len(canonical_keys) != num_shards:
            raise ValueError(""Failed to compute enough keys. "" +
                             ""Wanted %d, got %d (search_amp=%d)."".format(
                                 num_shards, len(canonical_keys),
                                 search_amplifier))

        return canonical_keys","A canonical key id is the lowest integer key id that maps to
        a particular shard. The mapping to canonical key ids depends on the
        number of shards.

        Returns a dictionary mapping from shard number to canonical key id.

        This method will throw an exception if it fails to compute all of
        the canonical key ids."
"def radial_symmetry(mesh):
    """"""
    Check whether a mesh has rotational symmetry.

    Returns
    -----------
    symmetry : None or str
         None         No rotational symmetry
         'radial'     Symmetric around an axis
         'spherical'  Symmetric around a point
    axis : None or (3,) float
      Rotation axis or point
    section : None or (3, 2) float
      If radial symmetry provide vectors
      to get cross section
    """"""

    # if not a volume this is meaningless
    if not mesh.is_volume:
        return None, None, None

    # the sorted order of the principal components of inertia (3,) float
    order = mesh.principal_inertia_components.argsort()

    # we are checking if a geometry has radial symmetry
    # if 2 of the PCI are equal, it is a revolved 2D profile
    # if 3 of the PCI (all of them) are equal it is a sphere
    # thus we take the diff of the sorted PCI, scale it as a ratio
    # of the largest PCI, and then scale to the tolerance we care about
    # if tol is 1e-3, that means that 2 components are identical if they
    # are within .1% of the maximum PCI.
    diff = np.abs(np.diff(mesh.principal_inertia_components[order]))
    diff /= np.abs(mesh.principal_inertia_components).max()
    # diffs that are within tol of zero
    diff_zero = (diff / 1e-3).astype(int) == 0

    if diff_zero.all():
        # this is the case where all 3 PCI are identical
        # this means that the geometry is symmetric about a point
        # examples of this are a sphere, icosahedron, etc
        axis = mesh.principal_inertia_vectors[0]
        section = mesh.principal_inertia_vectors[1:]

        return 'spherical', axis, section

    elif diff_zero.any():
        # this is the case for 2/3 PCI are identical
        # this means the geometry is symmetric about an axis
        # probably a revolved 2D profile

        # we know that only 1/2 of the diff values are True
        # if the first diff is 0, it means if we take the first element
        # in the ordered PCI we will have one of the non- revolve axis
        # if the second diff is 0, we take the last element of
        # the ordered PCI for the section axis
        # if we wanted the revolve axis we would just switch [0,-1] to
        # [-1,0]

        # since two vectors are the same, we know the middle
        # one is one of those two
        section_index = order[np.array([[0, 1],
                                        [1, -1]])[diff_zero]].flatten()
        section = mesh.principal_inertia_vectors[section_index]

        # we know the rotation axis is the sole unique value
        # and is either first or last of the sorted values
        axis_index = order[np.array([-1, 0])[diff_zero]][0]
        axis = mesh.principal_inertia_vectors[axis_index]
        return 'radial', axis, section

    return None, None, None","Check whether a mesh has rotational symmetry.

    Returns
    -----------
    symmetry : None or str
         None         No rotational symmetry
         'radial'     Symmetric around an axis
         'spherical'  Symmetric around a point
    axis : None or (3,) float
      Rotation axis or point
    section : None or (3, 2) float
      If radial symmetry provide vectors
      to get cross section"
"def apply_transformer_types(network):
    """"""Calculate transformer electrical parameters x, r, b, g from
    standard types.

    """"""

    trafos_with_types_b = network.transformers.type != """"
    if trafos_with_types_b.zsum() == 0:
        return

    missing_types = (pd.Index(network.transformers.loc[trafos_with_types_b, 'type'].unique())
                     .difference(network.transformer_types.index))
    assert missing_types.empty, (""The type(s) {} do(es) not exist in network.transformer_types""
                                 .format("", "".join(missing_types)))

    # Get a copy of the transformers data
    # (joining pulls in ""phase_shift"", ""s_nom"", ""tap_side"" from TransformerType)
    t = (network.transformers.loc[trafos_with_types_b, [""type"", ""tap_position"", ""num_parallel""]]
         .join(network.transformer_types, on='type'))

    t[""r""] = t[""vscr""] /100.
    t[""x""] = np.sqrt((t[""vsc""]/100.)**2 - t[""r""]**2)

    #NB: b and g are per unit of s_nom
    t[""g""] = t[""pfe""]/(1000. * t[""s_nom""])

    #for some bizarre reason, some of the standard types in pandapower have i0^2 < g^2
    t[""b""] = - np.sqrt(((t[""i0""]/100.)**2 - t[""g""]**2).clip(lower=0))

    for attr in [""r"",""x""]:
        t[attr] /= t[""num_parallel""]

    for attr in [""b"",""g""]:
        t[attr] *= t[""num_parallel""]

    #deal with tap positions

    t[""tap_ratio""] = 1. + (t[""tap_position""] - t[""tap_neutral""]) * (t[""tap_step""]/100.)

    # now set calculated values on live transformers
    for attr in [""r"", ""x"", ""g"", ""b"", ""phase_shift"", ""s_nom"", ""tap_side"", ""tap_ratio""]:
        network.transformers.loc[trafos_with_types_b, attr] = t[attr]","Calculate transformer electrical parameters x, r, b, g from
    standard types."
"def create_content_instance(content_plugin_class, page, placeholder_name='main', **kwargs):
    """"""
    Creates a content instance from a content plugin class.

    :param content_plugin_class: The class of the content plugin.
    :param page: The fluent_page instance to create the content
    instance one.
    :param placeholder_name: The placeholder name defined in the
    template. [DEFAULT: main]
    :param kwargs: Additional keyword arguments to be used in the
    content instance creation.
    :return: The content instance created.
    """"""
    # Get the placeholders that are currently available for the slot.
    placeholders = page.get_placeholder_by_slot(placeholder_name)

    # If a placeholder exists for the placeholder_name use the first one provided otherwise create
    # a new placeholder instance.
    if placeholders.exists():
        placeholder = placeholders[0]
    else:
        placeholder = page.create_placeholder(placeholder_name)

    # Obtain the content type for the page instance class.
    ct = ContentType.objects.get_for_model(type(page))

    # Create the actual plugin instance.
    try:
        content_instance = content_plugin_class.objects.create(
            parent_type=ct,
            parent_id=page.id,
            placeholder=placeholder,
            **kwargs
        )
    except TypeError:
        raise Exception(
            'Could not create content item instance, ensure you '
            'have all required field values for the Model.'
        )
    return content_instance","Creates a content instance from a content plugin class.

    :param content_plugin_class: The class of the content plugin.
    :param page: The fluent_page instance to create the content
    instance one.
    :param placeholder_name: The placeholder name defined in the
    template. [DEFAULT: main]
    :param kwargs: Additional keyword arguments to be used in the
    content instance creation.
    :return: The content instance created."
"def _resolve_name(self, name):
        """"""TODO: Docstring for _resolve_names.

        :name: TODO
        :returns: TODO

        """"""
        res = [name]
        while True:
            orig_names = self._search(""types"", name)
            if orig_names is not None:
                name = orig_names[-1]
                # pop off the typedefd name
                res.pop()
                # add back on the original names
                res += orig_names
            else:
                break

        return res","TODO: Docstring for _resolve_names.

        :name: TODO
        :returns: TODO"
"def _id_or_key(list_item):
    '''
    Return the value at key 'id' or 'key'.
    '''
    if isinstance(list_item, dict):
        if 'id' in list_item:
            return list_item['id']
        if 'key' in list_item:
            return list_item['key']
    return list_item",Return the value at key 'id' or 'key'.
"def do_var(self, arg, arguments):
        """"""
        Usage:
            var list 
            var delete NAMES
            var NAME=VALUE
            var NAME

        Arguments:
            NAME    Name of the variable
            NAMES   Names of the variable separated by spaces
            VALUE   VALUE to be assigned

        special vars date and time are defined
        """"""
        if arguments['list'] or arg == '' or arg is None:
            self._list_variables()
            return

        elif arguments['NAME=VALUE'] and ""="" in arguments[""NAME=VALUE""]:
            (variable, value) = arg.split('=', 1)
            if value == ""time"" or value == ""now"":
                value = datetime.datetime.now().strftime(""%H:%M:%S"")
            elif value == ""date"":
                value = datetime.datetime.now().strftime(""%Y-%m-%d"")
            self._add_variable(variable, value)
            return
        elif arguments['NAME=VALUE'] and ""="" in arguments[""NAME=VALUE""]:
            try:
                v = arguments['NAME=VALUE']
                Console.ok(str(self.variables[v]))
            except:
                Console.error('variable {:} not defined'.format(arguments['NAME=VALUE']))
            
        elif arg.startswith('delete'):
            variable = arg.split(' ')[1]
            self._delete_variable(variable)
            return","Usage:
            var list 
            var delete NAMES
            var NAME=VALUE
            var NAME

        Arguments:
            NAME    Name of the variable
            NAMES   Names of the variable separated by spaces
            VALUE   VALUE to be assigned

        special vars date and time are defined"
"def s3(ctx, bucket_name, data_file, region):
    """"""Use the S3 SWAG backend.""""""
    if not ctx.data_file:
        ctx.data_file = data_file

    if not ctx.bucket_name:
        ctx.bucket_name = bucket_name

    if not ctx.region:
        ctx.region = region

    ctx.type = 's3'",Use the S3 SWAG backend.
"def login_exists(login, domain='', **kwargs):
    '''
    Find if a login exists in the MS SQL server.
    domain, if provided, will be prepended to login

    CLI Example:

    .. code-block:: bash

        salt minion mssql.login_exists 'LOGIN'
    '''
    if domain:
        login = '{0}\\{1}'.format(domain, login)
    try:
        # We should get one, and only one row
        return len(tsql_query(query=""SELECT name FROM sys.syslogins WHERE name='{0}'"".format(login), **kwargs)) == 1

    except Exception as e:
        return 'Could not find the login: {0}'.format(e)","Find if a login exists in the MS SQL server.
    domain, if provided, will be prepended to login

    CLI Example:

    .. code-block:: bash

        salt minion mssql.login_exists 'LOGIN'"
"def network_interface_delete(name, resource_group, **kwargs):
    '''
    .. versionadded:: 2019.2.0

    Delete a network interface.

    :param name: The name of the network interface to delete.

    :param resource_group: The resource group name assigned to the
        network interface.

    CLI Example:

    .. code-block:: bash

        salt-call azurearm_network.network_interface_delete test-iface0 testgroup

    '''
    result = False

    netconn = __utils__['azurearm.get_client']('network', **kwargs)
    try:
        nic = netconn.network_interfaces.delete(
            network_interface_name=name,
            resource_group_name=resource_group
        )
        nic.wait()
        result = True
    except CloudError as exc:
        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)

    return result",".. versionadded:: 2019.2.0

    Delete a network interface.

    :param name: The name of the network interface to delete.

    :param resource_group: The resource group name assigned to the
        network interface.

    CLI Example:

    .. code-block:: bash

        salt-call azurearm_network.network_interface_delete test-iface0 testgroup"
"def list_attributes(self, name):
        """"""
        Look up the attributes of a list.

        Args:
            name (str): The name of the list

        Returns:
            dict: attributes of the list
        """"""
        result = self.client.service.getListAttributes(name, self.proxy_id)
        if isinstance(result, list) and len(result) == 1:
            return result[0]
        return result","Look up the attributes of a list.

        Args:
            name (str): The name of the list

        Returns:
            dict: attributes of the list"
"def multiget(client, keys, **options):
    """"""Executes a parallel-fetch across multiple threads. Returns a list
    containing :class:`~riak.riak_object.RiakObject` or
    :class:`~riak.datatypes.Datatype` instances, or 4-tuples of
    bucket-type, bucket, key, and the exception raised.

    If a ``pool`` option is included, the request will use the given worker
    pool and not a transient :class:`~riak.client.multi.MultiGetPool`. This
    option will be passed by the client if the ``multiget_pool_size``
    option was set on client initialization.

    :param client: the client to use
    :type client: :class:`~riak.client.RiakClient`
    :param keys: the keys to fetch in parallel
    :type keys: list of three-tuples -- bucket_type/bucket/key
    :param options: request options to
        :meth:`RiakBucket.get <riak.bucket.RiakBucket.get>`
    :type options: dict
    :rtype: list

    """"""
    transient_pool = False
    outq = Queue()

    if 'pool' in options:
        pool = options['pool']
        del options['pool']
    else:
        pool = MultiGetPool()
        transient_pool = True

    try:
        pool.start()
        for bucket_type, bucket, key in keys:
            task = Task(client, outq, bucket_type, bucket, key, None, options)
            pool.enq(task)

        results = []
        for _ in range(len(keys)):
            if pool.stopped():
                raise RuntimeError(
                        'Multi-get operation interrupted by pool '
                        'stopping!')
            results.append(outq.get())
            outq.task_done()
    finally:
        if transient_pool:
            pool.stop()

    return results","Executes a parallel-fetch across multiple threads. Returns a list
    containing :class:`~riak.riak_object.RiakObject` or
    :class:`~riak.datatypes.Datatype` instances, or 4-tuples of
    bucket-type, bucket, key, and the exception raised.

    If a ``pool`` option is included, the request will use the given worker
    pool and not a transient :class:`~riak.client.multi.MultiGetPool`. This
    option will be passed by the client if the ``multiget_pool_size``
    option was set on client initialization.

    :param client: the client to use
    :type client: :class:`~riak.client.RiakClient`
    :param keys: the keys to fetch in parallel
    :type keys: list of three-tuples -- bucket_type/bucket/key
    :param options: request options to
        :meth:`RiakBucket.get <riak.bucket.RiakBucket.get>`
    :type options: dict
    :rtype: list"
"def get_link(self, task_id):
        """"""Get a ``LinkOfTrust`` by task id.

        Args:
            task_id (str): the task id to find.

        Returns:
            LinkOfTrust: the link matching the task id.

        Raises:
            CoTError: if no ``LinkOfTrust`` matches.

        """"""
        links = [x for x in self.links if x.task_id == task_id]
        if len(links) != 1:
            raise CoTError(""No single Link matches task_id {}!\n{}"".format(task_id, self.dependent_task_ids()))
        return links[0]","Get a ``LinkOfTrust`` by task id.

        Args:
            task_id (str): the task id to find.

        Returns:
            LinkOfTrust: the link matching the task id.

        Raises:
            CoTError: if no ``LinkOfTrust`` matches."
"def _get_rc():
    '''
    Returns a dict where the key is the daemon's name and
    the value a boolean indicating its status (True: enabled or False: disabled).
    Check the daemons started by the system in /etc/rc and
    configured in /etc/rc.conf and /etc/rc.conf.local.
    Also add to the dict all the localy enabled daemons via $pkg_scripts.
    '''
    daemons_flags = {}

    try:
        # now read the system startup script /etc/rc
        # to know what are the system enabled daemons
        with salt.utils.files.fopen('/etc/rc', 'r') as handle:
            lines = salt.utils.data.decode(handle.readlines())
    except IOError:
        log.error('Unable to read /etc/rc')
    else:
        for line in lines:
            match = start_daemon_call_regex.match(line)
            if match:
                # the matched line is a call to start_daemon()
                # we remove the function name
                line = line[len(match.group(1)):]
                # we retrieve each daemon name from the parameters of start_daemon()
                for daemon in start_daemon_parameter_regex.findall(line):
                    # mark it as enabled
                    daemons_flags[daemon] = True

    # this will execute rc.conf and rc.conf.local
    # used in /etc/rc at boot to start the daemons
    variables = __salt__['cmd.run']('(. /etc/rc.conf && set)',
                                    clean_env=True,
                                    output_loglevel='quiet',
                                    python_shell=True).split('\n')
    for var in variables:
        match = service_flags_regex.match(var)
        if match:
            # the matched var look like daemon_name_flags=, we test its assigned value
            # NO: disabled, everything else: enabled
            # do not create a new key if the service hasn't been found in /etc/rc, see $pkg_scripts
            if match.group(2) == 'NO':
                daemons_flags[match.group(1)] = False
        else:
            match = pkg_scripts_regex.match(var)
            if match:
                # the matched var is pkg_scripts
                # we can retrieve the name of each localy enabled daemon that wasn't hand started via /etc/rc
                for daemon in match.group(1).split():
                    # create a new key and mark it as enabled
                    daemons_flags[daemon] = True

    return daemons_flags","Returns a dict where the key is the daemon's name and
    the value a boolean indicating its status (True: enabled or False: disabled).
    Check the daemons started by the system in /etc/rc and
    configured in /etc/rc.conf and /etc/rc.conf.local.
    Also add to the dict all the localy enabled daemons via $pkg_scripts."
"def sell(self, product_id, order_type, **kwargs):
        """"""Place a sell order.

        This is included to maintain backwards compatibility with older versions
        of cbpro-Python. For maximum support from docstrings and function
        signatures see the order type-specific functions place_limit_order,
        place_market_order, and place_stop_order.

        Args:
            product_id (str): Product to order (eg. 'BTC-USD')
            order_type (str): Order type ('limit', 'market', or 'stop')
            **kwargs: Additional arguments can be specified for different order
                types.

        Returns:
            dict: Order details. See `place_order` for example.

        """"""
        return self.place_order(product_id, 'sell', order_type, **kwargs)","Place a sell order.

        This is included to maintain backwards compatibility with older versions
        of cbpro-Python. For maximum support from docstrings and function
        signatures see the order type-specific functions place_limit_order,
        place_market_order, and place_stop_order.

        Args:
            product_id (str): Product to order (eg. 'BTC-USD')
            order_type (str): Order type ('limit', 'market', or 'stop')
            **kwargs: Additional arguments can be specified for different order
                types.

        Returns:
            dict: Order details. See `place_order` for example."
"def get_reference_repository(self, reference: Optional[Path], repo: str) -> Optional[Path]:
        """"""
        Returns a repository to use in clone command, if there is one to be referenced.
        Either provided by the user of generated from already cloned branches (master is preferred).

        :param reference: Path to a local repository provided by the user or None.
        :param repo: Reference for which remote repository.
        """"""
        if reference is not None:
            return reference.absolute()

        repo_path = self.get_path_to_repo(repo)

        if not repo_path.exists():
            return None

        master = repo_path / ""master""

        if master.exists() and master.is_dir():
            return master

        for existing_branch in repo_path.iterdir():
            if not existing_branch.is_dir():
                continue

            return existing_branch.resolve()

        return None","Returns a repository to use in clone command, if there is one to be referenced.
        Either provided by the user of generated from already cloned branches (master is preferred).

        :param reference: Path to a local repository provided by the user or None.
        :param repo: Reference for which remote repository."
"def at(self, timestamp):
        """"""
        Force the create date of an object to be at a certain time; This
        method can be invoked only on a freshly created Versionable object.
        It must not have been cloned yet. Raises a SuspiciousOperation
        exception, otherwise.
        :param timestamp: a datetime.datetime instance
        """"""
        # Ensure, it's not a historic item
        if not self.is_current:
            raise SuspiciousOperation(
                ""Cannot relocate this Versionable instance in time, since it ""
                ""is a historical item"")
        # Ensure it's not a versioned item (that would lead to some ugly
        # situations...
        if not self.version_birth_date == self.version_start_date:
            raise SuspiciousOperation(
                ""Cannot relocate this Versionable instance in time, since it ""
                ""is a versioned instance"")
        # Ensure the argument is really a timestamp
        if not isinstance(timestamp, datetime.datetime):
            raise ValueError(""This is not a datetime.datetime timestamp"")
        self.version_birth_date = self.version_start_date = timestamp
        return self","Force the create date of an object to be at a certain time; This
        method can be invoked only on a freshly created Versionable object.
        It must not have been cloned yet. Raises a SuspiciousOperation
        exception, otherwise.
        :param timestamp: a datetime.datetime instance"
"def from_string(cls, s):
        """"""Return a :class:`JobStatus` instance from its string representation.""""""
        for num, text in cls._STATUS_TABLE.items():
            if text == s: return cls(num)
        else:
            #raise ValueError(""Wrong string %s"" % s)
            logger.warning(""Got unknown status: %s"" % s)
            return cls.from_string(""UNKNOWN"")",Return a :class:`JobStatus` instance from its string representation.
"def _data_from_df(df):
        ''' Create a ``dict`` of columns from a Pandas ``DataFrame``,
        suitable for creating a ColumnDataSource.

        Args:
            df (DataFrame) : data to convert

        Returns:
            dict[str, np.array]

        '''
        _df = df.copy()

        # Flatten columns
        if isinstance(df.columns, pd.MultiIndex):
            try:
                _df.columns = ['_'.join(col) for col in _df.columns.values]
            except TypeError:
                raise TypeError('Could not flatten MultiIndex columns. '
                                'use string column names or flatten manually')
        # Transform columns CategoricalIndex in list
        if isinstance(df.columns, pd.CategoricalIndex):
            _df.columns = df.columns.tolist()
        # Flatten index
        index_name = ColumnDataSource._df_index_name(df)
        if index_name == 'index':
            _df.index = pd.Index(_df.index.values)
        else:
            _df.index = pd.Index(_df.index.values, name=index_name)
        _df.reset_index(inplace=True)

        tmp_data = {c: v.values for c, v in _df.iteritems()}

        new_data = {}
        for k, v in tmp_data.items():
            new_data[k] = v

        return new_data","Create a ``dict`` of columns from a Pandas ``DataFrame``,
        suitable for creating a ColumnDataSource.

        Args:
            df (DataFrame) : data to convert

        Returns:
            dict[str, np.array]"
"def community_post_subscriptions(self, post_id, **kwargs):
        ""https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-post-subscriptions""
        api_path = ""/api/v2/community/posts/{post_id}/subscriptions.json""
        api_path = api_path.format(post_id=post_id)
        return self.call(api_path, **kwargs)",https://developer.zendesk.com/rest_api/docs/help_center/subscriptions#list-post-subscriptions
"def fit_df(self, dfs, pstate_col=PSTATE_COL):
        """"""
        Convenience function to fit a model from a list of dataframes
        """"""
        obs_cols = list(self.emission_name)
        obs = [df[df.columns.difference([pstate_col])][obs_cols].values for df in dfs]
        pstates = [df[pstate_col].values for df in dfs]
        return self.fit(obs, pstates)",Convenience function to fit a model from a list of dataframes
"def _step(self,
              model: TrainingModel,
              batch: mx.io.DataBatch,
              checkpoint_interval: int,
              metric_train: mx.metric.EvalMetric,
              metric_loss: Optional[mx.metric.EvalMetric] = None):
        """"""
        Performs an update to model given a batch and updates metrics.
        """"""

        if model.monitor is not None:
            model.monitor.tic()

        ####################
        # Forward & Backward
        ####################
        model.run_forward_backward(batch, metric_train)

        # If using an extended optimizer, provide extra state information about the current batch
        optimizer = model.optimizer
        if metric_loss is not None and isinstance(optimizer, SockeyeOptimizer):
            # Loss for this batch
            metric_loss.reset()
            metric_loss.update(batch.label, model.module.get_outputs())
            [(_, m_val)] = metric_loss.get_name_value()
            batch_state = BatchState(metric_val=m_val)
            optimizer.pre_update_batch(batch_state)

        ########
        # UPDATE
        ########
        if self.update_interval == 1 or self.state.batches % self.update_interval == 0:

            # Gradient rescaling
            gradient_norm = None
            if self.state.updates > 0 and (self.state.updates + 1) % checkpoint_interval == 0:
                # compute values for logging to metrics (before rescaling...)
                gradient_norm = self.state.gradient_norm = model.get_global_gradient_norm()
                self.state.gradients = model.get_gradients()

            # note: C.GRADIENT_CLIPPING_TYPE_ABS is handled by the mxnet optimizer directly
            if self.optimizer_config.gradient_clipping_type == C.GRADIENT_CLIPPING_TYPE_NORM:
                if gradient_norm is None:
                    gradient_norm = model.get_global_gradient_norm()
                # clip gradients
                if gradient_norm > self.optimizer_config.gradient_clipping_threshold:
                    ratio = self.optimizer_config.gradient_clipping_threshold / gradient_norm
                    model.rescale_gradients(ratio)

            model.update()

            if self.update_interval > 1:
                model.zero_gradients()

            self.state.updates += 1

        if model.monitor is not None:
            results = model.monitor.toc()
            if results:
                for _, k, v in results:
                    logger.info('Monitor: Batch [{:d}] {:s} {:s}'.format(self.state.updates, k, v))",Performs an update to model given a batch and updates metrics.
"def find_resource_list(self):
        """"""Finf resource list by hueristics, returns ResourceList object.

        1. Use explicitly specified self.sitemap_name (and
            fail if that doesn't work)
        2. Use explicitly specified self.capability_list_uri (and
            fail is that doesn't work)
        3. Look for base_url/.well-known/resourcesync (then look
            for capability, look for resourcelist)
        4. Look for host/.well-known/resourcesync (then look
            for capability, look for resourcelist)
        5. Look for base_url/resourcelist.xml
        6. Look for base_url/sitemap.xml
        7. Look for host/sitemap.xml
        """"""
        # 1 & 2
        if (self.sitemap_name is not None):
            return(self.read_resource_list(self.sitemap_name))
        if (self.capability_list_uri is not None):
            rluri = self.find_resource_list_from_capability_list(self.capability_list_uri)
            return(self.read_resource_list(rluri))
        # 3 & 4
        parts = urlsplit(self.sitemap)
        uri_host = urlunsplit([parts[0], parts[1], '', '', ''])
        errors = []
        for uri in [urljoin(self.sitemap, '.well-known/resourcesync'),
                    urljoin(uri_host, '.well-known/resourcesync')]:
            uri = uri.lstrip('file:///')  # urljoin adds this for local files
            try:
                rluri = self.find_resource_list_from_source_description(uri)
                return(self.read_resource_list(rluri))
            except ClientError as e:
                errors.append(str(e))
        # 5, 6 & 7
        for uri in [urljoin(self.sitemap, 'resourcelist.xml'),
                    urljoin(self.sitemap, 'sitemap.xml'),
                    urljoin(uri_host, 'sitemap.xml')]:
            uri = uri.lstrip('file:///')  # urljoin adds this for local files
            try:
                return(self.read_resource_list(uri))
            except ClientError as e:
                errors.append(str(e))
        raise ClientFatalError(
            ""Failed to find source resource list from common patterns (%s)"" %
            "". "".join(errors))","Finf resource list by hueristics, returns ResourceList object.

        1. Use explicitly specified self.sitemap_name (and
            fail if that doesn't work)
        2. Use explicitly specified self.capability_list_uri (and
            fail is that doesn't work)
        3. Look for base_url/.well-known/resourcesync (then look
            for capability, look for resourcelist)
        4. Look for host/.well-known/resourcesync (then look
            for capability, look for resourcelist)
        5. Look for base_url/resourcelist.xml
        6. Look for base_url/sitemap.xml
        7. Look for host/sitemap.xml"
"def header(
            self,
            text,
            level):
        """"""*convert plain-text to MMD header*

        **Key Arguments:**
            - ``text`` -- the text to convert to MMD header
            - ``level`` -- the header level to convert the text to

        **Return:**
            - ``header`` -- the MMD header

        **Usage:**

            To convert a text MMD header:

            .. code-block:: python

                header = md.header("" This is my header  "", level=3)
                print header

                # OUTPUT:
                # ### This is my header
                #
        """"""
        m = self.reWS.match(text)

        prefix = m.group(1)
        text = m.group(2)
        suffix = m.group(3)

        return ""#"" * level + "" %(text)s  \n"" % locals()","*convert plain-text to MMD header*

        **Key Arguments:**
            - ``text`` -- the text to convert to MMD header
            - ``level`` -- the header level to convert the text to

        **Return:**
            - ``header`` -- the MMD header

        **Usage:**

            To convert a text MMD header:

            .. code-block:: python

                header = md.header("" This is my header  "", level=3)
                print header

                # OUTPUT:
                # ### This is my header
                #"
"def fit_freq_std_dev(self, training_signal):
        """"""Defines a spectral mask based on training data using the standard deviation values of
             each frequency component

        Args:
            training_signal: Training data

        """"""

        window_length = len(self.window)
        window_weight = sum(self.window)
        num_of_windows = len(training_signal) - window_length - 1
        mean = np.zeros(int(window_length / 2) + 1)
        pow = np.zeros(int(window_length / 2) + 1)
        temp = np.zeros(int(window_length / 2) + 1)
        rfft = np.fft.rfft(training_signal[0:0 + window_length] * self.window)
        max = np.abs(rfft) / window_weight
        min = max

        for i in range(0, num_of_windows):
            rfft = np.fft.rfft(training_signal[i:i + window_length] * self.window)
            temp = np.abs(rfft) / window_weight
            max = np.maximum(temp, max)
            min = np.minimum(temp, min)
            mean = mean + temp
            pow = pow + np.power(temp, 2)

        mean = mean / num_of_windows
        pow = pow / num_of_windows
        std_dev = np.sqrt(pow - np.power(mean, 2))
        self.mask_top = mean + self.gain * std_dev
        self.mask_bottom = np.maximum(mean - self.gain * std_dev,
                                      np.zeros(int(window_length / 2) + 1))","Defines a spectral mask based on training data using the standard deviation values of
             each frequency component

        Args:
            training_signal: Training data"
"def list_documents(self, page_size=None):
        """"""List all subdocuments of the current collection.

        Args:
            page_size (Optional[int]]): The maximum number of documents
            in each page of results from this request. Non-positive values
            are ignored. Defaults to a sensible value set by the API.

        Returns:
            Sequence[~.firestore_v1beta1.collection.DocumentReference]:
                iterator of subdocuments of the current collection. If the
                collection does not exist at the time of `snapshot`, the
                iterator will be empty
        """"""
        parent, _ = self._parent_info()

        iterator = self._client._firestore_api.list_documents(
            parent,
            self.id,
            page_size=page_size,
            show_missing=True,
            metadata=self._client._rpc_metadata,
        )
        iterator.collection = self
        iterator.item_to_value = _item_to_document_ref
        return iterator","List all subdocuments of the current collection.

        Args:
            page_size (Optional[int]]): The maximum number of documents
            in each page of results from this request. Non-positive values
            are ignored. Defaults to a sensible value set by the API.

        Returns:
            Sequence[~.firestore_v1beta1.collection.DocumentReference]:
                iterator of subdocuments of the current collection. If the
                collection does not exist at the time of `snapshot`, the
                iterator will be empty"
"def runSearchReads(self, request):
        """"""
        Runs the specified SearchReadsRequest.
        """"""
        return self.runSearchRequest(
            request, protocol.SearchReadsRequest,
            protocol.SearchReadsResponse,
            self.readsGenerator)",Runs the specified SearchReadsRequest.
"def copy_from(self, other_state):
    """"""Copy data from another shard state entity to self.""""""
    for prop in self.properties().values():
      setattr(self, prop.name, getattr(other_state, prop.name))",Copy data from another shard state entity to self.
"def _find_erasures_locator(self, erasures_pos):
        '''Compute the erasures locator polynomial from the erasures positions (the positions must be relative to the x coefficient, eg: ""hello worldxxxxxxxxx"" is tampered to ""h_ll_ worldxxxxxxxxx"" with xxxxxxxxx being the ecc of length n-k=9, here the string positions are [1, 4], but the coefficients are reversed since the ecc characters are placed as the first coefficients of the polynomial, thus the coefficients of the erased characters are n-1 - [1, 4] = [18, 15] = erasures_loc to be specified as an argument.'''
        # See: http://ocw.usu.edu/Electrical_and_Computer_Engineering/Error_Control_Coding/lecture7.pdf and Blahut, Richard E. ""Transform techniques for error control codes."" IBM Journal of Research and development 23.3 (1979): 299-315. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.600&rep=rep1&type=pdf and also a MatLab implementation here: http://www.mathworks.com/matlabcentral/fileexchange/23567-reed-solomon-errors-and-erasures-decoder/content//RS_E_E_DEC.m
        erasures_loc = Polynomial([GF2int(1)]) # just to init because we will multiply, so it must be 1 so that the multiplication starts correctly without nulling any term
        # erasures_loc is very simple to compute: erasures_loc = prod(1 - x*alpha[j]**i) for i in erasures_pos and where alpha is the alpha chosen to evaluate polynomials (here in this library it's gf(3)). To generate c*x where c is a constant, we simply generate a Polynomial([c, 0]) where 0 is the constant and c is positionned to be the coefficient for x^1. See https://en.wikipedia.org/wiki/Forney_algorithm#Erasures
        for i in erasures_pos:
            erasures_loc = erasures_loc * (Polynomial([GF2int(1)]) - Polynomial([GF2int(self.generator)**i, 0]))
        return erasures_loc","Compute the erasures locator polynomial from the erasures positions (the positions must be relative to the x coefficient, eg: ""hello worldxxxxxxxxx"" is tampered to ""h_ll_ worldxxxxxxxxx"" with xxxxxxxxx being the ecc of length n-k=9, here the string positions are [1, 4], but the coefficients are reversed since the ecc characters are placed as the first coefficients of the polynomial, thus the coefficients of the erased characters are n-1 - [1, 4] = [18, 15] = erasures_loc to be specified as an argument."
"def findall(self, string, pos=0, endpos=None, extra_types=None, evaluate_result=True):
        '''Search ""string"" for all occurrences of ""format"".

        Optionally start the search at ""pos"" character index and limit the
        search to a maximum index of endpos - equivalent to
        search(string[:endpos]).

        Returns an iterator that holds Result or Match instances for each format match
        found.
        '''
        if endpos is None:
            endpos = len(string)
        return ResultIterator(self, string, pos, endpos, evaluate_result=evaluate_result)","Search ""string"" for all occurrences of ""format"".

        Optionally start the search at ""pos"" character index and limit the
        search to a maximum index of endpos - equivalent to
        search(string[:endpos]).

        Returns an iterator that holds Result or Match instances for each format match
        found."
"def refresh_urls_and_dirs(self, urls, dirs):
        """"""
         http://developer.qiniu.com/article/fusion/api/refresh.html

        Args:
           urls: 
           dirs: 

        Returns:
           dictResponseInfo
            examples/cdn_manager.py
       """"""
        req = {}
        if urls is not None and len(urls) > 0:
            req.update({""urls"": urls})
        if dirs is not None and len(dirs) > 0:
            req.update({""dirs"": dirs})

        body = json.dumps(req)
        url = '{0}/v2/tune/refresh'.format(self.server)
        return self.__post(url, body)"," http://developer.qiniu.com/article/fusion/api/refresh.html

        Args:
           urls: 
           dirs: 

        Returns:
           dictResponseInfo
            examples/cdn_manager.py"
"def run(self, argv):
        """"""
        Parses the inputted options and executes the method.
        
        :param      argv | [<str>, ..]
        """"""
        (opts, args) = self.parser().parse_args(argv)
        func_args = args[args.index(self.__name__) + 1:]
        func_kwds = opts.__dict__

        return self.__call__(*func_args, **func_kwds)","Parses the inputted options and executes the method.
        
        :param      argv | [<str>, ..]"
"def name(self):
        """"""
        Returns the physical volume device path.
        """"""
        self.open()
        name = lvm_pv_get_name(self.handle)
        self.close()
        return name",Returns the physical volume device path.
"def get_block_operator(self):
        """"""Determine the immediate parent boolean operator for a filter""""""
        # Top level operator is `and`
        block_stack = ['and']
        for f in self.manager.iter_filters(block_end=True):
            if f is None:
                block_stack.pop()
                continue
            if f.type in ('and', 'or', 'not'):
                block_stack.append(f.type)
            if f == self:
                break
        return block_stack[-1]",Determine the immediate parent boolean operator for a filter
"async def AddUser(self, users):
        '''
        users : typing.Sequence[~AddUser]
        Returns -> typing.Sequence[~AddUserResult]
        '''
        # map input types to rpc msg
        _params = dict()
        msg = dict(type='UserManager',
                   request='AddUser',
                   version=2,
                   params=_params)
        _params['users'] = users
        reply = await self.rpc(msg)
        return reply","users : typing.Sequence[~AddUser]
        Returns -> typing.Sequence[~AddUserResult]"
"def _set_key_algorithm(self, v, load=False):
    """"""
    Setter method for key_algorithm, mapped from YANG variable /keychain/key/key_algorithm (key_algo)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_key_algorithm is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_key_algorithm() directly.
    """"""
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=RestrictedClassType(base_type=unicode,                                     restriction_type=""dict_key"",                                     restriction_arg={u'HMAC-SHA-256': {'value': 1}, u'HMAC-SHA-512': {'value': 3}, u'HMAC-SHA-1': {'value': 0}, u'HMAC-SHA-384': {'value': 2}},), is_leaf=True, yang_name=""key-algorithm"", rest_name=""key-algorithm"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-full-command': None, u'info': u'specify HMAC algorithm for the key', u'cli-full-no': None}}, namespace='urn:brocade.com:mgmt:brocade-keychain', defining_module='brocade-keychain', yang_type='key_algo', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': """"""key_algorithm must be of a type compatible with key_algo"""""",
          'defined-type': ""brocade-keychain:key_algo"",
          'generated-type': """"""YANGDynClass(base=RestrictedClassType(base_type=unicode,                                     restriction_type=""dict_key"",                                     restriction_arg={u'HMAC-SHA-256': {'value': 1}, u'HMAC-SHA-512': {'value': 3}, u'HMAC-SHA-1': {'value': 0}, u'HMAC-SHA-384': {'value': 2}},), is_leaf=True, yang_name=""key-algorithm"", rest_name=""key-algorithm"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-full-command': None, u'info': u'specify HMAC algorithm for the key', u'cli-full-no': None}}, namespace='urn:brocade.com:mgmt:brocade-keychain', defining_module='brocade-keychain', yang_type='key_algo', is_config=True)"""""",
        })

    self.__key_algorithm = t
    if hasattr(self, '_set'):
      self._set()","Setter method for key_algorithm, mapped from YANG variable /keychain/key/key_algorithm (key_algo)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_key_algorithm is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_key_algorithm() directly."
"def profile(n):
    """"""
    Usage:
    @profile(""my_func"")
    def my_func(): code
    """"""
    def decorator_with_name(func):
        def func_wrapper(*args, **kwargs):
            with ProfileKV(n):
                return func(*args, **kwargs)
        return func_wrapper
    return decorator_with_name","Usage:
    @profile(""my_func"")
    def my_func(): code"
"def get_device(self, rid):
        """"""
            Retrieve the device object for a given RID.

            http://docs.exosite.com/portals/#get-device
        """"""
        headers = {
                'User-Agent': self.user_agent(),
                'Content-Type': self.content_type()
        }
        headers.update(self.headers())

        url = self.portals_url()+'/devices/'+rid
        # print(""URL: {0}"".format(url))

        r = requests.get(   url,
                            headers=headers,
                            auth=self.auth())
    
        if HTTP_STATUS.OK == r.status_code:
            # fix the 'meta' to be dictionary instead of string
            device_obj = r.json()
            # device_obj['info']['description']['meta'] = \
                    # json.loads(device_obj['info']['description']['meta'])
            return device_obj
        else:
            print(""get_device: Something went wrong: <{0}>: {1}"".format(
                        r.status_code, r.reason))
            r.raise_for_status()","Retrieve the device object for a given RID.

            http://docs.exosite.com/portals/#get-device"
"def _check_min_density(self, min_density):
        """"""Validator to ensure proper usage.""""""

        if min_density is None:
            self._min_density = -np.Inf
        elif (isinstance(min_density, float) and (0.0 <= min_density < 1.0)):
            self._min_density = min_density
        else:
            raise ValueError('min_density must be float and be >=0.0 and < 1.0')",Validator to ensure proper usage.
"def exceptionToString(e):
    """"""when you ""except Exception as e"", give me the e and I'll give you a string.""""""
    exc_type, exc_obj, exc_tb = sys.exc_info()
    s=""EXCEPTION THROWN UNEXPECTEDLY""
    s+=""  FILE: %s\n""%os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
    s+=""  LINE: %s\n""%exc_tb.tb_lineno
    s+=""  TYPE: %s\n""%exc_type
    return s","when you ""except Exception as e"", give me the e and I'll give you a string."
"def _get_translate():
    """"""
    Get a dictionary which translates from a neural network output to
    semantics.
    """"""
    translate = {}
    model_path = pkg_resources.resource_filename('hwrt', 'misc/')
    translation_csv = os.path.join(model_path, 'latex2writemathindex.csv')
    arguments = {'newline': '', 'encoding': 'utf8'}
    with open(translation_csv, 'rt', **arguments) as csvfile:
        contents = csvfile.read()
    lines = contents.split(""\n"")
    for csvrow in lines:
        csvrow = csvrow.split(',')
        if len(csvrow) == 1:
            writemathid = csvrow[0]
            latex = """"
        else:
            writemathid, latex = csvrow[0], csvrow[1:]
            latex = ','.join(latex)
        translate[latex] = writemathid
    return translate","Get a dictionary which translates from a neural network output to
    semantics."
"def calcMassFromMz(mz, charge):
    """"""Calculate the mass of a peptide from its mz and charge.

    :param mz: float, mass to charge ratio (Dalton / charge)
    :param charge: int, charge state

    :returns: non protonated mass (charge = 0)
    """"""
    mass = (mz - maspy.constants.atomicMassProton) * charge
    return mass","Calculate the mass of a peptide from its mz and charge.

    :param mz: float, mass to charge ratio (Dalton / charge)
    :param charge: int, charge state

    :returns: non protonated mass (charge = 0)"
"def get_document(self, document_id):
        """"""
        :param document_id: Document unique identifier.
        :returns: a dictionary containing the document content and
                  any associated metadata.
        """"""
        key = 'doc.%s.%s' % (self.name, decode(document_id))
        return decode_dict(self.db.hgetall(key))",":param document_id: Document unique identifier.
        :returns: a dictionary containing the document content and
                  any associated metadata."
"def destroy(self, stream=False):
        """"""
        Run a 'terraform destroy'

        :param stream: whether or not to stream TF output in realtime
        :type stream: bool
        """"""
        self._setup_tf(stream=stream)
        args = ['-refresh=true', '-force', '.']
        logger.warning('Running terraform destroy: %s', ' '.join(args))
        out = self._run_tf('destroy', cmd_args=args, stream=stream)
        if stream:
            logger.warning('Terraform destroy finished successfully.')
        else:
            logger.warning(""Terraform destroy finished successfully:\n%s"", out)","Run a 'terraform destroy'

        :param stream: whether or not to stream TF output in realtime
        :type stream: bool"
"def do_dictsort(value, case_sensitive=False, by='key', reverse=False):
    """"""Sort a dict and yield (key, value) pairs. Because python dicts are
    unsorted you may want to use this function to order them by either
    key or value:

    .. sourcecode:: jinja

        {% for item in mydict|dictsort %}
            sort the dict by key, case insensitive

        {% for item in mydict|dictsort(reverse=true) %}
            sort the dict by key, case insensitive, reverse order

        {% for item in mydict|dictsort(true) %}
            sort the dict by key, case sensitive

        {% for item in mydict|dictsort(false, 'value') %}
            sort the dict by value, case insensitive
    """"""
    if by == 'key':
        pos = 0
    elif by == 'value':
        pos = 1
    else:
        raise FilterArgumentError(
            'You can only sort by either ""key"" or ""value""'
        )

    def sort_func(item):
        value = item[pos]

        if not case_sensitive:
            value = ignore_case(value)

        return value

    return sorted(value.items(), key=sort_func, reverse=reverse)","Sort a dict and yield (key, value) pairs. Because python dicts are
    unsorted you may want to use this function to order them by either
    key or value:

    .. sourcecode:: jinja

        {% for item in mydict|dictsort %}
            sort the dict by key, case insensitive

        {% for item in mydict|dictsort(reverse=true) %}
            sort the dict by key, case insensitive, reverse order

        {% for item in mydict|dictsort(true) %}
            sort the dict by key, case sensitive

        {% for item in mydict|dictsort(false, 'value') %}
            sort the dict by value, case insensitive"
"def _sub_period_array(self, other):
        """"""
        Subtract a Period Array/Index from self.  This is only valid if self
        is itself a Period Array/Index, raises otherwise.  Both objects must
        have the same frequency.

        Parameters
        ----------
        other : PeriodIndex or PeriodArray

        Returns
        -------
        result : np.ndarray[object]
            Array of DateOffset objects; nulls represented by NaT.
        """"""
        if not is_period_dtype(self):
            raise TypeError(""cannot subtract {dtype}-dtype from {cls}""
                            .format(dtype=other.dtype,
                                    cls=type(self).__name__))

        if len(self) != len(other):
            raise ValueError(""cannot subtract arrays/indices of ""
                             ""unequal length"")
        if self.freq != other.freq:
            msg = DIFFERENT_FREQ.format(cls=type(self).__name__,
                                        own_freq=self.freqstr,
                                        other_freq=other.freqstr)
            raise IncompatibleFrequency(msg)

        new_values = checked_add_with_arr(self.asi8, -other.asi8,
                                          arr_mask=self._isnan,
                                          b_mask=other._isnan)

        new_values = np.array([self.freq.base * x for x in new_values])
        if self._hasnans or other._hasnans:
            mask = (self._isnan) | (other._isnan)
            new_values[mask] = NaT
        return new_values","Subtract a Period Array/Index from self.  This is only valid if self
        is itself a Period Array/Index, raises otherwise.  Both objects must
        have the same frequency.

        Parameters
        ----------
        other : PeriodIndex or PeriodArray

        Returns
        -------
        result : np.ndarray[object]
            Array of DateOffset objects; nulls represented by NaT."
"def verify_message(self, expected_response, break_in_fail=True):
        """"""
        Verifies that expected_response is found in self.lines.

        :param expected_response: response or responses to look for. Must be list or str.
        :param break_in_fail: If set to True,
        re-raises exceptions caught or if message was not found
        :return: True or False
        :raises: LookupError if message was not found and break_in_fail was True. Other exceptions
        might also be raised through searcher.verify_message.
        """"""
        ok = True
        try:
            ok = verify_message(self.lines, expected_response)
        except (TypeError, LookupError) as inst:
            ok = False
            if break_in_fail:
                raise inst
        if ok is False and break_in_fail:
            raise LookupError(""Unexpected message found"")
        return ok","Verifies that expected_response is found in self.lines.

        :param expected_response: response or responses to look for. Must be list or str.
        :param break_in_fail: If set to True,
        re-raises exceptions caught or if message was not found
        :return: True or False
        :raises: LookupError if message was not found and break_in_fail was True. Other exceptions
        might also be raised through searcher.verify_message."
"def assignment_propagation(node):
  """"""Perform assignment propagation.

  Assignment propagation is not a compiler optimization as much as a
  readability optimization. If a variable name is used only once, it gets
  renamed when possible e.g. `y = x; z = y` will become `z = x`.

  Args:
    node: The AST to optimize.

  Returns:
    The optimized AST.
  """"""
  n_reads = read_counts(node)

  to_remove = []
  for succ in gast.walk(node):
    # We found an assignment of the form a = b
    # - Left-hand side is a Name, right-hand side is a Name.
    if (isinstance(succ, gast.Assign) and isinstance(succ.value, gast.Name) and
        len(succ.targets) == 1 and isinstance(succ.targets[0], gast.Name)):
      rhs_name = succ.value.id
      # We now find all the places that b was defined
      rhs_defs = [def_[1] for def_ in anno.getanno(succ, 'definitions_in')
                  if def_[0] == rhs_name]
      # If b was defined in only one place (not an argument), and wasn't used
      # anywhere else but in a == b, and was defined as b = x, then we can fold
      # the statements
      if (len(rhs_defs) == 1 and isinstance(rhs_defs[0], gast.Assign) and
          n_reads[rhs_defs[0]] == 1 and
          isinstance(rhs_defs[0].value, gast.Name) and
          isinstance(rhs_defs[0].targets[0], gast.Name)):
        # Mark rhs_def for deletion
        to_remove.append(rhs_defs[0])
        # Propagate the definition
        succ.value = rhs_defs[0].value

  # Remove the definitions we folded
  transformers.Remove(to_remove).visit(node)
  anno.clearanno(node)
  return node","Perform assignment propagation.

  Assignment propagation is not a compiler optimization as much as a
  readability optimization. If a variable name is used only once, it gets
  renamed when possible e.g. `y = x; z = y` will become `z = x`.

  Args:
    node: The AST to optimize.

  Returns:
    The optimized AST."
"def seeds(args):
    """"""
    %prog seeds [pngfile|jpgfile]

    Extract seed metrics from [pngfile|jpgfile]. Use --rows and --cols to crop image.
    """"""
    p = OptionParser(seeds.__doc__)
    p.set_outfile()
    opts, args, iopts = add_seeds_options(p, args)

    if len(args) != 1:
        sys.exit(not p.print_help())

    pngfile, = args
    pf = opts.prefix or op.basename(pngfile).rsplit(""."", 1)[0]
    sigma, kernel = opts.sigma, opts.kernel
    rows, cols = opts.rows, opts.cols
    labelrows, labelcols = opts.labelrows, opts.labelcols
    ff = opts.filter
    calib = opts.calibrate
    outdir = opts.outdir
    if outdir != '.':
        mkdir(outdir)
    if calib:
        calib = json.load(must_open(calib))
        pixel_cm_ratio, tr = calib[""PixelCMratio""], calib[""RGBtransform""]
        tr = np.array(tr)

    resizefile, mainfile, labelfile, exif = \
                      convert_image(pngfile, pf, outdir=outdir,
                                    rotate=opts.rotate,
                                    rows=rows, cols=cols,
                                    labelrows=labelrows, labelcols=labelcols)

    oimg = load_image(resizefile)
    img = load_image(mainfile)

    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, nrows=1,
                                             figsize=(iopts.w, iopts.h))

    # Edge detection
    img_gray = rgb2gray(img)
    logging.debug(""Running {0} edge detection ..."".format(ff))
    if ff == ""canny"":
        edges = canny(img_gray, sigma=opts.sigma)
    elif ff == ""roberts"":
        edges = roberts(img_gray)
    elif ff == ""sobel"":
        edges = sobel(img_gray)
    edges = clear_border(edges, buffer_size=opts.border)
    selem = disk(kernel)
    closed = closing(edges, selem) if kernel else edges
    filled = binary_fill_holes(closed)

    # Watershed algorithm
    if opts.watershed:
        distance = distance_transform_edt(filled)
        local_maxi = peak_local_max(distance, threshold_rel=.05, indices=False)
        coordinates = peak_local_max(distance, threshold_rel=.05)
        markers, nmarkers = label(local_maxi, return_num=True)
        logging.debug(""Identified {0} watershed markers"".format(nmarkers))
        labels = watershed(closed, markers, mask=filled)
    else:
        labels = label(filled)

    # Object size filtering
    w, h = img_gray.shape
    canvas_size = w * h
    min_size = int(round(canvas_size * opts.minsize / 100))
    max_size = int(round(canvas_size * opts.maxsize / 100))
    logging.debug(""Find objects with pixels between {0} ({1}%) and {2} ({3}%)""\
                    .format(min_size, opts.minsize, max_size, opts.maxsize))

    # Plotting
    ax1.set_title('Original picture')
    ax1.imshow(oimg)

    params = ""{0}, $\sigma$={1}, $k$={2}"".format(ff, sigma, kernel)
    if opts.watershed:
        params += "", watershed""
    ax2.set_title('Edge detection\n({0})'.format(params))
    closed = gray2rgb(closed)
    ax2_img = labels
    if opts.edges:
        ax2_img = closed
    elif opts.watershed:
        ax2.plot(coordinates[:, 1], coordinates[:, 0], 'g.')
    ax2.imshow(ax2_img, cmap=iopts.cmap)

    ax3.set_title('Object detection')
    ax3.imshow(img)

    filename = op.basename(pngfile)
    if labelfile:
        accession = extract_label(labelfile)
    else:
        accession = pf

    # Calculate region properties
    rp = regionprops(labels)
    rp = [x for x in rp if min_size <= x.area <= max_size]
    nb_labels = len(rp)
    logging.debug(""A total of {0} objects identified."".format(nb_labels))
    objects = []
    for i, props in enumerate(rp):
        i += 1
        if i > opts.count:
            break

        y0, x0 = props.centroid
        orientation = props.orientation
        major, minor = props.major_axis_length, props.minor_axis_length
        major_dx = cos(orientation) * major / 2
        major_dy = sin(orientation) * major / 2
        minor_dx = sin(orientation) * minor / 2
        minor_dy = cos(orientation) * minor / 2
        ax2.plot((x0 - major_dx, x0 + major_dx),
                 (y0 + major_dy, y0 - major_dy), 'r-')
        ax2.plot((x0 - minor_dx, x0 + minor_dx),
                 (y0 - minor_dy, y0 + minor_dy), 'r-')

        npixels = int(props.area)
        # Sample the center of the blob for color
        d = min(int(round(minor / 2 * .35)) + 1, 50)
        x0d, y0d = int(round(x0)), int(round(y0))
        square = img[(y0d - d):(y0d + d), (x0d - d):(x0d + d)]
        pixels = []
        for row in square:
            pixels.extend(row)
        logging.debug(""Seed #{0}: {1} pixels ({2} sampled) - {3:.2f}%"".\
                        format(i, npixels, len(pixels), 100. * npixels / canvas_size))

        rgb = pixel_stats(pixels)
        objects.append(Seed(filename, accession, i, rgb, props, exif))
        minr, minc, maxr, maxc = props.bbox
        rect = Rectangle((minc, minr), maxc - minc, maxr - minr,
                                  fill=False, ec='w', lw=1)
        ax3.add_patch(rect)
        mc, mr = (minc + maxc) / 2, (minr + maxr) / 2
        ax3.text(mc, mr, ""{0}"".format(i), color='w',
                    ha=""center"", va=""center"", size=6)

    for ax in (ax2, ax3):
        ax.set_xlim(0, h)
        ax.set_ylim(w, 0)

    # Output identified seed stats
    ax4.text(.1, .92, ""File: {0}"".format(latex(filename)), color='g')
    ax4.text(.1, .86, ""Label: {0}"".format(latex(accession)), color='m')
    yy = .8
    fw = must_open(opts.outfile, ""w"")
    if not opts.noheader:
        print(Seed.header(calibrate=calib), file=fw)
    for o in objects:
        if calib:
            o.calibrate(pixel_cm_ratio, tr)
        print(o, file=fw)
        i = o.seedno
        if i > 7:
            continue
        ax4.text(.01, yy, str(i), va=""center"", bbox=dict(fc='none', ec='k'))
        ax4.text(.1, yy, o.pixeltag, va=""center"")
        yy -= .04
        ax4.add_patch(Rectangle((.1, yy - .025), .12, .05, lw=0,
                      fc=rgb_to_hex(o.rgb)))
        ax4.text(.27, yy, o.hashtag, va=""center"")
        yy -= .06
    ax4.text(.1 , yy, ""(A total of {0} objects displayed)"".format(nb_labels),
             color=""darkslategrey"")
    normalize_axes(ax4)

    for ax in (ax1, ax2, ax3):
        xticklabels = [int(x) for x in ax.get_xticks()]
        yticklabels = [int(x) for x in ax.get_yticks()]
        ax.set_xticklabels(xticklabels, family='Helvetica', size=8)
        ax.set_yticklabels(yticklabels, family='Helvetica', size=8)

    image_name = op.join(outdir, pf + ""."" + iopts.format)
    savefig(image_name, dpi=iopts.dpi, iopts=iopts)
    return objects","%prog seeds [pngfile|jpgfile]

    Extract seed metrics from [pngfile|jpgfile]. Use --rows and --cols to crop image."
"def expect_file_to_have_valid_table_header(self, regex, skip=None,
                                               result_format=None,
                                               include_config=False,
                                               catch_exceptions=None, meta=None):
        """"""
        Checks to see if a file has a line with unique delimited values,
        such a line may be used as a table header.

        Keyword Args:
            skip (nonnegative integer): \
                Integer specifying the first lines in the file the method
                should skip before assessing expectations

            regex (string):
                A string that can be compiled as valid regular expression.
                Used to specify the elements of the table header (the column headers)

            result_format (str or None):
                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.
                For more detail, see :ref:`result_format <result_format>`.

            include_config (boolean): \
                If True, then include the expectation config as part of the result object. \
                For more detail, see :ref:`include_config`.

            catch_exceptions (boolean or None): \
                If True, then catch exceptions and include them as part of the result object. \
                For more detail, see :ref:`catch_exceptions`.

            meta (dict or None): \
                A JSON-serializable dictionary (nesting allowed) that will be
                included in the output without modification. For more detail,
                see :ref:`meta`.

        Returns:
            A JSON-serializable expectation result object.

        Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and
        :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.
        """"""

        try:
            comp_regex = re.compile(regex)
        except:
            raise ValueError(""Must enter valid regular expression for regex"")

        success = False

        try:
            with open(self._path, 'r') as f:
                lines = f.readlines() #Read in file lines

        except IOError:
            raise

            #Skip k initial lines designated by the user
        if skip is not None and skip <= len(lines):
            try:
                assert float(skip).is_integer()
                assert float(skip) >= 0
            except:
                raise ValueError(""skip must be a positive integer"")

            lines = lines[skip:]

        header_line = lines[0].strip()
        header_names = comp_regex.split(header_line)
        if len(set(header_names)) == len(header_names):
            success = True

        return {""success"":success}","Checks to see if a file has a line with unique delimited values,
        such a line may be used as a table header.

        Keyword Args:
            skip (nonnegative integer): \
                Integer specifying the first lines in the file the method
                should skip before assessing expectations

            regex (string):
                A string that can be compiled as valid regular expression.
                Used to specify the elements of the table header (the column headers)

            result_format (str or None):
                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.
                For more detail, see :ref:`result_format <result_format>`.

            include_config (boolean): \
                If True, then include the expectation config as part of the result object. \
                For more detail, see :ref:`include_config`.

            catch_exceptions (boolean or None): \
                If True, then catch exceptions and include them as part of the result object. \
                For more detail, see :ref:`catch_exceptions`.

            meta (dict or None): \
                A JSON-serializable dictionary (nesting allowed) that will be
                included in the output without modification. For more detail,
                see :ref:`meta`.

        Returns:
            A JSON-serializable expectation result object.

        Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and
        :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`."
"def run(self, scenario, learn=True):
        """"""Run the algorithm, utilizing the classifier set to choose the
        most appropriate action for each situation produced by the
        scenario. If learn is True, improve the situation/action mapping to
        maximize reward. Otherwise, ignore any reward received.

        Usage:
            model.run(scenario, learn=True)

        Arguments:
            scenario: A Scenario instance which this classifier set is to
                interact with.
            learn: A bool indicating whether the classifier set should
                attempt to optimize its performance based on reward
                received for each action, as opposed to simply using what
                it has already learned from previous runs and ignoring
                reward received; default is True.
        Return: None
        """"""

        assert isinstance(scenario, scenarios.Scenario)

        previous_match_set = None

        # Repeat until the scenario has run its course.
        while scenario.more():
            # Gather information about the current state of the
            # environment.
            situation = scenario.sense()

            # Determine which rules match the current situation.
            match_set = self.match(situation)

            # Select the best action for the current situation (or a random
            # one, if we are on an exploration step).
            match_set.select_action()

            # Perform the selected action
            # and find out what the received reward was.
            reward = scenario.execute(match_set.selected_action)

            # If the scenario is dynamic, don't immediately apply the
            # reward; instead, wait until the next iteration and factor in
            # not only the reward that was received on the previous step,
            # but the (discounted) reward that is expected going forward
            # given the resulting situation observed after the action was
            # taken. This is a classic feature of temporal difference (TD)
            # algorithms, which acts to stitch together a general picture
            # of the future expected reward without actually waiting the
            # full duration to find out what it will be.
            if learn:
                # Ensure we are not trying to learn in a non-learning
                # scenario.
                assert reward is not None

                if scenario.is_dynamic:
                    if previous_match_set is not None:
                        match_set.pay(previous_match_set)
                        previous_match_set.apply_payoff()
                    match_set.payoff = reward

                    # Remember the current reward and match set for the
                    # next iteration.
                    previous_match_set = match_set
                else:
                    match_set.payoff = reward
                    match_set.apply_payoff()

        # This serves to tie off the final stitch. The last action taken
        # gets only the immediate reward; there is no future reward
        # expected.
        if learn and previous_match_set is not None:
            previous_match_set.apply_payoff()","Run the algorithm, utilizing the classifier set to choose the
        most appropriate action for each situation produced by the
        scenario. If learn is True, improve the situation/action mapping to
        maximize reward. Otherwise, ignore any reward received.

        Usage:
            model.run(scenario, learn=True)

        Arguments:
            scenario: A Scenario instance which this classifier set is to
                interact with.
            learn: A bool indicating whether the classifier set should
                attempt to optimize its performance based on reward
                received for each action, as opposed to simply using what
                it has already learned from previous runs and ignoring
                reward received; default is True.
        Return: None"
"def init_prov_graph(self):
        """"""
        Initialize PROV graph with all we know at the start of the recording
        """"""

        try:
            # Use git2prov to get prov on the repo
            repo_prov = check_output(
                ['node_modules/git2prov/bin/git2prov', 'https://github.com/{}/{}/'.format(self.user, self.repo),
                 'PROV-O']).decode(""utf-8"")
            repo_prov = repo_prov[repo_prov.find('@'):]
            # glogger.debug('Git2PROV output: {}'.format(repo_prov))
            glogger.debug('Ingesting Git2PROV output into RDF graph')
            with open('temp.prov.ttl', 'w') as temp_prov:
                temp_prov.write(repo_prov)

            self.prov_g.parse('temp.prov.ttl', format='turtle')
        except Exception as e:
            glogger.error(e)
            glogger.error(""Couldn't parse Git2PROV graph, continuing without repo PROV"")
            pass

        self.prov_g.add((self.agent, RDF.type, self.prov.Agent))
        self.prov_g.add((self.entity_d, RDF.type, self.prov.Entity))
        self.prov_g.add((self.activity, RDF.type, self.prov.Activity))

        # entity_d
        self.prov_g.add((self.entity_d, self.prov.wasGeneratedBy, self.activity))
        self.prov_g.add((self.entity_d, self.prov.wasAttributedTo, self.agent))
        # later: entity_d genereated at time (when we know the end time)

        # activity
        self.prov_g.add((self.activity, self.prov.wasAssociatedWith, self.agent))
        self.prov_g.add((self.activity, self.prov.startedAtTime, Literal(datetime.now())))",Initialize PROV graph with all we know at the start of the recording
"def _format_pipe(fd):  
    # Check if the file descriptor is subprocess.PIPE  
    if fd == subprocess.PIPE:  
        # Return a string representing a pipe  
        return '<pipe>'  
    
    # Check if the file descriptor is subprocess.STDOUT  
    elif fd == subprocess.STDOUT:  
        # Return a string indicating it points to stdout  
        return '<stdout>'  
    
    else:  
        # For all other cases, return the string representation of the descriptor
        return repr(fd)  
","The _format_pipe function is a utility method used to format file descriptors for display. It primarily works with special values from the subprocess module, such as subprocess.PIPE and subprocess.STDOUT, and returns human-readable string representations for these. If the file descriptor does not match these special cases, the function returns the string representation of the descriptor itself using repr(). This function is useful for debugging or logging purposes, making it easier to interpret the status or configuration of file descriptors in subprocess management."
"def _set_reuseport(sock):  
    # Check if the socket module has the SO_REUSEPORT attribute
    if not hasattr(socket, 'SO_REUSEPORT'):  
        # Raise an error if the system does not support reuse_port
        raise ValueError('reuse_port not supported by socket module')  
    
    else:  
        try:  
            # Attempt to set the SO_REUSEPORT option on the socket
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)  
        except OSError:  
            # Raise an error if SO_REUSEPORT is defined but not implemented on the platform
            raise ValueError('reuse_port not supported by socket module, '  
                             'SO_REUSEPORT defined but not implemented.')  
","The _set_reuseport function enables the SO_REUSEPORT socket option on a given socket object. This option allows multiple sockets on the same host to bind to the same port, which is useful in scenarios such as load balancing or high-performance server applications. The function first checks if the SO_REUSEPORT attribute is available in the socket module. If not, it raises an error. If the attribute exists but fails during the setsockopt call, the function raises an error indicating that the platform defines SO_REUSEPORT but doesn't support it in practice."
"def _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):  
    # Check if the system supports inet_pton, which parses IP addresses.
    if not hasattr(socket, 'inet_pton'):  
        return  # Exit if IP parsing is not supported.
    
    # Skip if the protocol is not TCP, UDP, or unspecified, or if the host is None.
    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or host is None:  
        return None  

    # Map socket type to protocol.
    if type == socket.SOCK_STREAM:  
        proto = socket.IPPROTO_TCP  
    elif type == socket.SOCK_DGRAM:  
        proto = socket.IPPROTO_UDP  
    else:  
        return None  # Exit for unsupported socket types.

    # Normalize the port value.
    if port is None:  
        port = 0  # Default to port 0 if none is provided.
    elif isinstance(port, bytes) and port == b'':  
        port = 0  
    elif isinstance(port, str) and port == '':  
        port = 0  
    else:  
        try:  
            # Convert port to an integer if possible.
            port = int(port)  
        except (TypeError, ValueError):  
            return None  # Exit if the port is a service name.

    # Determine address families to check based on input and system support.
    if family == socket.AF_UNSPEC:  
        afs = [socket.AF_INET]  # Default to IPv4.
        if _HAS_IPv6:  # Include IPv6 if available.
            afs.append(socket.AF_INET6)  
    else:  
        afs = [family]  

    # Decode host if it's in bytes and check for zone index in IPv6.
    if isinstance(host, bytes):  
        host = host.decode('idna')  
    if '%' in host:  
        # Zone indices like '::1%lo0' are not supported in inet_pton.
        return None  

    # Attempt to validate the IP address for each address family.
    for af in afs:  
        try:  
            socket.inet_pton(af, host)  # Check if the host is a valid IP.
            if _HAS_IPv6 and af == socket.AF_INET6:  
                # Return formatted IPv6 address info.
                return af, type, proto, '', (host, port, flowinfo, scopeid)  
            else:  
                # Return formatted IPv4 address info.
                return af, type, proto, '', (host, port)  
        except OSError:  
            pass  # Ignore errors and try the next family.

    # Return None if the host is not a valid IP address.
    return None  
","The _ipaddr_info function attempts to optimize network operations by skipping DNS resolution if a given host is already an IP address. It checks the validity and type of the provided host and port and returns address information formatted for socket usage. This function is helpful in scenarios where applications handle name resolution independently, improving performance by avoiding redundant system calls like getaddrinfo.

The function also ensures compatibility with both IPv4 and IPv6 based on the system's capabilities. If the host is not a valid IP address, it defaults to returning None, indicating that resolution via standard means might be required"
"def _interleave_addrinfos(addrinfos, first_address_family_count=1):  
    """"""Interleave list of addrinfo tuples by family.""""""  

    # Group addresses by their family into an ordered dictionary.
    addrinfos_by_family = collections.OrderedDict()  
    for addr in addrinfos:  
        family = addr[0]  # Extract the address family from the tuple.
        if family not in addrinfos_by_family:  
            # Initialize a list for this family if not already present.
            addrinfos_by_family[family] = []  
        # Append the current address info tuple to the corresponding family.
        addrinfos_by_family[family].append(addr)  

    # Convert grouped address info lists to a list of lists.
    addrinfos_lists = list(addrinfos_by_family.values())  

    # Initialize the reordered list for the result.
    reordered = []  
    
    if first_address_family_count > 1:  
        # Prioritize the specified number of addresses from the first family.
        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])  
        # Remove the prioritized addresses from the first family list.
        del addrinfos_lists[0][:first_address_family_count - 1]  
    
    # Interleave the remaining address info tuples from all families.
    reordered.extend(  
        a for a in itertools.chain.from_iterable(  
            itertools.zip_longest(*addrinfos_lists)  # Combine families alternately.
        ) if a is not None  # Ignore placeholders for missing values.
    )  

    # Return the reordered list of address infos.
    return reordered  
","The _interleave_addrinfos function takes a list of addrinfo tuples (typically from socket.getaddrinfo) and rearranges them to interleave the address families (e.g., IPv4, IPv6) for better distribution. The function ensures that address records from different families are returned in an alternating order, which can help prioritize certain address families without starving others. The first_address_family_count parameter allows specifying how many initial addresses from the first family should be prioritized before interleaving starts"
"def _run_until_complete_cb(fut):  
    # Check if the Future has been cancelled.
    if not fut.cancelled():  
        # Retrieve any exception raised during the Future's execution.
        exc = fut.exception()  
        if isinstance(exc, (SystemExit, KeyboardInterrupt)):  
            # If the exception is SystemExit or KeyboardInterrupt:
            # These exceptions indicate normal termination, so do not stop the loop.
            # Issue #22429: run_forever() has already finished.
            return  
    
    # Stop the event loop associated with the Future, regardless of the outcome.
    futures._get_loop(fut).stop()  
","The _run_until_complete_cb function serves as a callback that is executed upon the completion of a Future object in an asynchronous event loop. If the Future is not cancelled, it checks for exceptions. If the exception is a SystemExit or KeyboardInterrupt, it returns early without stopping the event loop, as these exceptions typically indicate that the application is terminating naturally. For other cases, it stops the event loop associated with the Future. This function ensures graceful handling of event loop termination based on the Future's outcome."
"def _check_ssl_socket(sock):  
    # Check if the ssl module is available and the socket is an SSL socket.
    if ssl is not None and isinstance(sock, ssl.SSLSocket):  
        # Raise a TypeError if the provided socket is an SSL socket.
        raise TypeError(""Socket cannot be of type SSLSocket"")  
","The _check_ssl_socket function verifies whether a given socket is an instance of ssl.SSLSocket. If it is, the function raises a TypeError, indicating that SSL sockets are not supported for the specific operation. This function is useful for enforcing constraints in applications where SSL connections are either unnecessary or could cause unexpected behavior."
"def __init__(self, transp):  
    # Check if the provided transport implements the _FlowControlMixin interface.
    if not isinstance(transp, transports._FlowControlMixin):  
        # Raise a TypeError if the transport is not an instance of _FlowControlMixin.
        raise TypeError(""transport should be _FlowControlMixin instance"")  
    
    # Store the provided transport for later use.
    self._transport = transp  
    
    # Retrieve and store the protocol associated with the transport.
    self._proto = transp.get_protocol()  
    
    # Check if the transport is currently reading and store the state.
    self._should_resume_reading = transp.is_reading()  
    
    # Check if the transport's protocol is paused for writing and store the state.
    self._should_resume_writing = transp._protocol_paused  
    
    # Pause reading on the transport to manage flow control.
    transp.pause_reading()  
    
    # Replace the transport's protocol with the current object.
    transp.set_protocol(self)  
    
    # If the transport's protocol is paused for writing, create a future for write readiness.
    if self._should_resume_writing:  
        self._write_ready_fut = self._transport._loop.create_future()  
    else:  
        # Otherwise, set the write-ready future to None.
        self._write_ready_fut = None  
","The __init__ method initializes an object that interacts with a transport implementing the _FlowControlMixin interface, which is part of Python's asynchronous I/O framework. It validates the transport type, stores references to its protocol and state, and configures the transport by pausing reading and replacing its protocol with the current object. The method also sets up a future for write readiness if the transport's protocol is paused. This setup helps manage flow control in asynchronous communication, ensuring proper handling of data reads and writes."
"def connection_lost(self, exc):  
    # Check if there is a pending write-ready future.
    if self._write_ready_fut is not None:  
        # Handle the case where the connection is closed without an error.
        # This typically doesn't happen unless the peer closes the connection gracefully.
        if exc is None:  
            # Set a ConnectionError exception on the write-ready future to indicate closure.
            self._write_ready_fut.set_exception(  
                ConnectionError(""Connection is closed by peer"")  
            )  
        else:  
            # If an exception caused the disconnection, propagate that exception.
            self._write_ready_fut.set_exception(exc)  
    
    # Delegate the connection lost event to the protocol's `connection_lost` method.
    self._proto.connection_lost(exc)  
","The connection_lost method is a callback triggered when a transport's connection is lost. It handles the disconnection event by updating the write readiness future (_write_ready_fut) to reflect the connection status. If the disconnection is clean but unexpected (no exception), it raises a ConnectionError. If an exception caused the disconnection, that exception is propagated. The method then delegates further handling of the event to the underlying protocol's connection_lost method. This function ensures that the protocol and transport layers are properly notified and updated during connection termination."
"def pause_writing(self):  
    # If there is already a pending write-ready future, do nothing.
    if self._write_ready_fut is not None:  
        return  
    
    # If no pending future exists, create a new one to signal that writing is paused.
    self._write_ready_fut = self._transport._loop.create_future()  
","The pause_writing method pauses the writing operations on the transport layer. If there is already a Future object (_write_ready_fut) indicating that writing should be resumed once the transport is ready, the method does nothing. If there isn't a pending write readiness future, it creates one, signaling that writing is paused and will resume when the future is completed. This is part of managing flow control in asynchronous communication, ensuring that writes are paused when necessary and can be resumed later."
"def resume_writing(self):  
    # If there is no pending write-ready future, do nothing.
    if self._write_ready_fut is None:  
        return  
    
    # Set the result of the write-ready future to False, signaling that writing can resume.
    self._write_ready_fut.set_result(False)  
    
    # Clear the write-ready future, indicating that writing is no longer paused.
    self._write_ready_fut = None  
","The resume_writing method resumes writing operations on the transport layer. If there is no pending write readiness future (_write_ready_fut), it simply returns without taking any action. If a future exists, it sets the result of the future to False, signaling that writing can resume. After that, the method clears the _write_ready_fut, indicating that the pause is lifted and writing is no longer blocked. This is part of the flow control mechanism, ensuring that writes are resumed once the necessary conditions are met."
"async def restore(self):  
    # Restore the transport's protocol to the original one stored in self._proto.
    self._transport.set_protocol(self._proto)  
    
    # If reading was previously paused, resume reading on the transport.
    if self._should_resume_reading:  
        self._transport.resume_reading()  
    
    # If there is a pending write-ready future, cancel it as it is no longer needed.
    # The cancellation has no effect because the protocol is switched back and no code
    # should be waiting for the future anymore.
    if self._write_ready_fut is not None:  
        self._write_ready_fut.cancel()  
    
    # If writing was previously paused, resume writing on the protocol.
    if self._should_resume_writing:  
        self._proto.resume_writing()  
","The restore method is an asynchronous function that restores the transport's protocol and flow control settings. It first sets the protocol back to its original state, then checks if reading or writing should be resumed based on the saved states. If reading should be resumed, it calls the resume_reading method on the transport. If there is a pending write readiness future (_write_ready_fut), it cancels the future, ensuring that no code is waiting for it anymore. Finally, if writing should be resumed, it invokes the resume_writing method on the protocol. This method is used to undo the temporary flow control changes and restore normal operations"
"def _attach(self, transport):  
    # Assert that the _sockets attribute is not None, ensuring that the object is initialized properly.
    assert self._sockets is not None  
    
    # Add the provided transport to the _clients set, which tracks active client connections.
    self._clients.add(transport)  
","The _attach method is used to attach a transport to the current object, typically adding the transport to a set of client connections. The method asserts that the _sockets attribute is not None, which is likely a precondition to ensure that the object has been properly initialized before attaching a new transport. It then adds the provided transport to the _clients set, which presumably tracks active client connections. This function is useful for managing and keeping track of multiple transports in networking or server-side applications."
"def _start_serving(self):  
    # If the server is already serving, do nothing.
    if self._serving:  
        return  
    
    # Set the _serving flag to True, indicating that the server is now active.
    self._serving = True  
    
    # Iterate over each socket in the _sockets collection and start listening for connections.
    for sock in self._sockets:  
        sock.listen(self._backlog)  # Begin accepting incoming connections on the socket.
        
        # Start serving by calling the _start_serving method on the event loop,
        # passing the protocol factory, socket, SSL context, and other parameters.
        self._loop._start_serving(  
            self._protocol_factory,  # Protocol factory for creating protocol instances.
            sock,                    # The socket that will accept incoming connections.
            self._ssl_context,       # The SSL context for secure connections (if applicable).
            self,                    # The current object, likely representing the server.
            self._backlog,           # The backlog value, defining how many connections can be queued.
            self._ssl_handshake_timeout,  # Timeout for the SSL handshake.
            self._ssl_shutdown_timeout  # Timeout for the SSL shutdown.
        )
","The _start_serving method initiates the serving process for the server by setting the server state to ""serving"" and then configuring the sockets to start accepting connections. If the server is already in the ""serving"" state (self._serving is True), the method simply returns without performing any further action. Otherwise, it sets the _serving flag to True, indicating that the server is now actively serving. It then iterates over the _sockets collection, invoking the listen() method on each socket to begin accepting incoming connections. The method also calls _start_serving on the event loop (self._loop), passing necessary parameters like the protocol factory, SSL context, and various timeouts to manage SSL handshake and shutdown operations."
"def create_task(self, coro, *, name=None, context=None):  
    """"""Schedule a coroutine object.

    Return a task object.
    """"""
    
    # Check if the event loop is closed. If it is, prevent scheduling further tasks.
    self._check_closed()  
    
    # If no custom task factory is defined, create a Task using the default constructor.
    if self._task_factory is None:  
        # Create a task for the given coroutine (coro), with the event loop (self), name, and context.
        task = tasks.Task(coro, loop=self, name=name, context=context)  
        
        # If the task has a source traceback (it could have been copied from elsewhere), remove the last trace.
        if task._source_traceback:  
            del task._source_traceback[-1]  
    
    else:  
        # If a custom task factory is set, use it to create the task.
        if context is None:  
            # Use the legacy API if context is not provided.
            task = self._task_factory(self, coro)  
        else:  
            # Use the custom task factory and provide the context if specified.
            task = self._task_factory(self, coro, context=context)  

        # Set the task's name, ensuring it is set appropriately.
        task.set_name(name)  
    
    # Return the created task object, which represents the scheduled coroutine.
    return task  
","The create_task method schedules a coroutine for execution in the event loop, returning a task object that tracks the coroutine's execution. It first checks if the event loop is closed by calling _check_closed(). If no custom task factory (_task_factory) is set, it creates a Task object using the default task constructor. If a custom task factory is defined, it uses it to create the task, optionally passing a context. The method also ensures that the task name is set, and if the task has a source traceback, it removes the last element to avoid unwanted tracebacks from propagating. This method is crucial for managing coroutines within an event loop, scheduling them to run asynchronously."
"def _asyncgen_finalizer_hook(self, agen):  
    # Remove the asynchronous generator (agen) from the set of active generators.
    self._asyncgens.discard(agen)  
    
    # If the event loop is not closed, schedule the closing of the asynchronous generator.
    if not self.is_closed():  
        # Use call_soon_threadsafe to safely schedule the closing of the generator in the event loop.
        self.call_soon_threadsafe(self.create_task, agen.aclose())  
","The _asyncgen_finalizer_hook method is a finalizer for asynchronous generators, designed to clean up and close asynchronous generators when they are no longer in use. It removes the generator (agen) from the set of active asynchronous generators (self._asyncgens). If the event loop is still open (not closed), it schedules the asynchronous generator's aclose() method to be called, which ensures that any remaining asynchronous resources held by the generator are properly cleaned up. This method helps in managing the lifecycle of asynchronous generators, ensuring that resources are released when the generator is discarded."
"def _asyncgen_firstiter_hook(self, agen):  
    # If asynchronous generators have already been shut down, issue a warning.
    if self._asyncgens_shutdown_called:  
        warnings.warn(  
            f""asynchronous generator {agen!r} was scheduled after ""  
            f""loop.shutdown_asyncgens() call"",  # Warning message, indicating that the generator was scheduled too late.
            ResourceWarning,  # The type of warning issued (ResourceWarning).
            source=self)  # The source of the warning (self, indicating it's the current loop instance).

    # Add the asynchronous generator (agen) to the set of active async generators.
    self._asyncgens.add(agen)  
","The _asyncgen_firstiter_hook method is invoked when an asynchronous generator (agen) is first iterated. This hook is responsible for tracking the generator and ensuring proper warning handling if the generator is scheduled after the event loop has initiated its shutdown sequence for asynchronous generators. If the loop's shutdown process (loop.shutdown_asyncgens()) has already been called, it raises a warning indicating that an asynchronous generator was scheduled too late in the process. Regardless of the shutdown state, the generator is added to the set of active asynchronous generators (self._asyncgens), signaling that the event loop should manage it until it is completed or closed."
"async def shutdown_asyncgens(self):  
    """"""Shutdown all active asynchronous generators.""""""  
    # Mark that the asynchronous generator shutdown process has started.
    self._asyncgens_shutdown_called = True  

    # If there are no active generators or the Python version is less than 3.6,
    # return early as there is nothing to shut down.
    if not len(self._asyncgens):  
        # This covers both cases where no async generators are active or the
        # system doesn't support async generators (e.g., Python version < 3.6).
        return  

    # Convert the set of active async generators into a list for closing.
    closing_agens = list(self._asyncgens)  
    self._asyncgens.clear()  # Clear the list of active asynchronous generators.

    # Gather results from calling 'aclose()' on each generator asynchronously.
    results = await tasks.gather(  
        *[ag.aclose() for ag in closing_agens],  # Schedule the closing of each async generator.
        return_exceptions=True  # Return exceptions as results instead of raising them.
    )  

    # Iterate over the results and their corresponding generators.
    for result, agen in zip(results, closing_agens):  
        # If an exception was raised during the closing of the generator, handle it.
        if isinstance(result, Exception):  
            # Call the exception handler to manage and log the error.
            self.call_exception_handler({  
                'message': f'an error occurred during closing of '  
                           f'asynchronous generator {agen!r}',  # Error message including the generator.
                'exception': result,  # The exception that was raised.
                'asyncgen': agen  # The generator that caused the error.
            })
","The shutdown_asyncgens method is responsible for shutting down all active asynchronous generators within the event loop. It first sets a flag (_asyncgens_shutdown_called) to True to mark that the shutdown process has started. If there are no active asynchronous generators, or if the Python version is lower than 3.6 (which doesn't support the necessary asynchronous generator features), the method returns early without performing any action. If there are active generators, it collects them, clears the list of active generators, and then proceeds to close each asynchronous generator using its aclose() method. The method gathers the results of the aclose() calls, handling any exceptions that occur during the closure. If an exception occurs for any generator, the exception handler is called to log or manage the error appropriately."
"async def shutdown_default_executor(self, timeout=None):  
    """"""Schedule the shutdown of the default executor.

    The timeout parameter specifies the amount of time the executor will
    be given to finish joining. The default value is None, which means
    that the executor will be given an unlimited amount of time.
    """"""  
    # Mark that the shutdown of the executor has been requested.
    self._executor_shutdown_called = True  

    # If there is no default executor, nothing to shut down.
    if self._default_executor is None:  
        return  

    # Create a future that will represent the shutdown operation.
    future = self.create_future()  

    # Start a new thread to perform the shutdown operation asynchronously.
    thread = threading.Thread(target=self._do_shutdown, args=(future,))  
    thread.start()  # Start the shutdown thread.

    try:  
        # Wait for the future to complete within the specified timeout.
        async with timeouts.timeout(timeout):  
            await future  # Await the completion of the executor shutdown.
    except TimeoutError:  
        # If the shutdown takes longer than the timeout, issue a warning.
        warnings.warn(  
            ""The executor did not finishing joining ""  
            f""its threads within {timeout} seconds."",  # Warning message.
            RuntimeWarning,  # The warning type.
            stacklevel=2)  # Set the stack level to highlight where the warning is generated.
        
        # Force the executor to shut down without waiting for threads.
        self._default_executor.shutdown(wait=False)  
    else:  
        # If shutdown completed successfully within the timeout, join the shutdown thread.
        thread.join()  
","The shutdown_default_executor method is used to shut down the default executor associated with the event loop. The method schedules the shutdown of the executor by starting a separate thread that handles the shutdown process. The timeout parameter specifies the maximum time allowed for the executor to finish its operations. If the timeout expires before the executor finishes, a TimeoutError is raised, and a warning is issued. If the shutdown completes within the given timeout, the method waits for the thread to join and clean up. This method ensures that the executor is cleanly shut down, and handles cases where the shutdown takes longer than expected by issuing appropriate warnings."
"def _do_shutdown(self, future):  
    try:  
        # Attempt to shut down the default executor and wait for threads to finish.
        self._default_executor.shutdown(wait=True)  

        # If the event loop is not closed, set the result of the future as 'None',
        # indicating the shutdown was successful and completed.
        if not self.is_closed():  
            self.call_soon_threadsafe(  # Schedule the result setting on the event loop.
                futures._set_result_unless_cancelled,  # Helper function to set result if not cancelled.
                future,  # The future that will be set.
                None  # No exception, indicating successful completion.
            )  
    except Exception as ex:  
        # If an error occurs during the shutdown, handle it.
        if not self.is_closed() and not future.cancelled():  
            # If the future is not cancelled and the loop is not closed, set the exception on the future.
            self.call_soon_threadsafe(future.set_exception, ex)  # Set the exception on the future.
","The _do_shutdown method is responsible for shutting down the default executor by calling its shutdown() method and waiting for all threads to finish. If the shutdown process completes successfully and the event loop is still open, the method sets the result of the provided future to None, indicating that the shutdown has completed without issues. If any exceptions occur during the shutdown process, they are captured and the exception is set on the future (if it hasn't been cancelled), allowing the event loop to handle or log the error accordingly."
"def _run_forever_setup(self):  
    """"""Prepare the run loop to process events.

    This method exists so that custom custom event loop subclasses (e.g., event loops
    that integrate a GUI event loop with Python's event loop) have access to all the
    loop setup logic.
    """"""  
    # Ensure the event loop is not closed before starting the run forever process.
    self._check_closed()  

    # Ensure the event loop is not already running (cannot run again if already running).
    self._check_running()  

    # Set up coroutine origin tracking if debugging is enabled.
    self._set_coroutine_origin_tracking(self._debug)  

    # Save the current asynchronous generator hooks for later restoration.
    self._old_agen_hooks = sys.get_asyncgen_hooks()  

    # Record the thread identifier of the current thread.
    self._thread_id = threading.get_ident()  

    # Set custom async generator hooks for first iteration and finalization.
    sys.set_asyncgen_hooks(  
        firstiter=self._asyncgen_firstiter_hook,  # Hook for the first iteration of async generators.
        finalizer=self._asyncgen_finalizer_hook  # Hook for finalization of async generators.
    )  

    # Mark the current event loop as the running loop in the events module.
    events._set_running_loop(self)  
","The _run_forever_setup method is responsible for preparing the event loop to process events. It performs essential setup tasks such as verifying that the loop is not closed and that it is not already running, enabling coroutine origin tracking for debugging, and setting asynchronous generator hooks. The method also records the thread identifier and sets custom asynchronous generator hooks that handle the first iteration and finalization of asynchronous generators. Additionally, it marks the event loop as the current running loop, ensuring that the system is aware of the loop that is active. This setup is crucial for the proper functioning of the event loop, especially when integrating with custom or GUI event loops."
"def run_until_complete(self, future):  
    """"""Run until the Future is done.

    If the argument is a coroutine, it is wrapped in a Task.

    WARNING: It would be disastrous to call run_until_complete()
    with the same coroutine twice -- it would wrap it in two
    different Tasks and that can't be good.

    Return the Future's result, or raise its exception.
    """"""  
    # Ensure the event loop is not closed before running.
    self._check_closed()  

    # Ensure the event loop is not already running (cannot run again if already running).
    self._check_running()  

    # Check if the provided argument is a Future, if not, wrap it in a Task.
    new_task = not futures.isfuture(future)  
    future = tasks.ensure_future(future, loop=self)  # Ensure it's a future (wrap if it's a coroutine).
    
    if new_task:  
        # If the future is newly created (not passed as a Future), prevent logging pending task destruction.
        future._log_destroy_pending = False  

    # Add a callback that will run when the Future is done.
    future.add_done_callback(_run_until_complete_cb)  

    try:  
        # Run the event loop until the Future completes.
        self.run_forever()  
    except:  
        # Handle the case where the Future raised an exception.
        if new_task and future.done() and not future.cancelled():
            # If the coroutine raised a BaseException, consume the exception.
            # This avoids logging a warning since the caller doesn't have access to the task.
            future.exception()  
        raise  # Re-raise the exception if there was one.
    finally:  
        # Remove the done callback to avoid it being called again.
        future.remove_done_callback(_run_until_complete_cb)  

    # If the future is not done after the event loop stops, raise an error.
    if not future.done():  
        raise RuntimeError('Event loop stopped before Future completed.')  

    # Return the result of the future (or raise its exception).
    return future.result()  
","The run_until_complete method runs the event loop until the given Future is completed. If the argument is a coroutine, it wraps the coroutine in a Task. This method ensures that the event loop processes events until the specified Future is done, handling the completion or failure of the Future. It also ensures that the provided Future is properly handled and cleaned up, including managing any exceptions that may arise during the event loop's execution. The method returns the result of the Future or raises its exception. If the event loop stops before the Future is completed, a RuntimeError is raised."
"async def _sendfile_fallback(self, transp, file, offset, count):  
    # If an offset is specified, seek to that position in the file.
    if offset:  
        file.seek(offset)  
    
    # Set blocksize to the minimum of the requested count or 16KB (16384 bytes).
    blocksize = min(count, 16384) if count else 16384  
    
    # Initialize a buffer to store the read data.
    buf = bytearray(blocksize)  
    
    # Variable to track the total amount of data sent.
    total_sent = 0  
    
    # Create a custom protocol for managing the flow of data.
    proto = _SendfileFallbackProtocol(transp)  
    
    try:  
        while True:  
            # If a count is provided, adjust the blocksize to the remaining count.
            if count:  
                blocksize = min(count - total_sent, blocksize)  
                if blocksize <= 0:  
                    return total_sent  # If all data is sent, return the total sent.

            # Create a memory view of the buffer up to the current blocksize.
            view = memoryview(buf)[:blocksize]  

            # Read data from the file into the buffer asynchronously.
            read = await self.run_in_executor(None, file.readinto, view)  
            if not read:  
                return total_sent  # If no more data is read (EOF), return the total sent.

            # Ensure the transport is ready to send more data (drain).
            await proto.drain()  

            # Write the read data to the transport connection.
            transp.write(view[:read])  

            # Update the total number of bytes sent.
            total_sent += read  

    finally:  
        # After sending data, update the file's position if data was sent.
        if total_sent > 0 and hasattr(file, 'seek'):  
            file.seek(offset + total_sent)  

        # Restore the protocol after the send operation.
        await proto.restore()  
","The _sendfile_fallback method is an asynchronous function designed to send a file in chunks over a transport connection, with a fallback mechanism when the sendfile system call is not available or supported. It reads from the provided file starting from the specified offset, sending data in blocks to the transport connection. The function handles the reading of the file, writing data over the transport, and ensuring that the file pointer is correctly updated after data is sent. The method also uses a custom protocol (_SendfileFallbackProtocol) to manage the flow of data and ensure that the transport is ready for the next chunk of data."
"async def _ensure_resolved(self, address, *,  # The function takes an address (host, port) and additional parameters for resolving.
                               family=0,  # The address family (AF_INET for IPv4, AF_INET6 for IPv6, etc.), default is 0 (AF_UNSPEC).
                               type=socket.SOCK_STREAM,  # The socket type, default is SOCK_STREAM (TCP).
                               proto=0,  # The protocol type, default is 0 (any protocol).
                               flags=0,  # Additional flags for getaddrinfo, default is 0.
                               loop):  # The event loop used for asynchronous operations.
    
    host, port = address[:2]  # Extract host and port from the address tuple.
    
    # Attempt to resolve the address using the _ipaddr_info function (local resolution).
    info = _ipaddr_info(host, port, family, type, proto, *address[2:])
    
    if info is not None:  # If the address is already resolved to an IP address.
        return [info]  # Return the resolved information as a list.
    else:
        # If the address is not resolved (i.e., its a domain name), use getaddrinfo asynchronously.
        return await loop.getaddrinfo(host, port, family=family, type=type, 
                                      proto=proto, flags=flags)
","The _ensure_resolved method is an asynchronous utility function designed to ensure that a given address (host and port) is resolved to an IP address. The function first attempts to resolve the address using a local method (_ipaddr_info) to check if the provided host is already an IP address. If it is, the method returns the resolved address. If not, it asynchronously resolves the host and port using the event loop's getaddrinfo method. This function is useful for handling both pre-resolved IP addresses and domain names that need resolution."
"async def connect_accepted_socket(  # Define the asynchronous function to handle accepted socket connections.
            self, protocol_factory, sock,  # Self refers to the instance, protocol_factory is used to create a new protocol, sock is the accepted socket.
            *, ssl=None,  # Optional SSL configuration for the connection.
            ssl_handshake_timeout=None,  # Optional timeout for the SSL handshake.
            ssl_shutdown_timeout=None):  # Optional timeout for the SSL shutdown.

    # Check if the socket is of the expected SOCK_STREAM type (TCP socket).
    if sock.type != socket.SOCK_STREAM:
        raise ValueError(f'A Stream Socket was expected, got {sock!r}')

    # If ssl_handshake_timeout is provided, ensure that ssl is also provided.
    if ssl_handshake_timeout is not None and not ssl:
        raise ValueError(
            'ssl_handshake_timeout is only meaningful with ssl')

    # Similarly, ensure ssl_shutdown_timeout is only set if ssl is provided.
    if ssl_shutdown_timeout is not None and not ssl:
        raise ValueError(
            'ssl_shutdown_timeout is only meaningful with ssl')

    # If a socket is provided, check if it's an SSL socket and raise an error if it is.
    if sock is not None:
        _check_ssl_socket(sock)

    # Asynchronously create the connection transport and protocol.
    transport, protocol = await self._create_connection_transport(
        sock, protocol_factory, ssl, '', server_side=True,  # Establish the transport and protocol, indicating that this is a server-side connection.
        ssl_handshake_timeout=ssl_handshake_timeout,  # Pass the SSL handshake timeout if provided.
        ssl_shutdown_timeout=ssl_shutdown_timeout)  # Pass the SSL shutdown timeout if provided.

    # Debugging: If debugging is enabled, log the details of the transport and protocol.
    if self._debug:
        # The transport may have created a new socket for SSL, so get the original socket from the transport.
        sock = transport.get_extra_info('socket')
        logger.debug(""%r handled: (%r, %r)"", sock, transport, protocol)  # Log the socket, transport, and protocol for debugging.

    # Return the established transport and protocol for further use.
    return transport, protocol
","The connect_accepted_socket method is an asynchronous function that handles the connection setup for an accepted socket, typically in the context of a server accepting incoming client connections. It validates the socket type, handles optional SSL configurations, and establishes a transport and protocol using the provided socket. If SSL is involved, the method manages SSL handshake and shutdown timeouts. After successfully establishing the transport and protocol, the function returns them, providing the caller with the necessary components to communicate over the accepted connection."
"def _log_subprocess(self, msg, stdin, stdout, stderr):  # Define the function to log subprocess details.
    info = [msg]  # Start by adding the initial message (e.g., command being executed).
    
    # If stdin is provided, add its formatted representation to the log.
    if stdin is not None:
        info.append(f'stdin={_format_pipe(stdin)}')

    # If stdout and stderr are both directed to the same stream (STDOUT), log them together.
    if stdout is not None and stderr == subprocess.STDOUT:
        info.append(f'stdout=stderr={_format_pipe(stdout)}')
    else:
        # Otherwise, log stdout and stderr separately.
        if stdout is not None:
            info.append(f'stdout={_format_pipe(stdout)}')
        if stderr is not None:
            info.append(f'stderr={_format_pipe(stderr)}')

    # Join the message parts into a single string and log it as a debug message.
    logger.debug(' '.join(info))
","The _log_subprocess method is used to log information about a subprocess invocation, including details about the stdin, stdout, and stderr streams. It formats these streams appropriately (using the _format_pipe function) and constructs a log message. The method then logs this message using the logger.debug() function, ensuring that relevant details about the subprocess communication are captured for debugging purposes. This function helps monitor the input/output streams of subprocesses, especially in complex systems where multiple subprocesses are managed."
"def _set_coroutine_origin_tracking(self, enabled):  # Method to toggle coroutine origin tracking.
    # Check if the desired state (enabled/disabled) is different from the current state.
    if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):
        return  # No change needed if the state is already as requested.

    if enabled:  # If enabling origin tracking:
        # Save the current depth of coroutine origin tracking before modifying it.
        self._coroutine_origin_tracking_saved_depth = (
            sys.get_coroutine_origin_tracking_depth())
        # Set the coroutine origin tracking depth to a predefined constant (DEBUG_STACK_DEPTH).
        sys.set_coroutine_origin_tracking_depth(
            constants.DEBUG_STACK_DEPTH)
    else:  # If disabling origin tracking:
        # Restore the original tracking depth that was saved earlier.
        sys.set_coroutine_origin_tracking_depth(
            self._coroutine_origin_tracking_saved_depth)

    # Update the internal state to reflect whether coroutine origin tracking is enabled or not.
    self._coroutine_origin_tracking_enabled = enabled
","The _set_coroutine_origin_tracking method is used to manage the tracking of coroutine origins in Python, which helps identify the stack trace when a coroutine is created. The method enables or disables this tracking based on the enabled parameter. If tracking is enabled, it saves the current depth of coroutine origin tracking, and then sets it to a predefined constant value (constants.DEBUG_STACK_DEPTH). If tracking is disabled, it restores the saved depth of the tracking. This method ensures that the coroutine origin tracking is only modified when necessary, providing more granular control over how coroutine origins are traced."
"def _format_callbacks(cb):  # Function to format the list of callbacks.
    """"""helper function for Future.__repr__""""""
    size = len(cb)  # Get the number of callbacks in the list.
    if not size:  # If there are no callbacks.
        cb = ''  # Set cb to an empty string.

    # Function to format a single callback.
    def format_cb(callback):
        return format_helpers._format_callback_source(callback, ())

    # Handle different callback list sizes.
    if size == 1:
        cb = format_cb(cb[0][0])  # Format the single callback.
    elif size == 2:
        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))  # Format two callbacks.
    elif size > 2:
        # For more than two callbacks, display the first and last callback and the count of the omitted ones.
        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]), size - 2, format_cb(cb[-1][0]))

    # Return the formatted string with all the callbacks.
    return f'cb=[{cb}]'
","The _format_callbacks function is a helper used to format the representation of callbacks associated with a Future object (as seen in Future.__repr__). It takes a list of callback functions, processes it, and returns a string that formats the callbacks in a concise and readable way. If there are too many callbacks, it reduces the list to a more compact form while still indicating how many callbacks are omitted."
"def _future_repr_info(future):
    # (Future) -> str
    """"""helper function for Future.__repr__""""""
    info = [future._state.lower()]  # Start with the state of the Future (e.g., 'pending', 'done', etc.)

    # If the future is finished, we add more detailed information about the result or exception.
    if future._state == _FINISHED:
        if future._exception is not None:
            # If there's an exception, add it to the info.
            info.append(f'exception={future._exception!r}')
        else:
            # If there is no exception, add the result. Use reprlib to limit string length.
            result = reprlib.repr(future._result)
            info.append(f'result={result}')

    # If there are any callbacks, format them and add them to the info.
    if future._callbacks:
        info.append(_format_callbacks(future._callbacks))

    # If the future has a source traceback, include the location where it was created.
    if future._source_traceback:
        frame = future._source_traceback[-1]  # Get the last frame from the traceback.
        info.append(f'created at {frame[0]}:{frame[1]}')  # Append the file and line number.

    return info  # Return the list of formatted strings.
","The _future_repr_info function is a helper used for generating a string representation of a Future object in the __repr__ method. It collects various pieces of information about the Future's state, result, exception, callbacks, and creation source, and returns them as a list of formatted strings. This representation can be helpful for debugging and logging purposes."
"def _task_repr_info(task):
    # Generate the basic future representation info using _future_repr_info.
    info = base_futures._future_repr_info(task)

    # If the task is cancelling and not yet done, change the state representation.
    if task.cancelling() and not task.done():
        info[0] = 'cancelling'

    # Insert the task name after the state.
    info.insert(1, 'name=%r' % task.get_name())

    # If the task is waiting for another future, insert that information.
    if task._fut_waiter is not None:
        info.insert(2, f'wait_for={task._fut_waiter!r}')

    # If the task has an associated coroutine, insert it as well.
    if task._coro:
        coro = coroutines._format_coroutine(task._coro)  # Format the coroutine for display.
        info.insert(2, f'coro=<{coro}>')

    return info  # Return the list of formatted strings representing the task.
","The _task_repr_info function is a helper designed to generate a string representation of a Task object, which is a subclass of Future in Python's asynchronous programming model. It builds upon the _future_repr_info function, adding additional details specific to a task, such as its cancellation status, associated coroutine, and future waiter."
"def _task_print_stack(task, limit, file):
    # Initialize a list to hold the stack trace details
    extracted_list = []
    checked = set()

    # Iterate over the frames in the task's stack trace, limited by 'limit'
    for f in task.get_stack(limit=limit):
        lineno = f.f_lineno  # Line number where the frame is located
        co = f.f_code  # Code object of the frame
        filename = co.co_filename  # Filename of the code
        name = co.co_name  # Name of the function or method

        # Ensure that we only check the cache for a given file once
        if filename not in checked:
            checked.add(filename)
            linecache.checkcache(filename)  # Refresh the linecache for this file

        # Get the source code line at the given line number
        line = linecache.getline(filename, lineno, f.f_globals)

        # Append the stack trace information for this frame
        extracted_list.append((filename, lineno, name, line))

    # Check if the task has an exception associated with it
    exc = task._exception

    # If no stack trace information was collected
    if not extracted_list:
        print(f'No stack for {task!r}', file=file)
    elif exc is not None:
        # If the task has an exception, print a traceback
        print(f'Traceback for {task!r} (most recent call last):', file=file)
    else:
        # Otherwise, just print the stack trace
        print(f'Stack for {task!r} (most recent call last):', file=file)

    # Print the extracted stack trace to the provided file
    traceback.print_list(extracted_list, file=file)

    # If an exception was caught in the task, print the exception details
    if exc is not None:
        for line in traceback.format_exception_only(exc.__class__, exc):
            print(line, file=file, end='')
","The _task_print_stack function is designed to print the stack trace for a Task object, including the source code locations and any associated exception details. It retrieves the stack information from the task, processes it, and formats the output to display the trace in a human-readable manner. This function is useful for debugging and tracing the execution path of tasks in asynchronous code, particularly when dealing with failed or incomplete tasks."
"def _is_debug_mode():
    # See: https://docs.python.org/3/library/asyncio-dev.html#asyncio-debug-mode.
    
    # Check if the 'dev_mode' flag is set in the sys.flags
    return sys.flags.dev_mode or (not sys.flags.ignore_environment and
                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))
","The _is_debug_mode() function checks whether Python's asyncio debug mode is enabled. This function is useful for determining if the program should run in debug mode, which enables additional debugging information and logging for asyncio-related activities."
"def _get_function_source(func):
    # Unwrap the function to handle cases where it's a wrapped function (e.g., through decorators)
    func = inspect.unwrap(func)

    # If the function is a standard function
    if inspect.isfunction(func):
        # Get the code object of the function and extract the filename and first line number
        code = func.__code__
        return (code.co_filename, code.co_firstlineno)

    # If the function is a functools.partial (a partially-applied function)
    if isinstance(func, functools.partial):
        return _get_function_source(func.func)

    # If the function is a functools.partialmethod (a partially-applied method)
    if isinstance(func, functools.partialmethod):
        return _get_function_source(func.func)

    # If the function type is not recognized, return None
    return None
","The function _get_function_source extracts the source file and line number where a function was defined. It supports regular functions, functools.partial objects, and functools.partialmethod objects."
"def _format_callback_source(func, args, *, debug=False):
    # Get the formatted string representation of the function and its arguments
    func_repr = _format_callback(func, args, None, debug=debug)
    
    # Retrieve the source information (file and line number) of the function
    source = _get_function_source(func)
    
    # If the source information is available, append it to the function's representation
    if source:
        func_repr += f' at {source[0]}:{source[1]}'
    
    # Return the formatted function representation with source info if available
    return func_repr
","This function is used to generate a string representation of the function and its arguments. It formats the function in a way that's suitable for logging or debugging, including handling the function name and the arguments passed to it.
The debug flag might control additional details, such as whether to include extra debugging information."
"def _format_args_and_kwargs(args, kwargs, *, debug=False):
    """"""Format function arguments and keyword arguments.

    Special case for a single parameter: ('hello',) is formatted as ('hello').

    Note that this function only returns argument details when
    debug=True is specified, as arguments may contain sensitive
    information.
    """"""
    if not debug:
        return '()'

    # use reprlib to limit the length of the output
    items = []
    if args:
        items.extend(reprlib.repr(arg) for arg in args)
    if kwargs:
        items.extend(f'{k}={reprlib.repr(v)}' for k, v in kwargs.items())
    return '({})'.format(', '.join(items))
","The function _format_args_and_kwargs formats a function's positional (args) and keyword (kwargs) arguments for display or logging. This function is designed with a special focus on controlling the verbosity of the argument details based on the debug flag, and it uses reprlib to limit the output length for potentially large objects."
"def _format_callback(func, args, kwargs, *, debug=False, suffix=''):
    if isinstance(func, functools.partial):
        suffix = _format_args_and_kwargs(args, kwargs, debug=debug) + suffix
        return _format_callback(func.func, func.args, func.keywords,
                                debug=debug, suffix=suffix)

    if hasattr(func, '__qualname__') and func.__qualname__:
        func_repr = func.__qualname__
    elif hasattr(func, '__name__') and func.__name__:
        func_repr = func.__name__
    else:
        func_repr = repr(func)

    func_repr += _format_args_and_kwargs(args, kwargs, debug=debug)
    if suffix:
        func_repr += suffix
    return func_repr
","The function _format_callback is designed to generate a formatted string representation of a callback function, including its arguments, keyword arguments, and other relevant details. It's particularly useful when debugging or logging asynchronous callbacks."
"def extract_stack(f=None, limit=None):
    """"""Replacement for traceback.extract_stack() that only does the
    necessary work for asyncio debug mode.
    """"""
    if f is None:
        f = sys._getframe().f_back
    if limit is None:
        # Limit the amount of work to a reasonable amount, as extract_stack()
        # can be called for each coroutine and future in debug mode.
        limit = constants.DEBUG_STACK_DEPTH
    stack = traceback.StackSummary.extract(traceback.walk_stack(f),
                                           limit=limit,
                                           lookup_lines=False)
    stack.reverse()
    return stack
","The function extract_stack is a custom replacement for traceback.extract_stack() that is tailored for use in asyncio debug mode. It helps to efficiently extract stack traces with a focus on reducing overhead, particularly when debugging asynchronous operations."
"def wrap_future(future, *, loop=None):
    """"""Wrap concurrent.futures.Future object.""""""
    if isfuture(future):
        return future
    assert isinstance(future, concurrent.futures.Future), \
        f'concurrent.futures.Future is expected, got {future!r}'
    if loop is None:
        loop = events.get_event_loop()
    new_future = loop.create_future()
    _chain_future(future, new_future)
    return new_future
","The function wrap_future is designed to wrap a concurrent.futures.Future object into an asyncio Future object. This is useful when integrating concurrent.futures.Future with an asyncio event loop, as asyncio has its own Future implementation."
"def _call_set_state(source):
    if (destination.cancelled() and
            dest_loop is not None and dest_loop.is_closed()):
        return
    if dest_loop is None or dest_loop is source_loop:
        _set_state(destination, source)
    else:
        if dest_loop.is_closed():
            return
        dest_loop.call_soon_threadsafe(_set_state, destination, source)
","The function _call_set_state appears to be a helper function for managing the state of a Future object, potentially in a multi-loop asyncio context. It looks like it's designed to ensure that the state of the destination Future is updated based on the state of the source Future, possibly across different event loops."
"def _call_check_cancel(destination):
    """"""
    Checks if the destination Future is cancelled, and if so, propagates the cancellation
    to the source Future, depending on whether both futures belong to the same event loop.
    
    Args:
        destination (Future): The destination Future to check for cancellation.
    
    This function ensures that if the destination Future is cancelled, the source Future 
    is also cancelled, either immediately (if both are in the same event loop) or safely 
    in the event loop where the source Future resides (if they are in different loops).
    """"""
    if destination.cancelled():
        # Check if the destination Future is cancelled.
        
        if source_loop is None or source_loop is dest_loop:
            # If the source Future is not associated with an event loop or is
            # in the same event loop as the destination, directly cancel it.
            source.cancel()
        else:
            # If the source Future is in a different event loop, schedule the
            # cancellation in the source loop using call_soon_threadsafe to ensure 
            # cancellation is propagated asynchronously in the correct loop.
            source_loop.call_soon_threadsafe(source.cancel)
","The _call_check_cancel function propagates the cancellation of a destination Future to a source Future, ensuring that the cancellation occurs appropriately based on whether the futures are in the same or different event loops. If both futures are in the same loop, the source is cancelled immediately. If they are in different event loops, the function schedules the cancellation in the source's loop using call_soon_threadsafe. This is crucial for managing asynchronous tasks across multiple event loops, particularly in multi-threaded applications, where it ensures the proper cancellation of dependent tasks."
"def _set_state(future, other):
    """"""
    Sets the state of a future (or concurrent future) by copying the state from another future.
    
    Args:
        future (Future): The future whose state needs to be set.
        other (Future): The source future from which the state will be copied.
    
    This function determines the type of the `future` (whether it is a regular `Future` or a
    `concurrent.Future`), and depending on the type, it either copies the state from `other`
    to `future` using `_copy_future_state()` or uses `_set_concurrent_future_state()` to set
    the state for `concurrent.Future`.
    """"""
    if isfuture(future):
        # If 'future' is a regular Future, copy its state from 'other'.
        _copy_future_state(other, future)
    else:
        # If 'future' is a concurrent Future, set its state using a different method.
        _set_concurrent_future_state(future, other)
","The _set_state function is responsible for setting the state of one future (future) by copying the state from another (other). It checks whether the future is a regular Future or a concurrent.Future, and based on that, calls either _copy_future_state() for regular futures or _set_concurrent_future_state() for concurrent futures. This ensures that the appropriate method is used for setting the state, whether the future is part of the asyncio event loop or a concurrent task in a multi-threaded environment."
"def _chain_future(source, destination):
    """"""
    Chain two futures so that when one completes, the other does as well.

    This function ensures that the result (or exception) of the source future 
    is transferred to the destination future when the source completes. If the 
    destination is cancelled, the source future will also be cancelled.
    
    Args:
        source (Future or concurrent.futures.Future): The source future.
        destination (Future or concurrent.futures.Future): The destination future to be chained.

    Raises:
        TypeError: If either 'source' or 'destination' is not a valid Future or concurrent future.
        
    The function checks the type of both 'source' and 'destination' to ensure they are valid futures 
    (either asyncio.Future or concurrent.futures.Future). It retrieves the event loop for both futures 
    (if they are asyncio futures) and proceeds to chain them by copying the result or exception from 
    the source to the destination and handling cancellation propagation.
    """"""
    # Ensure both source and destination are valid futures
    if not isfuture(source) and not isinstance(source, concurrent.futures.Future):
        raise TypeError('A future is required for source argument')
    if not isfuture(destination) and not isinstance(destination, concurrent.futures.Future):
        raise TypeError('A future is required for destination argument')

    # Get the event loop for the source and destination futures, if applicable
    source_loop = _get_loop(source) if isfuture(source) else None
    dest_loop = _get_loop(destination) if isfuture(destination) else None
","The _chain_future function links two futures (source and destination) in such a way that when one completes, the other is also completed with the same result or exception. If the source future is cancelled, the destination future will also be cancelled. The function checks if both source and destination are valid futures (either asyncio.Future or concurrent.futures.Future). It retrieves the event loops for both futures if they are asyncio-based and prepares the futures to transfer the result or exception between them while ensuring cancellation propagation."
"def mean(data):
    """"""Return the sample arithmetic mean of data.

    >>> mean([1, 2, 3, 4, 4])
    2.8

    >>> from fractions import Fraction as F
    >>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])
    Fraction(13, 21)

    >>> from decimal import Decimal as D
    >>> mean([D(""0.5""), D(""0.75""), D(""0.625""), D(""0.375"")])
    Decimal('0.5625')

    If ``data`` is empty, StatisticsError will be raised.
    """"""
    # Calculate the total sum and count of elements in the dataset
    T, total, n = _sum(data)

    # If the dataset is empty, raise a StatisticsError
    if n < 1:
        raise StatisticsError('mean requires at least one data point')

    # Return the mean value by dividing the total sum by the count
    return _convert(total / n, T)
","The mean function computes the arithmetic mean (average) of a given dataset data. It first calculates the total sum and number of elements in the dataset using the _sum() helper function. If the dataset is empty, it raises a StatisticsError. The function then computes the mean by dividing the total sum by the number of elements. It also handles the conversion of different numeric types (such as Fraction or Decimal) through the _convert() function, ensuring the result is returned in the appropriate format.

"
"def fmean(data, weights=None):
    """"""Convert data to floats and compute the arithmetic mean.

    This runs faster than the mean() function and it always returns a float.
    If the input dataset is empty, it raises a StatisticsError.

    >>> fmean([3.5, 4.0, 5.25])
    4.25
    """"""
    # If no weights are provided, compute the simple mean
    if weights is None:
        try:
            n = len(data)
        except TypeError:
            # Handle iterators that do not define __len__().
            counter = count()
            total = fsum(map(itemgetter(0), zip(data, counter)))
            n = next(counter)
        else:
            total = fsum(data)

        # If the dataset is empty, raise an error
        if not n:
            raise StatisticsError('fmean requires at least one data point')

        return total / n

    # If weights are provided, ensure they are in a list or tuple format
    if not isinstance(weights, (list, tuple)):
        weights = list(weights)

    try:
        # Calculate the weighted sum of the data and weights
        num = sumprod(data, weights)
    except ValueError:
        raise StatisticsError('data and weights must be the same length')

    # Calculate the total weight
    den = fsum(weights)

    # If the sum of weights is zero, raise an error
    if not den:
        raise StatisticsError('sum of weights must be non-zero')

    return num / den
","The fmean function computes the arithmetic mean of the data by first converting the values to floats. It is optimized for performance and always returns a float result. If weights are provided, it computes the weighted mean, ensuring that both data and weights are of equal length. If no weights are provided, the function calculates the unweighted mean. It uses fsum() for accurate floating-point summation. If the dataset is empty or the sum of weights is zero, it raises a StatisticsError."
"def geometric_mean(data):
    """"""Convert data to floats and compute the geometric mean.

    Raises a StatisticsError if the input dataset is empty
    or if it contains a negative value.

    Returns zero if the product of inputs is zero.

    No special efforts are made to achieve exact results.
    (However, this may change in the future.)

    >>> round(geometric_mean([54, 24, 36]), 9)
    36.0
    """"""
    n = 0
    found_zero = False

    # Helper function to handle positive values, and check for zero or negative values
    def count_positive(iterable):
        nonlocal n, found_zero
        for n, x in enumerate(iterable, start=1):
            if x > 0.0 or math.isnan(x):
                yield x
            elif x == 0.0:
                found_zero = True
            else:
                raise StatisticsError('No negative inputs allowed', x)

    # Compute the sum of the logarithms of positive values in the data
    total = fsum(map(log, count_positive(data)))

    # If the dataset is empty, raise an error
    if not n:
        raise StatisticsError('Must have a non-empty dataset')

    # Handle edge cases for zero or infinite products
    if math.isnan(total):
        return math.nan
    if found_zero:
        return math.nan if total == math.inf else 0.0

    # Return the geometric mean by exponentiating the average of the log values
    return exp(total / n)
","The geometric_mean function computes the geometric mean of a dataset data. It first filters out negative values and handles zeros by checking the log of each element in the dataset. If the dataset contains negative values, it raises a StatisticsError. If the product of values is zero, the function returns 0.0. The function utilizes the logarithmic transformation to compute the geometric mean, summing the logs of positive values and then exponentiating the average of the logs to obtain the result. It ensures that edge cases, like infinite or NaN results, are handled appropriately."
"def harmonic_mean(data, weights=None):
    """"""Return the harmonic mean of data.

    The harmonic mean is the reciprocal of the arithmetic mean of the
    reciprocals of the data.  It can be used for averaging ratios or
    rates, for example speeds.

    Suppose a car travels 40 km/hr for 5 km and then speeds-up to
    60 km/hr for another 5 km. What is the average speed?

        >>> harmonic_mean([40, 60])
        48.0

    Suppose a car travels 40 km/hr for 5 km, and when traffic clears,
    speeds-up to 60 km/hr for the remaining 30 km of the journey. What
    is the average speed?

        >>> harmonic_mean([40, 60], weights=[5, 30])
        56.0

    If ``data`` is empty, or any element is less than zero,
    ``harmonic_mean`` will raise ``StatisticsError``.
    """"""
    # Convert data to a list if it's an iterable (like an iterator)
    if iter(data) is data:
        data = list(data)

    errmsg = 'harmonic mean does not support negative values'

    # Get the length of data
    n = len(data)
    
    # If data is empty, raise a StatisticsError
    if n < 1:
        raise StatisticsError('harmonic_mean requires at least one data point')
    
    # Handle case where there is only one value and no weights are provided
    elif n == 1 and weights is None:
        x = data[0]
        if isinstance(x, (numbers.Real, Decimal)):
            if x < 0:
                raise StatisticsError(errmsg)
            return x
        else:
            raise TypeError('unsupported type')

    # If weights are not provided, default to equal weights for each element
    if weights is None:
        weights = repeat(1, n)
        sum_weights = n
    else:
        # Convert weights to a list if they are an iterable
        if iter(weights) is weights:
            weights = list(weights)
        
        # Ensure weights and data have the same length
        if len(weights) != n:
            raise StatisticsError('Number of weights does not match data size')

        # Sum the weights after ensuring no negative values
        _, sum_weights, _ = _sum(w for w in _fail_neg(weights, errmsg))

    # Handle possible negative values in data
    try:
        data = _fail_neg(data, errmsg)
        
        # Calculate the harmonic sum using weights and data
        T, total, count = _sum(w / x if w else 0 for w, x in zip(weights, data))
    except ZeroDivisionError:
        return 0

    # Ensure the total sum is positive
    if total <= 0:
        raise StatisticsError('Weighted sum must be positive')

    # Return the harmonic mean after applying the appropriate conversion
    return _convert(sum_weights / total, T)
","The harmonic_mean function calculates the harmonic mean, which is defined as the reciprocal of the arithmetic mean of the reciprocals of the dataset. It can be particularly useful for averaging rates, like speeds. If weights are provided, it computes the weighted harmonic mean, ensuring that both the data and weights are non-negative. The function handles cases for empty datasets, single data points, and the presence of negative values, raising appropriate errors when necessary. It uses helper functions such as _fail_neg and _sum to process the data and weights, ensuring the correct calculations.

"
"def median(data):
    """"""Return the median (middle value) of numeric data.

    When the number of data points is odd, return the middle data point.
    When the number of data points is even, the median is interpolated by
    taking the average of the two middle values:

    >>> median([1, 3, 5])
    3
    >>> median([1, 3, 5, 7])
    4.0
    """"""
    # Sort the data in ascending order
    data = sorted(data)
    n = len(data)
    
    # Raise an error if the dataset is empty
    if n == 0:
        raise StatisticsError(""no median for empty data"")
    
    # If the number of elements is odd, return the middle value
    if n % 2 == 1:
        return data[n // 2]
    else:
        # If the number of elements is even, return the average of the two middle values
        i = n // 2
        return (data[i - 1] + data[i]) / 2
","The median function calculates the median of a dataset. It first sorts the data in ascending order, then checks the number of elements. If the number of elements is odd, it returns the middle value. If the number of elements is even, it returns the average of the two middle values. The function raises a StatisticsError if the dataset is empty, ensuring that no invalid results are produced. It is useful for finding the central value in a dataset, especially when the data contains outliers, as the median is less sensitive to extreme values than the mean."
"def median_low(data):
    """"""Return the low median of numeric data.

    When the number of data points is odd, the middle value is returned.
    When it is even, the smaller of the two middle values is returned.

    >>> median_low([1, 3, 5])
    3
    >>> median_low([1, 3, 5, 7])
    3
    """"""
    # Sort the data in ascending order
    data = sorted(data)
    n = len(data)
    
    # Raise an error if the dataset is empty
    if n == 0:
        raise StatisticsError(""no median for empty data"")
    
    # If the number of elements is odd, return the middle value
    if n % 2 == 1:
        return data[n // 2]
    else:
        # If the number of elements is even, return the smaller of the two middle values
        return data[n // 2 - 1]
","The median_low function calculates the ""low"" median of a dataset, which is defined as the smaller of the two middle values when the dataset has an even number of elements. Like the median function, it first sorts the dataset and checks if the dataset is empty. If the dataset has an odd number of elements, it returns the middle element. For even-sized datasets, it returns the smaller of the two middle values, providing a measure of central tendency that biases towards the lower middle."
"def median_high(data):
    """"""Return the high median of data.

    When the number of data points is odd, the middle value is returned.
    When it is even, the larger of the two middle values is returned.

    >>> median_high([1, 3, 5])
    3
    >>> median_high([1, 3, 5, 7])
    5
    """"""
    # Sort the data in ascending order
    data = sorted(data)
    n = len(data)
    
    # Raise an error if the dataset is empty
    if n == 0:
        raise StatisticsError(""no median for empty data"")
    
    # Return the middle value if odd number of elements
    return data[n // 2]
","The median_high function calculates the ""high"" median of a dataset, which is defined as the larger of the two middle values when the dataset has an even number of elements. Similar to the other median functions, it first sorts the data and checks for an empty dataset. If the number of elements is odd, it returns the middle element. For even-sized datasets, it returns the larger of the two middle values, providing a central tendency measure biased towards the upper middle.

"
"def median_grouped(data, interval=1.0):
    # Sort the input data to ensure it's in increasing order
    data = sorted(data)
    n = len(data)
    
    # Raise an error if the data is empty
    if not n:
        raise StatisticsError(""no median for empty data"")

    # Find the median's approximate midpoint value
    x = data[n // 2]

    # Use binary search to find the range of values where the median lies
    i = bisect_left(data, x)
    j = bisect_right(data, x, lo=i)

    # Ensure interval is a float, raising error if not
    try:
        interval = float(interval)
        x = float(x)
    except ValueError:
        raise TypeError(f'Value cannot be converted to a float')

    # Compute lower limit of the median class interval
    L = x - interval / 2.0    
    # Cumulative frequency before the median class
    cf = i                    
    # Frequency of the median class
    f = j - i                 
    # Return the interpolated median value
    return L + interval * (n / 2 - cf) / f
","The median_grouped function estimates the median of grouped numeric data, where the data is binned around the midpoints of fixed-width intervals. The function is useful for estimating the median when the exact data points are not available, such as in demographic statistics or frequency distributions. It assumes that data points are uniformly distributed within the intervals. The function takes in two parameters: the data (which should consist of midpoints of the bins) and the interval width, which is the width of each bin. The function uses interpolation based on the position of the median within its bin to estimate its value."
"def mode(data):
    # Count the frequency of each element in the data using Counter
    pairs = Counter(iter(data)).most_common(1)
    try:
        # Return the most common element (mode)
        return pairs[0][0]
    except IndexError:
        # Raise error if the data is empty
        raise StatisticsError('no mode for empty data') from None
","The mode function returns the most common data point (the mode) from a collection of discrete or nominal data. It assumes that the data is discrete and may be either numeric or non-numeric (e.g., strings). If there are multiple modes, the function returns the first one it encounters. If the data is empty, a StatisticsError is raised. This function is typically used to determine the most frequent value in a dataset."
"def multimode(data):
    # Create a Counter object to count the frequency of each data element
    counts = Counter(iter(data))
    if not counts:
        # Return an empty list if the data is empty
        return []
    # Find the maximum count (most frequent occurrences)
    maxcount = max(counts.values())
    # Return a list of all values that occur with the maximum frequency
    return [value for value, count in counts.items() if count == maxcount]
","The multimode function identifies all modes (most frequent values) in a dataset and returns them in a list. Unlike mode, which returns a single mode, multimode accounts for multiple modes and can return more than one value if several data points have the same maximum frequency. If the dataset is empty, it returns an empty list. This function is useful when analyzing data with multiple frequently occurring values."
"def variance(data, xbar=None):
    # Calculate the sum of squared differences and other statistics using _ss function
    T, ss, c, n = _ss(data, xbar)
    if n < 2:
        # Raise an error if there are fewer than two data points
        raise StatisticsError('variance requires at least two data points')
    # Return the sample variance by dividing the sum of squares by (n - 1)
    return _convert(ss / (n - 1), T)
","The variance function calculates the sample variance of a dataset, which measures the spread of data points around the mean. It is used when the data represents a sample from a population. The variance is calculated as the average of the squared differences between each data point and the sample mean, normalized by the number of data points minus one (Bessels correction). The function optionally accepts a precomputed mean (xbar) to avoid recalculating it, which can be useful for efficiency in large datasets. This function supports both decimal and fractional numbers."
"def pvariance(data, mu=None):
    """"""Return the population variance of ``data``.""""""
    # Call _ss to get necessary values such as total type, sum of squares, count, and number of items.
    T, ss, c, n = _ss(data, mu)
    
    # Raise an error if there are no data points
    if n < 1:
        raise StatisticsError('pvariance requires at least one data point')
    
    # Calculate population variance by dividing sum of squares by the number of items
    return _convert(ss / n, T)
","The pvariance function calculates the population variance, a measure of the spread or dispersion of a set of data points. If a mean (mu) is provided, it is used in the calculations; otherwise, the mean is computed internally. The function handles different numeric types, including Decimal and Fraction, and returns the variance by dividing the sum of squared differences from the mean by the total number of elements. It raises a StatisticsError if there are fewer than one data point."
"def stdev(data, xbar=None):
    """"""Return the square root of the sample variance.""""""
    # Call _ss to get necessary values such as total type, sum of squares, count, and number of items.
    T, ss, c, n = _ss(data, xbar)
    
    # Raise an error if there are fewer than 2 data points
    if n < 2:
        raise StatisticsError('stdev requires at least two data points')
    
    # Calculate sample variance
    mss = ss / (n - 1)
    
    # Return the square root of the sample variance based on the type of data (Decimal or float)
    if issubclass(T, Decimal):
        return _decimal_sqrt_of_frac(mss.numerator, mss.denominator)
    return _float_sqrt_of_frac(mss.numerator, mss.denominator)
","The stdev function calculates the sample standard deviation, which measures the amount of variation or dispersion in a sample dataset. It computes the sample variance first by dividing the sum of squared differences from the sample mean by the number of data points minus one (Bessel's correction). Then, it returns the square root of the sample variance. The function supports Decimal and Fraction types, ensuring precision when dealing with such data types."
"def pstdev(data, mu=None):
    """"""Return the square root of the population variance.""""""
    # Call _ss to get necessary values such as total type, sum of squares, count, and number of items.
    T, ss, c, n = _ss(data, mu)
    
    # Raise an error if there are no data points
    if n < 1:
        raise StatisticsError('pstdev requires at least one data point')
    
    # Calculate population variance
    mss = ss / n
    
    # Return the square root of the population variance based on the type of data (Decimal or float)
    if issubclass(T, Decimal):
        return _decimal_sqrt_of_frac(mss.numerator, mss.denominator)
    return _float_sqrt_of_frac(mss.numerator, mss.denominator)
","The pstdev function calculates the population standard deviation, which quantifies the amount of variation or dispersion in a dataset assuming the dataset represents an entire population. It calculates the population variance by dividing the sum of squared differences from the population mean by the total number of data points. It then returns the square root of the population variance. The function works with Decimal and Fraction types for precise calculations."
"def covariance(x, y, /):
    """"""Covariance: Return the sample covariance of two inputs.""""""
    # Check that both inputs have the same number of data points
    n = len(x)
    if len(y) != n:
        raise StatisticsError('covariance requires that both inputs have same number of data points')
    
    # Raise an error if there are fewer than 2 data points
    if n < 2:
        raise StatisticsError('covariance requires at least two data points')
    
    # Compute the means of x and y
    xbar = fsum(x) / n
    ybar = fsum(y) / n
    
    # Calculate the sum of products of differences from the means
    sxy = sumprod((xi - xbar for xi in x), (yi - ybar for yi in y))
    
    # Return the covariance, which is the sum of products divided by the number of data points minus 1
    return sxy / (n - 1)
","The covariance function calculates the sample covariance, which is a measure of the joint variability between two datasets. It computes how much two variables change together by averaging the product of their deviations from their respective means. The covariance is calculated by summing the product of the deviations of corresponding elements from each dataset and dividing by the number of data points minus one. If the datasets are not of equal length or if there are fewer than two data points, a StatisticsError is raised."
"def correlation(x, y, /, *, method='linear'):
    """"""Pearson's correlation coefficient: Return the correlation coefficient for two inputs.""""""
    # Ensure both datasets have the same number of elements
    n = len(x)
    if len(y) != n:
        raise StatisticsError('correlation requires that both inputs have same number of data points')
    
    # Raise an error if there are fewer than 2 data points
    if n < 2:
        raise StatisticsError('correlation requires at least two data points')
    
    # Validate the method argument
    if method not in {'linear', 'ranked'}:
        raise ValueError(f'Unknown method: {method!r}')
    
    # Compute the rankings for Spearman's method
    if method == 'ranked':
        start = (n - 1) / -2            # Center rankings around zero
        x = _rank(x, start=start)
        y = _rank(y, start=start)

    # For linear correlation, compute the mean deviations
    else:
        xbar = fsum(x) / n
        ybar = fsum(y) / n
        x = [xi - xbar for xi in x]
        y = [yi - ybar for yi in y]

    # Calculate the sum of products and sum of squared deviations for both datasets
    sxy = sumprod(x, y)
    sxx = sumprod(x, x)
    syy = sumprod(y, y)

    # Return the correlation coefficient, or raise an error if there is a zero division
    try:
        return sxy / _sqrtprod(sxx, syy)
    except ZeroDivisionError:
        raise StatisticsError('at least one of the inputs is constant')
","The correlation function computes the Pearson correlation coefficient, which measures the strength and direction of a linear relationship between two datasets. If the method parameter is set to 'ranked', it computes the Spearman rank correlation coefficient, which measures monotonic relationships using ranks rather than raw data values. The function first ensures both datasets are of the same length and contain at least two data points. It then calculates either the Pearson or Spearman correlation depending on the method, returning a coefficient between -1 and 1. If the datasets are constant, leading to a zero division error, it raises a StatisticsError."
"def linear_regression(x, y, /, *, proportional=False):
    """"""Slope and intercept for simple linear regression.""""""
    
    # Calculate the number of data points
    n = len(x)
    
    # Ensure the input lists have the same number of elements
    if len(y) != n:
        raise StatisticsError('linear regression requires that both inputs have same number of data points')
    
    # Ensure there are at least two data points
    if n < 2:
        raise StatisticsError('linear regression requires at least two data points')

    if not proportional:
        # Calculate the mean of x and y
        xbar = fsum(x) / n
        ybar = fsum(y) / n
        # Center the data by subtracting the mean
        x = [xi - xbar for xi in x]  # List is used for multiple iterations
        y = (yi - ybar for yi in y)  # Generator for a single iteration

    # Calculate the sum of products for covariance and variance
    sxy = sumprod(x, y) + 0.0        # Add zero to coerce result to a float
    sxx = sumprod(x, x)

    try:
        # Compute the slope (covariance / variance)
        slope = sxy / sxx   
    except ZeroDivisionError:
        raise StatisticsError('x is constant')  # If variance is zero, x is constant

    # Compute the intercept (zero if proportional, else calculated)
    intercept = 0.0 if proportional else ybar - slope * xbar

    # Return the slope and intercept as a named tuple
    return LinearRegression(slope=slope, intercept=intercept)
","This function performs simple linear regression to estimate the slope and intercept of the line that best fits the relationship between an independent variable x and a dependent variable y. It calculates the parameters using the Ordinary Least Squares method. If proportional=True, the function assumes a proportional relationship (i.e., the line passes through the origin, so intercept = 0). The function returns the slope and intercept as a named tuple (LinearRegression)."
"def register(*kernels):
    """"""Load the kernel's pdf, cdf, invcdf, and support into _kernel_specs.""""""
    
    # Define the decorator function to register kernel specifications
    def deco(builder):
        # Generate kernel specifications and associate them with kernels
        spec = dict(zip(('pdf', 'cdf', 'invcdf', 'support'), builder()))
        for kernel in kernels:
            _kernel_specs[kernel] = spec
        return builder
    
    return deco
","The register function is a decorator that registers kernel specifications (pdf, cdf, invcdf, and support) into a global dictionary _kernel_specs. It takes one or more kernel names and associates each with the kernel functions returned by the builder function, which defines the kernel's probability density function (pdf), cumulative distribution function (cdf), inverse cdf (invcdf), and support (the range of values for which the kernel is defined)."
"def kde(data, h, kernel='normal', *, cumulative=False):
    """"""Kernel Density Estimation: Create a continuous probability density
    function or cumulative distribution function from discrete samples.""""""
    
    # Get the number of data points
    n = len(data)
    
    # Raise an error if the data sequence is empty
    if not n:
        raise StatisticsError('Empty data sequence')

    # Check if the data elements are numerical
    if not isinstance(data[0], (int, float)):
        raise TypeError('Data sequence must contain ints or floats')

    # Check if the bandwidth is valid (positive)
    if h <= 0.0:
        raise StatisticsError(f'Bandwidth h must be positive, not {h=!r}')

    # Get the kernel specifications
    kernel_spec = _kernel_specs.get(kernel)
    if kernel_spec is None:
        raise StatisticsError(f'Unknown kernel name: {kernel!r}')
    K = kernel_spec['pdf']
    W = kernel_spec['cdf']
    support = kernel_spec['support']

    # If the kernel has no support, use all data points
    if support is None:
        def pdf(x):
            # Estimate the probability density function by summing kernel values
            return sum(K((x - x_i) / h) for x_i in data) / (len(data) * h)

        def cdf(x):
            # Estimate the cumulative distribution function by summing kernel values
            return sum(W((x - x_i) / h) for x_i in data) / len(data)
    else:
        # If the kernel has support, only use data points within the bandwidth range
        sample = sorted(data)
        bandwidth = h * support

        def pdf(x):
            # Find the relevant data points within the bandwidth
            nonlocal n, sample
            if len(data) != n:
                sample = sorted(data)
                n = len(data)
            i = bisect_left(sample, x - bandwidth)
            j = bisect_right(sample, x + bandwidth)
            supported = sample[i : j]
            return sum(K((x - x_i) / h) for x_i in supported) / (n * h)

        def cdf(x):
            # CDF estimation considering the bandwidth and kernel
            nonlocal n, sample
            if len(data) != n:
                sample = sorted(data)
                n = len(data)
            i = bisect_left(sample, x - bandwidth)
            j = bisect_right(sample, x + bandwidth)
            supported = sample[i : j]
            return sum((W((x - x_i) / h) for x_i in supported), i) / n

    # Return the cumulative distribution function or probability density function
    if cumulative:
        cdf.__doc__ = f'CDF estimate with {h=!r} and {kernel=!r}'
        return cdf
    else:
        pdf.__doc__ = f'PDF estimate with {h=!r} and {kernel=!r}'
        return pdf
","The kde function performs Kernel Density Estimation (KDE) to create a continuous estimate of a probability density function (PDF) or cumulative distribution function (CDF) based on a set of discrete data points. The function uses a kernel function to smooth the data, with the degree of smoothing controlled by the bandwidth parameter h. The available kernels include 'normal', 'logistic', 'sigmoid', and others. If cumulative=True, the function returns the CDF; otherwise, it returns the PDF. The function also handles kernels with and without support (i.e., kernels that give weight to data points within a certain range)."
"def kde_random(data, h, kernel='normal', *, seed=None):
    """"""Return a function that makes a random selection from the estimated
    probability density function created by kde(data, h, kernel).""""""
    
    n = len(data)  # Get the length of the input data
    if not n:  # Check if data is empty
        raise StatisticsError('Empty data sequence')  # Raise error if data is empty
    
    if not isinstance(data[0], (int, float)):  # Ensure data contains only numerical values
        raise TypeError('Data sequence must contain ints or floats')  # Raise error if data contains non-numeric values
    
    if h <= 0.0:  # Check if bandwidth h is positive
        raise StatisticsError(f'Bandwidth h must be positive, not {h=!r}')  # Raise error if h is non-positive

    kernel_spec = _kernel_specs.get(kernel)  # Retrieve the kernel specification from a predefined dictionary
    if kernel_spec is None:  # Check if the provided kernel name is valid
        raise StatisticsError(f'Unknown kernel name: {kernel!r}')  # Raise error if the kernel name is not found
    invcdf = kernel_spec['invcdf']  # Get the inverse cumulative distribution function from the kernel specification
    
    prng = _random.Random(seed)  # Initialize a random number generator with the given seed (or default)
    random = prng.random  # Assign the random function for generating random numbers
    choice = prng.choice  # Assign the choice function for selecting random values from data

    def rand():  # Define a random selection function
        return choice(data) + h * invcdf(random())  # Return a value sampled from the data and adjusted by the kernel

    rand.__doc__ = f'Random KDE selection with {h=!r} and {kernel=!r}'  # Set the documentation for the rand function

    return rand  # Return the rand function to generate random selections
","The kde_random function is used to generate random selections based on a Kernel Density Estimate (KDE) from a given dataset. It takes the input data and bandwidth (h), and optionally a kernel type (defaulting to 'normal') and a seed for reproducibility. The function ensures that the data is numeric and non-empty, and it checks for valid kernel names and bandwidth. It retrieves the inverse cumulative distribution function (invcdf) corresponding to the selected kernel. A random number generator (PRNG) is initialized with the provided seed, and a choice function is used to make selections from the data. The returned rand function uses the choice and kernel functions to sample from the KDE and adjust the selection by the bandwidth. The function enables reproducible random selections for statistical analysis or simulations."
"def quantiles(data, *, n=4, method='exclusive'):
    """"""Divide *data* into *n* continuous intervals with equal probability.
    Returns a list of (n - 1) cut points separating the intervals.""""""
    
    if n < 1:  # Ensure n is at least 1
        raise StatisticsError('n must be at least 1')  # Raise error if n is less than 1

    data = sorted(data)  # Sort the input data

    ld = len(data)  # Get the length of the sorted data
    if ld < 2:  # Check if the data has fewer than 2 elements
        if ld == 1:  # If only one element is present, return repeated values
            return data * (n - 1)
        raise StatisticsError('must have at least one data point')  # Raise error if data is empty

    if method == 'inclusive':  # Handle the 'inclusive' method
        m = ld - 1  # Set m to one less than the length of the data
        result = []  # Initialize an empty list to store the results
        for i in range(1, n):  # Loop over the range of n intervals
            j, delta = divmod(i * m, n)  # Compute the position and interpolation factor
            interpolated = (data[j] * (n - delta) + data[j + 1] * delta) / n  # Interpolate the value
            result.append(interpolated)  # Append the interpolated value to the result list
        return result  # Return the list of quantiles

    if method == 'exclusive':  # Handle the 'exclusive' method
        m = ld + 1  # Set m to one more than the length of the data
        result = []  # Initialize an empty list to store the results
        for i in range(1, n):  # Loop over the range of n intervals
            j = i * m // n  # Compute the index for the current interval
            j = 1 if j < 1 else ld-1 if j > ld-1 else j  # Clamp j to valid index range
            delta = i * m - j * n  # Compute the interpolation factor
            interpolated = (data[j - 1] * (n - delta) + data[j] * delta) / n  # Interpolate the value
            result.append(interpolated)  # Append the interpolated value to the result list
        return result  # Return the list of quantiles

    raise ValueError(f'Unknown method: {method!r}')  # Raise error if an invalid method is specified
","The quantiles function divides a dataset into n intervals of equal probability, producing n-1 quantiles that mark the boundaries of these intervals. The function takes two main arguments: the data to be divided and n, the number of intervals (defaulting to 4 for quartiles). The data is first sorted, and if fewer than two data points are present, an appropriate error is raised or the data is repeated. The function supports two methods for computing quantiles: 'inclusive' and 'exclusive'. The 'inclusive' method treats the data as population data, including the minimum and maximum values as the 0th and 100th percentiles, respectively. The 'exclusive' method excludes the endpoints, applying interpolation between the sorted data points. It returns the computed quantiles, which can be used for further statistical analysis or as thresholds for classification tasks."
"class NormalDist:
    ""Normal distribution of a random variable""
    # https://en.wikipedia.org/wiki/Normal_distribution
    # https://en.wikipedia.org/wiki/Variance#Properties

    __slots__ = {
        '_mu': 'Arithmetic mean of a normal distribution',
        '_sigma': 'Standard deviation of a normal distribution',
    }

    def __init__(self, mu=0.0, sigma=1.0):
        """"""Initializes the normal distribution with the specified mean (mu) and standard deviation (sigma).""""""
        if sigma < 0.0:
            raise StatisticsError('sigma must be non-negative')  # Raise error if standard deviation is negative
        self._mu = float(mu)  # Set the mean of the distribution
        self._sigma = float(sigma)  # Set the standard deviation of the distribution

    @classmethod
    def from_samples(cls, data):
        """"""Creates a normal distribution instance from sample data by calculating the mean and standard deviation.""""""
        return cls(*_mean_stdev(data))  # Return a NormalDist instance using the sample mean and stdev

    def samples(self, n, *, seed=None):
        """"""Generates *n* random samples from the normal distribution.""""""
        rnd = random.random if seed is None else random.Random(seed).random  # Use seed if provided for reproducibility
        inv_cdf = _normal_dist_inv_cdf  # Use the inverse CDF function to generate random samples
        mu = self._mu  # Retrieve the mean
        sigma = self._sigma  # Retrieve the standard deviation
        return [inv_cdf(rnd(), mu, sigma) for _ in repeat(None, n)]  # Generate and return the samples

    def pdf(self, x):
        """"""Calculates the probability density function (PDF) for a given value x.""""""
        variance = self._sigma * self._sigma  # Compute the variance (sigma^2)
        if not variance:
            raise StatisticsError('pdf() not defined when sigma is zero')  # Raise error if standard deviation is zero
        diff = x - self._mu  # Compute the difference from the mean
        return exp(diff * diff / (-2.0 * variance)) / sqrt(tau * variance)  # Return the PDF value

    def cdf(self, x):
        """"""Calculates the cumulative distribution function (CDF) for a given value x.""""""
        if not self._sigma:
            raise StatisticsError('cdf() not defined when sigma is zero')  # Raise error if standard deviation is zero
        return 0.5 * (1.0 + erf((x - self._mu) / (self._sigma * _SQRT2)))  # Return the CDF value

    def inv_cdf(self, p):
        """"""Calculates the inverse CDF (percent point function) for a given probability p.""""""
        if p <= 0.0 or p >= 1.0:
            raise StatisticsError('p must be in the range 0.0 < p < 1.0')  # Raise error if probability is out of bounds
        return _normal_dist_inv_cdf(p, self._mu, self._sigma)  # Return the inverse CDF value

    def quantiles(self, n=4):
        """"""Calculates the quantiles for the normal distribution, dividing it into n equal intervals.""""""
        return [self.inv_cdf(i / n) for i in range(1, n)]  # Return a list of quantile values

    def overlap(self, other):
        """"""Calculates the overlapping coefficient (OVL) between two normal distributions.""""""
        if not isinstance(other, NormalDist):
            raise TypeError('Expected another NormalDist instance')  # Raise error if the other object is not a NormalDist
        X, Y = self, other
        if (Y._sigma, Y._mu) < (X._sigma, X._mu):  # Sort distributions to ensure commutativity
            X, Y = Y, X
        X_var, Y_var = X.variance, Y.variance  # Retrieve the variances
        if not X_var or not Y_var:
            raise StatisticsError('overlap() not defined when sigma is zero')  # Raise error if sigma is zero
        dv = Y_var - X_var  # Compute the difference in variances
        dm = fabs(Y._mu - X._mu)  # Compute the absolute difference in means
        if not dv:
            return 1.0 - erf(dm / (2.0 * X._sigma * _SQRT2))  # Return overlap if variances are equal
        a = X._mu * Y_var - Y._mu * X_var
        b = X._sigma * Y._sigma * sqrt(dm * dm + dv * log(Y_var / X_var))
        x1 = (a + b) / dv  # Compute the first root
        x2 = (a - b) / dv  # Compute the second root
        return 1.0 - (fabs(Y.cdf(x1) - X.cdf(x1)) + fabs(Y.cdf(x2) - X.cdf(x2)))  # Return the overlapping area

    def zscore(self, x):
        """"""Calculates the Z-score for a given value x.""""""
        if not self._sigma:
            raise StatisticsError('zscore() not defined when sigma is zero')  # Raise error if standard deviation is zero
        return (x - self._mu) / self._sigma  # Return the Z-score

    @property
    def mean(self):
        """"""Returns the arithmetic mean of the normal distribution.""""""
        return self._mu

    @property
    def median(self):
        """"""Returns the median of the normal distribution, which is the same as the mean.""""""
        return self._mu

    @property
    def mode(self):
        """"""Returns the mode of the normal distribution, which is the same as the mean.""""""
        return self._mu

    @property
    def stdev(self):
        """"""Returns the standard deviation of the normal distribution.""""""
        return self._sigma

    @property
    def variance(self):
        """"""Returns the variance of the normal distribution (standard deviation squared).""""""
        return self._sigma * self._sigma

    def __add__(x1, x2):
        """"""Adds a constant or another NormalDist instance to the current instance.""""""
        if isinstance(x2, NormalDist):
            return NormalDist(x1._mu + x2._mu, hypot(x1._sigma, x2._sigma))  # Add the means and variances
        return NormalDist(x1._mu + x2, x1._sigma)  # Add the constant to the mean

    def __sub__(x1, x2):
        """"""Subtracts a constant or another NormalDist instance from the current instance.""""""
        if isinstance(x2, NormalDist):
            return NormalDist(x1._mu - x2._mu, hypot(x1._sigma, x2._sigma))  # Subtract the means and variances
        return NormalDist(x1._mu - x2, x1._sigma)  # Subtract the constant from the mean

    def __mul__(x1, x2):
        """"""Multiplies the mean and standard deviation by a constant.""""""
        return NormalDist(x1._mu * x2, x1._sigma * fabs(x2))  # Scale both mean and standard deviation

    def __truediv__(x1, x2):
        """"""Divides the mean and standard deviation by a constant.""""""
        return NormalDist(x1._mu / x2, x1._sigma / fabs(x2))  # Scale both mean and standard deviation

    def __pos__(x1):
        """"""Returns a copy of the current NormalDist instance.""""""
        return NormalDist(x1._mu, x1._sigma)

    def __neg__(x1):
        """"""Negates the mean while keeping the standard deviation the same.""""""
        return NormalDist(-x1._mu, x1._sigma)

    __radd__ = __add__

    def __rsub__(x1, x2):
        """"""Subtracts a NormalDist from a constant or another NormalDist.""""""
        return -(x1 - x2)  # Reverse the subtraction operation

    __rmul__ = __mul__

    def __eq__(x1, x2):
        """"""Checks if two NormalDist instances are equal based on their mean and standard deviation.""""""
        if not isinstance(x2, NormalDist):
            return NotImplemented
        return x1._mu == x2._mu and x1._sigma == x2._sigma  # Compare means and standard deviations

    def __hash__(self):
        """"""Hashes a NormalDist object based on its mean and standard deviation.""""""
        return hash((self._mu, self._sigma))  # Use a tuple of mean and standard deviation for hashing

    def __repr__(self):
        """"""Returns a string representation of the NormalDist object.""""""
        return f'{type(self).__name__}(mu={self._mu!r}, sigma={self._sigma!r})'

    def __getstate__(self):
        """"""Returns the state of the NormalDist object for serialization.""""""
        return self._mu, self._sigma

    def __setstate__(self, state):
        """"""Restores the state of the NormalDist object from serialization.""""""
        self._mu, self._sigma = state
","The NormalDist class models a normal (Gaussian) distribution, representing a continuous probability distribution with a mean (mu) and a standard deviation (sigma). It provides a variety of methods for statistical operations, such as generating samples (samples), computing the probability density function (pdf), the cumulative distribution function (cdf), and the inverse cumulative distribution function (inv_cdf). The class also allows for computing quantiles and overlap between two normal distributions (quantiles, overlap), as well as calculating the z-score for a value (zscore). The mean, median, mode, standard deviation, and variance of the distribution can be accessed through properties, while operations like addition, subtraction, multiplication, and division are supported for both constants and other instances of NormalDist. These operations adjust the mean and standard deviation accordingly. The class also supports comparison, hashing, and serialization, enabling storage and retrieval of instances. Each function is equipped with detailed logic to handle statistical calculations and edge cases, ensuring robust behavior when performing operations on normal distributions."
"def _sum(data):
    """"""_sum(data) -> (type, sum, count)

    Return a high-precision sum of the given numeric data as a fraction,
    together with the type to be converted to and the count of items.

    Examples
    --------
    >>> _sum([3, 2.25, 4.5, -0.5, 0.25])
    (<class 'float'>, Fraction(19, 2), 5)

    >>> _sum([1e50, 1, -1e50] * 1000)
    (<class 'float'>, Fraction(1000, 1), 3000)

    >>> from fractions import Fraction as F
    >>> _sum([F(2, 3), F(7, 5), F(1, 4), F(5, 6)])
    (<class 'fractions.Fraction'>, Fraction(63, 20), 4)

    >>> from decimal import Decimal as D
    >>> data = [D(""0.1375""), D(""0.2108""), D(""0.3061""), D(""0.0419"")]
    >>> _sum(data)
    (<class 'decimal.Decimal'>, Fraction(6963, 10000), 4)
    """"""
    count = 0  # Initialize counter for data items
    types = set()  # Create a set to track different data types
    types_add = types.add  # Local reference for set's add method
    partials = {}  # Dictionary to hold partial sums based on denominators
    partials_get = partials.get  # Local reference for dictionary's get method

    for typ, values in groupby(data, type):  # Group data by type
        types_add(typ)  # Add type to the types set
        for n, d in map(_exact_ratio, values):  # Convert each value to its exact ratio (numerator/denominator)
            count += 1  # Increment count for each value
            partials[d] = partials_get(d, 0) + n  # Add the numerator to the corresponding denominator's partial sum

    if None in partials:  # Check if a NAN or INF is encountered in partial sums
        total = partials[None]  # Use the NAN/INF total
        assert not _isfinite(total)  # Assert that total is not finite
    else:
        # Sum all the partial sums using Fraction for exact precision
        total = sum(Fraction(n, d) for d, n in partials.items())

    T = reduce(_coerce, types, int)  # Coerce types to a common type, with a fallback to int
    return (T, total, count)  # Return the common type, total sum, and count of items
","The _sum function computes the high-precision sum of a list of numerical values (data) while handling mixed data types (e.g., int, float, Fraction, Decimal). It ensures that the sum is calculated in the form of a Fraction to maintain precision and avoids round-off errors typically seen with floating-point arithmetic. It does so by first grouping the data by type, then using the _exact_ratio function to convert each value to a fraction, and accumulating the results in a dictionary of partial sums. It handles edge cases such as NaN or Infinity by isolating those special cases. Finally, it returns the common numeric type (after coercion), the calculated sum as a Fraction, and the count of items."
"def _ss(data, c=None):
    """"""Return the exact mean and sum of square deviations of sequence data.

    Calculations are done in a single pass, allowing the input to be an iterator.

    If given *c* is used the mean; otherwise, it is calculated from the data.
    Use the *c* argument with care, as it can lead to garbage results.
    """"""
    if c is not None:  # If a mean 'c' is provided
        T, ssd, count = _sum((d := x - c) * d for x in data)  # Calculate squared deviations from 'c'
        return (T, ssd, c, count)  # Return type, sum of squared deviations, provided mean, and count

    count = 0  # Initialize counter for data items
    types = set()  # Set to track the types of data
    types_add = types.add  # Local reference for add method
    sx_partials = defaultdict(int)  # Dictionary to store partial sums of the values (x)
    sxx_partials = defaultdict(int)  # Dictionary to store partial sums of the squared values (x^2)

    for typ, values in groupby(data, type):  # Group data by type
        types_add(typ)  # Add type to the types set
        for n, d in map(_exact_ratio, values):  # Convert each value to its exact ratio (numerator/denominator)
            count += 1  # Increment count
            sx_partials[d] += n  # Accumulate values for 'x'
            sxx_partials[d] += n * n  # Accumulate values for 'x^2'

    if not count:  # If no data is available
        ssd = c = Fraction(0)  # Set both sum of squared deviations and mean to 0
    elif None in sx_partials:  # If a NAN/INF is encountered in the data
        ssd = c = sx_partials[None]  # Use the NAN/INF total
        assert not _isfinite(ssd)  # Assert that ssd is not finite
    else:
        sx = sum(Fraction(n, d) for d, n in sx_partials.items())  # Sum the 'x' values
        sxx = sum(Fraction(n, d*d) for d, n in sxx_partials.items())  # Sum the 'x^2' values
        ssd = (count * sxx - sx * sx) / count  # Calculate the sum of squared deviations
        c = sx / count  # Calculate the mean

    T = reduce(_coerce, types, int)  # Coerce types to a common type, with a fallback to int
    return (T, ssd, c, count)  # Return type, sum of squared deviations, mean, and count
","The _ss function calculates the sum of squared deviations (SSD) and the mean of a sequence of numeric data (data) with high precision. It does so in a single pass through the data for efficiency, using a generator expression to compute the squared differences between data points and the provided mean (c). If no mean is given, the function computes the mean from the data. The function also tracks and sums the values of x and x^2 for each unique data type. The final result includes the computed mean, SSD, and the type after coercing all data into a common numeric type."
"def _isfinite(x):
    try:
        return x.is_finite()  # Likely a Decimal.
    except AttributeError:
        return math.isfinite(x)  # Coerces to float first.
","The _isfinite function checks whether a given value x is finite. It first attempts to use the is_finite() method, which is likely available for Decimal objects. If the attribute is not found (i.e., for non-Decimal objects), it falls back to using Python's math.isfinite() function, which works with floating-point values. This function ensures that the value is neither NaN nor Infinity."
"def _coerce(T, S):
    """"""Coerce types T and S to a common type, or raise TypeError.

    Coercion rules are currently an implementation detail. See the CoerceTest
    test class in test_statistics for details.
    """"""
    assert T is not bool, ""initial type T is bool""  # Ensure T is not bool
    if T is S:  return T  # If types are the same, no need to coerce
    if S is int or S is bool:  return T  # Coerce to T if S is int or bool
    if T is int:  return S  # Coerce to S if T is int
    if issubclass(S, T):  return S  # Coerce to subclass type if one is a subclass of the other
    if issubclass(T, S):  return T  # Coerce to subclass type if the other is a subclass of T
    if issubclass(T, int):  return S  # If T is int, coerce to S
    if issubclass(S, int):  return T  # If S is int, coerce to T
    if issubclass(T, Fraction) and issubclass(S, float):
        return S  # Coerce to float if T is Fraction and S is float
    if issubclass(T, float) and issubclass(S, Fraction):
        return T  # Coerce to float if T is float and S is Fraction
    msg = ""don't know how to coerce %s and %s""
    raise TypeError(msg % (T.__name__, S.__name__))  # Raise error if coercion is not possible
","The _coerce function is responsible for coercing two different numeric types (T and S) to a common type, following a set of predefined rules. These rules prioritize type compatibility, ensuring that types are coerced in a manner that preserves the data's integrity. If the coercion is not possible (e.g., incompatible types), the function raises a TypeError. This function is critical for handling operations between different numeric types like int, float, Fraction, and Decimal."
"def _exact_ratio(x):
    """"""Return Real number x to exact (numerator, denominator) pair.

    >>> _exact_ratio(0.25)
    (1, 4)

    x is expected to be an int, Fraction, Decimal or float.
    """"""
    try:
        return x.as_integer_ratio()  # Return exact ratio for Decimal or float
    except AttributeError:
        pass  # Ignore if the method doesn't exist for the type

    except (OverflowError, ValueError):  # Handle NAN/INF cases
        assert not _isfinite(x)  # Ensure x is not finite
        return (x, None)  # Return x as the numerator with denominator as None

    try:
        # Return the numerator and denominator for integer types (or subclasses)
        return (x.numerator, x.denominator)
    except AttributeError:
        msg = f""can't convert type '{type(x).__name__}' to numerator/denominator""
        raise TypeError(msg)  # Raise error if the type cannot be converted to exact ratio
","The _exact_ratio function converts a given number x (which can be an int, Fraction, Decimal, or float) into its exact numerator and denominator representation. If x is a Decimal or float, it attempts to use the as_integer_ratio() method. For other types, it checks if the type is an integer or subclass and returns its numerator and denominator attributes. In cases of NaN or Infinity, it handles the error and returns None for the denominator. If x cannot be converted to a ratio, it raises a TypeError."
"def _convert(value, T):
    """"""Convert value to given numeric type T.""""""
    if type(value) is T:
        # If the value is already of the required type T, return it directly.
        # This handles cases where value is already a Fraction, or where value is NaN or INF (Decimal or float).
        return value

    if issubclass(T, int) and value.denominator != 1:
        # If T is a subclass of int and the value is a Fraction (with denominator not equal to 1), 
        # convert value to float as it can't be represented as an integer.
        T = float

    try:
        # Attempt to convert the value to the desired type T.
        return T(value)
    except TypeError:
        if issubclass(T, Decimal):
            # If conversion to Decimal fails, convert the numerator and denominator of the Fraction
            # to Decimal and return the result.
            return T(value.numerator) / T(value.denominator)
        else:
            # If conversion fails for other types, raise the exception.
            raise
","The _convert function is designed to convert a given value to a specified numeric type T. If the value is already of the target type T, it is returned as-is. If the value is a Fraction and T is a subclass of int, the function converts the value to float if the fraction has a non-unit denominator. The function then attempts to cast the value to the desired type, and if this fails, it checks if the target type is Decimal. In this case, it manually converts the Fraction's numerator and denominator to Decimal. If neither approach works, a TypeError is raised."
"def _fail_neg(values, errmsg='negative value'):
    """"""Iterate over values, failing if any are less than zero.""""""
    for x in values:
        # For each value in the list, check if it's less than zero.
        if x < 0:
            # If a negative value is found, raise a StatisticsError with the specified error message.
            raise StatisticsError(errmsg)
        # Yield the value if it's non-negative.
        yield x
","The _fail_neg function iterates over a sequence of values and checks if any of them are negative. If a negative value is encountered, the function raises a StatisticsError with a customizable error message (errmsg). If no negative values are found, the function yields the non-negative values one by one. This function is useful for validating datasets where negative values are not allowed.

"
"def _rank(data, /, *, key=None, reverse=False, ties='average', start=1) -> list[float]:
    """"""Rank order a dataset. The lowest value has rank 1.""""""
    if ties != 'average':
        # Raise an error if an unknown tie resolution method is provided.
        raise ValueError(f'Unknown tie resolution method: {ties!r}')
    if key is not None:
        # If a key function is provided, apply it to each element of the data.
        data = map(key, data)
    
    # Sort the data along with their original indices, optionally in reverse order.
    val_pos = sorted(zip(data, count()), reverse=reverse)
    i = start - 1
    result = [0] * len(val_pos)
    
    # Group the sorted values by their value (for handling ties).
    for _, g in groupby(val_pos, key=itemgetter(0)):
        group = list(g)
        size = len(group)
        rank = i + (size + 1) / 2  # Compute the average rank for tied values.
        
        # Assign the computed rank to all items in the group.
        for value, orig_pos in group:
            result[orig_pos] = rank
        
        i += size  # Move to the next group of values.
    
    return result
","The _rank function ranks a dataset where the lowest value gets rank 1, and tied values receive the same average rank. The function supports ranking in reverse order (where the highest value has rank 1) and allows for a key function to extract ranking criteria. It handles ties by assigning the average rank to the tied values. If a non-average tie resolution method is specified, it raises a ValueError. The function returns the ranks as a list of floats, which can be used for various ranking scenarios, including sorting data by a custom key."
"def _integer_sqrt_of_frac_rto(n: int, m: int) -> int:
    """"""Square root of n/m, rounded to the nearest integer using round-to-odd.""""""
    a = math.isqrt(n // m)  # Compute the integer square root of n/m.
    return a | (a*a*m != n)  # Use round-to-odd to adjust the result if necessary.
","The _integer_sqrt_of_frac_rto function computes the square root of a fraction n/m, rounding the result to the nearest integer using a ""round-to-odd"" approach. It first calculates the integer square root of the fraction n/m using the math.isqrt function. Then, it adjusts the result using the bitwise OR operation, ensuring the result rounds to the nearest odd integer when necessary. This function is used for accurately calculating the square root of fractional values while adhering to specific rounding behavior.

"
"def _float_sqrt_of_frac(n: int, m: int) -> float:
    """"""Square root of n/m as a float, correctly rounded.""""""
    q = (n.bit_length() - m.bit_length() - _sqrt_bit_width) // 2  # Determine the scaling factor for precision.
    if q >= 0:
        # If q is non-negative, adjust the numerator and denominator to maintain precision.
        numerator = _integer_sqrt_of_frac_rto(n, m << 2 * q) << q
        denominator = 1
    else:
        # If q is negative, adjust both the numerator and denominator to maintain precision.
        numerator = _integer_sqrt_of_frac_rto(n << -2 * q, m)
        denominator = 1 << -q
    
    # Return the result as a float.
    return numerator / denominator
","The _float_sqrt_of_frac function calculates the square root of the fraction n/m as a floating-point number, ensuring correct rounding. It first calculates a scaling factor q based on the bit lengths of n and m and a predefined precision constant _sqrt_bit_width. Depending on the value of q, the function either adjusts the numerator and denominator accordingly or scales them to preserve precision. The final result is returned as a float. This method ensures accurate square root calculations for fractional values with the required precision."
"def _decimal_sqrt_of_frac(n: int, m: int) -> Decimal:
    """"""Square root of n/m as a Decimal, correctly rounded.""""""
    if n <= 0:
        if not n:
            return Decimal('0.0')  # Return zero if n is zero.
        n, m = -n, -m  # Handle negative values by flipping the signs of n and m.

    root = (Decimal(n) / Decimal(m)).sqrt()  # Calculate the square root using Decimal's built-in method.
    nr, dr = root.as_integer_ratio()  # Get the integer ratio representation of the result.

    plus = root.next_plus()  # Get the next higher Decimal value.
    np, dp = plus.as_integer_ratio()  # Get the integer ratio for the next value.
    
    if 4 * n * (dr*dp)**2 > m * (dr*np + dp*nr)**2:
        return plus  # If the result is closer to the next value, return it.

    minus = root.next_minus()  # Get the next lower Decimal value.
    nm, dm = minus.as_integer_ratio()  # Get the integer ratio for the previous value.
    
    if 4 * n * (dr*dm)**2 < m * (dr*nm + dm*nr)**2:
        return minus  # If the result is closer to the previous value, return it.

    return root  # Return the root if it's already correctly rounded.
","The _decimal_sqrt_of_frac function computes the square root of the fraction n/m using the Decimal type, ensuring correct rounding to the nearest valid Decimal. If n is negative, the signs of n and m are flipped to handle negative square roots. The function first computes the square root using Decimal's sqrt() method and then checks if the result needs rounding up or down by comparing the value to the next higher or lower Decimal values. It returns the appropriately rounded result as a Decimal."
"def _mean_stdev(data):
    """"""In one pass, compute the mean and sample standard deviation as floats.""""""
    T, ss, xbar, n = _ss(data)  # Compute the sum of squares, sample variance, mean, and sample size.
    if n < 2:
        # Raise an error if there are fewer than two data points.
        raise StatisticsError('stdev requires at least two data points')
    
    mss = ss / (n - 1)  # Calculate the sample variance.
    try:
        # Attempt to return the mean and standard deviation as floats.
        return float(xbar), _float_sqrt_of_frac(mss.numerator, mss.denominator)
    except AttributeError:
        # Handle NaN or infinite values gracefully if encountered.
        return float(xbar), float(xbar) / float(ss)
","The _mean_stdev function computes both the mean and the sample standard deviation of a dataset in a single pass. It relies on another function _ss to calculate the sum of squares, sample variance, mean, and the number of data points. If the dataset has fewer than two data points, a StatisticsError is raised. The function then computes the standard deviation as the square root of the sample variance and returns both the mean and the standard deviation as floating-point values. If the calculation encounters NaN or infinity, it handles these gracefully by adjusting the result.

"
"def _sqrtprod(x: float, y: float) -> float:
    ""Return sqrt(x * y) computed with improved accuracy and without overflow/underflow.""
    h = sqrt(x * y)  # Compute the square root of the product of x and y.

    if not isfinite(h):
        # If the result is not finite, check for overflow or underflow.
        if isinf(h) and not isinf(x) and not isinf(y):
            scale = 2.0 ** -512  # Scale down for overflow.
            return _sqrtprod(scale * x, scale * y) / scale
        return h

    if not h:
        # If the result is zero, check for underflow.
        if x and y:
            scale = 2.0 ** 537  # Scale up for underflow.
            return _sqrtprod(scale * x, scale * y) / scale
        return h

    # Improve accuracy using a differential correction technique.
    d = sumprod((x, h), (y, -h))
    return h + d / (2.0 * h)
","The _sqrtprod function computes the square root of the product of two values x and y, improving accuracy and avoiding overflow or underflow errors. It first computes the square root of x * y and checks if the result is finite. If the result is infinite, the function scales the input values to prevent overflow or underflow and recomputes the square root. It also handles the case where the square root is zero by scaling the values for underflow. Lastly, the function improves the accuracy of the result using a differential correction technique, ensuring that the final output is as precise as possible."
"def _normal_dist_inv_cdf(p, mu, sigma):
    # There is no closed-form solution to the inverse CDF for the normal
    # distribution, so we use a rational approximation instead:
    # Wichura, M.J. (1988). ""Algorithm AS241: The Percentage Points of the
    # Normal Distribution"". Applied Statistics. Blackwell Publishing. 37
    # (3): 477484. doi:10.2307/2347330. JSTOR 2347330.
    q = p - 0.5  # Center the input p around 0 by subtracting 0.5.

    # If the absolute value of q is less than or equal to 0.425, use a faster approximation.
    if fabs(q) <= 0.425:
        r = 0.180625 - q * q  # Compute a polynomial factor.
        
        # Compute the numerator of the rational approximation for the inverse CDF.
        num = (((((((2.50908_09287_30122_6727e+3 * r +
                     3.34305_75583_58812_8105e+4) * r +
                     6.72657_70927_00870_0853e+4) * r +
                     4.59219_53931_54987_1457e+4) * r +
                     1.37316_93765_50946_1125e+4) * r +
                     1.97159_09503_06551_4427e+3) * r +
                     1.33141_66789_17843_7745e+2) * r +
                     3.38713_28727_96366_6080e+0) * q
        # Compute the denominator of the rational approximation.
        den = (((((((5.22649_52788_52854_5610e+3 * r +
                     2.87290_85735_72194_2674e+4) * r +
                     3.93078_95800_09271_0610e+4) * r +
                     2.12137_94301_58659_5867e+4) * r +
                     5.39419_60214_24751_1077e+3) * r +
                     6.87187_00749_20579_0830e+2) * r +
                     4.23133_30701_60091_1252e+1) * r +
                     1.0)
        
        # Calculate the final result and scale by sigma, then add mu.
        x = num / den
        return mu + (x * sigma)

    # For q > 0.425, use a more robust calculation.
    r = p if q <= 0.0 else 1.0 - p  # Adjust r based on the value of q.
    r = sqrt(-log(r))  # Take the negative logarithm and square root.

    # If r is small, perform the rational approximation with a different set of coefficients.
    if r <= 5.0:
        r = r - 1.6  # Shift r.
        
        # Numerator and denominator for the approximation.
        num = (((((((7.74545_01427_83414_07640e-4 * r +
                     2.27238_44989_26918_45833e-2) * r +
                     2.41780_72517_74506_11770e-1) * r +
                     1.27045_82524_52368_38258e+0) * r +
                     3.64784_83247_63204_60504e+0) * r +
                     5.76949_72214_60691_40550e+0) * r +
                     4.63033_78461_56545_29590e+0) * r +
                     1.42343_71107_49683_57734e+0)
        den = (((((((1.05075_00716_44416_84324e-9 * r +
                     5.47593_80849_95344_94600e-4) * r +
                     1.51986_66563_61645_71966e-2) * r +
                     1.48103_97642_74800_74590e-1) * r +
                     6.89767_33498_51000_04550e-1) * r +
                     1.67638_48301_83803_84940e+0) * r +
                     2.05319_16266_37758_82187e+0) * r +
                     1.0)
    else:
        r = r - 5.0  # Shift r again.
        
        # Numerator and denominator for the approximation.
        num = (((((((2.01033_43992_92288_13265e-7 * r +
                     2.71155_55687_43487_57815e-5) * r +
                     1.24266_09473_88078_43860e-3) * r +
                     2.65321_89526_57612_30930e-2) * r +
                     2.96560_57182_85048_91230e-1) * r +
                     1.78482_65399_17291_33580e+0) * r +
                     5.46378_49111_64114_36990e+0) * r +
                     6.65790_46435_01103_77720e+0)
        den = (((((((2.04426_31033_89939_78564e-15 * r +
                     1.42151_17583_16445_88870e-7) * r +
                     1.84631_83175_10054_68180e-5) * r +
                     7.86869_13114_56132_59100e-4) * r +
                     1.48753_61290_85061_48525e-2) * r +
                     1.36929_88092_27358_05310e-1) * r +
                     5.99832_20655_58879_37690e-1) * r +
                     1.0)

    # Calculate the final result and adjust sign based on q.
    x = num / den
    if q < 0.0:
        x = -x

    return mu + (x * sigma)
","The _normal_dist_inv_cdf function approximates the inverse cumulative distribution function (CDF) of the normal distribution, which cannot be expressed in a simple closed form. The function uses rational approximations based on the work of Wichura (1988) to provide accurate results for percentile values. The function first handles cases where the absolute value of the difference between the input p and 0.5 is small, using a faster polynomial approximation. For larger values of the difference, a more robust method is applied, based on transforming p and using different sets of coefficients. The result is then scaled by the standard deviation sigma and adjusted by the mean mu to provide the final inverse CDF value. The function handles edge cases gracefully and ensures numerical stability with careful handling of large and small values."
"def abstractmethod(funcobj):
    """"""A decorator indicating abstract methods.

    Requires that the metaclass is ABCMeta or derived from it. A class that 
    has a metaclass derived from ABCMeta cannot be instantiated unless all 
    of its abstract methods are overridden. The abstract methods can be called 
    using any of the normal 'super' call mechanisms. abstractmethod() may be used 
    to declare abstract methods for properties and descriptors.

    Usage:

        class C(metaclass=ABCMeta):
            @abstractmethod
            def my_abstract_method(self, arg1, arg2, argN):
                ...
    """"""
    funcobj.__isabstractmethod__ = True  # Marks the function as abstract
    return funcobj  # Returns the function with the abstract method flag
","The abstractmethod function is a decorator used to mark methods as abstract, requiring that they must be implemented in any subclass of the class that declares them. The decorator works by setting the __isabstractmethod__ attribute to True on the given function object. This ensures that any class using ABCMeta as its metaclass cannot be instantiated unless all its abstract methods are overridden.

"
"class abstractclassmethod(classmethod):
    """"""A decorator indicating abstract classmethods.

    Deprecated, use 'classmethod' with 'abstractmethod' instead:

        class C(ABC):
            @classmethod
            @abstractmethod
            def my_abstract_classmethod(cls, ...):
                ...

    """"""

    __isabstractmethod__ = True  # Marks the class method as abstract

    def __init__(self, callable):
        callable.__isabstractmethod__ = True  # Marks the callable as abstract
        super().__init__(callable)  # Calls the parent class's constructor
","abstractclassmethod is a decorator class indicating that a method is an abstract class method. This means that any subclass of a class using this decorator must implement this method. This decorator is deprecated, and the recommended usage is to apply abstractmethod to a classmethod instead. The constructor marks the method as abstract and calls the superclass constructor."
"class abstractstaticmethod(staticmethod):
    """"""A decorator indicating abstract staticmethods.

    Deprecated, use 'staticmethod' with 'abstractmethod' instead:

        class C(ABC):
            @staticmethod
            @abstractmethod
            def my_abstract_staticmethod(...):
                ...

    """"""

    __isabstractmethod__ = True  # Marks the static method as abstract

    def __init__(self, callable):
        callable.__isabstractmethod__ = True  # Marks the callable as abstract
        super().__init__(callable)  # Calls the parent class's constructor
","abstractstaticmethod is a decorator that marks a method as an abstract static method, meaning subclasses must implement this method. It is similar to abstractclassmethod but applies to static methods. Like abstractclassmethod, it is deprecated and the recommended practice is to use abstractmethod with staticmethod instead. The constructor marks the method as abstract and invokes the parent class constructor."
"class abstractproperty(property):
    """"""A decorator indicating abstract properties.

    Deprecated, use 'property' with 'abstractmethod' instead:

        class C(ABC):
            @property
            @abstractmethod
            def my_abstract_property(self):
                ...

    """"""

    __isabstractmethod__ = True  # Marks the property as abstract
","The abstractproperty class decorator marks a property as abstract, meaning it must be implemented by any subclass. The decorator is deprecated, and the suggested approach is to use abstractmethod with the property decorator. The __isabstractmethod__ attribute is set to True to indicate that the property is abstract."
"class ABCMeta(type):
    """"""Metaclass for defining Abstract Base Classes (ABCs).

    Use this metaclass to create an ABC. An ABC can be subclassed directly, 
    and then acts as a mix-in class. You can also register unrelated concrete 
    classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- 
    these and their descendants will be considered subclasses of the registering 
    ABC by the built-in issubclass() function, but the registering ABC won't 
    show up in their MRO (Method Resolution Order) nor will method implementations 
    defined by the registering ABC be callable (not even via super()).
    """"""
    def __new__(mcls, name, bases, namespace, /, **kwargs):
        cls = super().__new__(mcls, name, bases, namespace, **kwargs)
        _abc_init(cls)  # Initialize the class as an ABC
        return cls  # Return the class created

    def register(cls, subclass):
        """"""Register a virtual subclass of an ABC.

        Returns the subclass, to allow usage as a class decorator.
        """"""
        return _abc_register(cls, subclass)  # Registers a subclass as virtual

    def __instancecheck__(cls, instance):
        """"""Override for isinstance(instance, cls).""""""
        return _abc_instancecheck(cls, instance)  # Checks if instance is of type cls

    def __subclasscheck__(cls, subclass):
        """"""Override for issubclass(subclass, cls).""""""
        return _abc_subclasscheck(cls, subclass)  # Checks subclass relationship

    def _dump_registry(cls, file=None):
        """"""Debug helper to print the ABC registry.""""""
        print(f""Class: {cls.__module__}.{cls.__qualname__}"", file=file)
        print(f""Inv. counter: {get_cache_token()}"", file=file)
        (_abc_registry, _abc_cache, _abc_negative_cache,
         _abc_negative_cache_version) = _get_dump(cls)
        print(f""_abc_registry: {_abc_registry!r}"", file=file)
        print(f""_abc_cache: {_abc_cache!r}"", file=file)
        print(f""_abc_negative_cache: {_abc_negative_cache!r}"", file=file)
        print(f""_abc_negative_cache_version: {_abc_negative_cache_version!r}"",
              file=file)

    def _abc_registry_clear(cls):
        """"""Clear the registry (for debugging or testing).""""""
        _reset_registry(cls)  # Clears the registry for debugging

    def _abc_caches_clear(cls):
        """"""Clear the caches (for debugging or testing).""""""
        _reset_caches(cls)  # Clears caches for debugging
","ABCMeta is a metaclass used to define Abstract Base Classes (ABCs). An ABC is a class that cannot be instantiated unless all its abstract methods are implemented by a subclass. The metaclass allows the registration of virtual subclasses and overrides issubclass() and isinstance() to handle ABCs and their virtual subclasses. The methods also include debugging helpers like _dump_registry, _abc_registry_clear, and _abc_caches_clear to manage internal caches and registries."
"def update_abstractmethods(cls):
    """"""Recalculate the set of abstract methods of an abstract class.

    If a class has had one of its abstract methods implemented after the
    class was created, the method will not be considered implemented until
    this function is called. Alternatively, if a new abstract method has been
    added to the class, it will only be considered an abstract method of the
    class after this function is called.

    This function should be called before any use is made of the class,
    usually in class decorators that add methods to the subject class.

    Returns cls, to allow usage as a class decorator.

    If cls is not an instance of ABCMeta, does nothing.
    """"""
    if not hasattr(cls, '__abstractmethods__'):
        # We check for __abstractmethods__ here because cls might be a C
        # implementation or a python implementation (especially during
        # testing), and we want to handle both cases.
        return cls

    abstracts = set()  # Initialize a set to store abstract methods
    # Check the existing abstract methods of the parents, keep only the ones
    # that are not implemented.
    for scls in cls.__bases__:
        for name in getattr(scls, '__abstractmethods__', ()):
            value = getattr(cls, name, None)
            if getattr(value, ""__isabstractmethod__"", False):
                abstracts.add(name)
    # Also add any other newly added abstract methods.
    for name, value in cls.__dict__.items():
        if getattr(value, ""__isabstractmethod__"", False):
            abstracts.add(name)
    cls.__abstractmethods__ = frozenset(abstracts)  # Update the abstract methods set
    return cls  # Return the class, allowing usage as a decorator
","update_abstractmethods recalculates the abstract methods of a class. It ensures that if a class adds or implements abstract methods after being defined, these methods are correctly identified. The function works by checking for the presence of abstract methods in both parent classes and the current class, adding new abstract methods to the __abstractmethods__ attribute, and updating it. It is typically used in decorators to modify classes dynamically."
"class ABC(metaclass=ABCMeta):
    """"""Helper class that provides a standard way to create an ABC using
    inheritance.
    """"""
    __slots__ = ()  # Prevents the creation of instance dictionaries, optimizing memory usage
","ABC is a helper class that provides an easy way to create Abstract Base Classes using inheritance. By using ABCMeta as the metaclass, it ensures that any subclass of ABC can be used as an abstract class. The __slots__ attribute prevents the creation of instance dictionaries, making it more memory efficient."
"class Dialect:
    """"""Describe a CSV dialect.

    This must be subclassed (see csv.excel).  Valid attributes are:
    delimiter, quotechar, escapechar, doublequote, skipinitialspace,
    lineterminator, quoting.
    """"""
    _name = """"
    _valid = False
    # placeholders
    delimiter = None
    quotechar = None
    escapechar = None
    doublequote = None
    skipinitialspace = None
    lineterminator = None
    quoting = None

    def __init__(self):
        if self.__class__ != Dialect:
            self._valid = True  # Mark the dialect as valid if it is a subclass
        self._validate()  # Call the _validate method to ensure the dialect settings are correct

    def _validate(self):
        try:
            _Dialect(self)  # Validate the dialect using the _Dialect class from the CSV module
        except TypeError as e:
            # Re-raise to get a traceback showing more user code.
            raise Error(str(e)) from None  # Raise an error if validation fails
","The Dialect class is designed to represent the CSV format configuration, such as delimiters, quote characters, and escaping behaviors. It is meant to be subclassed, with each subclass defining a specific CSV dialect (e.g., Excel, Unix). The attributes delimiter, quotechar, escapechar, doublequote, skipinitialspace, lineterminator, and quoting determine how a CSV file is read or written. The _validate() method ensures that the provided dialect is valid by attempting to initialize a _Dialect object with the class's attributes."
"class excel_tab(excel):
    """"""Describe the usual properties of Excel-generated TAB-delimited files.""""""
    delimiter = '\t'  # Override delimiter to tab character
register_dialect(""excel-tab"", excel_tab)
",The excel_tab class is a subclass of excel and modifies the delimiter property to represent tab-separated values (\t). This class is useful for reading/writing CSV files generated by Excel that use tabs as the field separator instead of commas. It is registered as a new dialect with register_dialect().
"class DictReader:
    def __init__(self, f, fieldnames=None, restkey=None, restval=None,
                 dialect=""excel"", *args, **kwds):
        if fieldnames is not None and iter(fieldnames) is fieldnames:
            fieldnames = list(fieldnames)
        self._fieldnames = fieldnames   # list of keys for the dict
        self.restkey = restkey          # key to catch long rows
        self.restval = restval          # default value for short rows
        self.reader = reader(f, dialect, *args, **kwds)  # Create a CSV reader object
        self.dialect = dialect  # Store the dialect used for reading
        self.line_num = 0  # Initialize line number tracker

    def __iter__(self):
        return self  # Enable iteration over the DictReader instance

    @property
    def fieldnames(self):
        if self._fieldnames is None:
            try:
                self._fieldnames = next(self.reader)  # Get fieldnames from the first row
            except StopIteration:
                pass
        self.line_num = self.reader.line_num  # Update line number
        return self._fieldnames

    @fieldnames.setter
    def fieldnames(self, value):
        self._fieldnames = value  # Allow setting custom fieldnames

    def __next__(self):
        if self.line_num == 0:
            self.fieldnames  # Get fieldnames side effect
        row = next(self.reader)  # Read the next row
        self.line_num = self.reader.line_num  # Update line number
        while row == []:
            row = next(self.reader)  # Skip empty rows
        d = dict(zip(self.fieldnames, row))  # Convert row to dictionary
        lf = len(self.fieldnames)
        lr = len(row)
        if lf < lr:
            d[self.restkey] = row[lf:]  # Add extra columns if present
        elif lf > lr:
            for key in self.fieldnames[lr:]:
                d[key] = self.restval  # Add default values for missing columns
        return d

    __class_getitem__ = classmethod(types.GenericAlias)
","The DictReader class is a CSV reader that returns each row as a dictionary where the keys are field names. It supports reading CSV files with custom delimiters and settings, as specified by the dialect parameter. The __init__ method initializes the reader and the list of field names. The __iter__ method makes it iterable, and the __next__ method reads the next row, ensuring that missing values are filled with defaults and that extra fields are captured using the restkey."
"class DictWriter:
    def __init__(self, f, fieldnames, restval="""", extrasaction=""raise"",
                 dialect=""excel"", *args, **kwds):
        if fieldnames is not None and iter(fieldnames) is fieldnames:
            fieldnames = list(fieldnames)
        self.fieldnames = fieldnames    # list of keys for the dict
        self.restval = restval          # for writing short dicts
        extrasaction = extrasaction.lower()
        if extrasaction not in (""raise"", ""ignore""):
            raise ValueError(""extrasaction (%s) must be 'raise' or 'ignore'""
                             % extrasaction)
        self.extrasaction = extrasaction
        self.writer = writer(f, dialect, *args, **kwds)

    def writeheader(self):
        header = dict(zip(self.fieldnames, self.fieldnames))  # Create a header row
        return self.writerow(header)  # Write the header row

    def _dict_to_list(self, rowdict):
        if self.extrasaction == ""raise"":
            wrong_fields = rowdict.keys() - self.fieldnames
            if wrong_fields:
                raise ValueError(""dict contains fields not in fieldnames: ""
                                 + "", "".join([repr(x) for x in wrong_fields]))
        return (rowdict.get(key, self.restval) for key in self.fieldnames)  # Convert dictionary to list

    def writerow(self, rowdict):
        return self.writer.writerow(self._dict_to_list(rowdict))  # Write a single row

    def writerows(self, rowdicts):
        return self.writer.writerows(map(self._dict_to_list, rowdicts))  # Write multiple rows

    __class_getitem__ = classmethod(types.GenericAlias)
","The DictWriter class writes CSV data from a dictionary format, where each dictionary corresponds to a row and the keys match the column names. The constructor initializes settings like fieldnames, restval, and extrasaction (how to handle extra fields). The writeheader method writes the header row, and writerow and writerows handle writing single and multiple rows, respectively. The _dict_to_list helper method converts the dictionary to a list of values matching the fieldnames."
"class Sniffer:
    '''
    ""Sniffs"" the format of a CSV file (i.e. delimiter, quotechar)
    Returns a Dialect object.
    '''
    def __init__(self):
        self.preferred = [',', '\t', ';', ' ', ':']  # Preferred delimiters

    def sniff(self, sample, delimiters=None):
        """"""
        Returns a dialect (or None) corresponding to the sample
        """"""
        quotechar, doublequote, delimiter, skipinitialspace = \
            self._sniff(sample, delimiters)  # Sniff out the delimiter and other properties
        if delimiter:
            dialect = Dialect()  # Return a new Dialect object
            dialect.delimiter = delimiter
            dialect.quotechar = quotechar
            dialect.doublequote = doublequote
            dialect.skipinitialspace = skipinitialspace
            return dialect
        return None
","The Sniffer class is responsible for detecting the CSV file format (such as delimiter and quote character). The sniff method analyzes a sample of the CSV file to determine the best delimiter, quote character, and other properties. It then returns a Dialect object that describes the detected format. The preferred list contains a set of delimiters that the sniffer checks in order."
"class Number(metaclass=ABCMeta):
    """"""All numbers inherit from this class.

    If you just want to check if an argument x is a number, without
    caring what kind, use isinstance(x, Number).
    """"""
    __slots__ = ()

    # Concrete numeric types must provide their own hash implementation
    __hash__ = None
","The Number class is an abstract base class (ABC) designed to represent all numeric types. It provides a common interface for all numbers and allows checking if an object is a number by using isinstance(x, Number). The __hash__ method is set to None, indicating that subclasses must implement their own hash function."
"""""""
opcode module - potentially shared between dis and other modules which
operate on bytecodes (e.g. peephole optimizers).
""""""
# This module provides utilities for working with Python bytecode operations.
# It is designed to be shared between the 'dis' module and other bytecode-related modules.

__all__ = [""cmp_op"", ""stack_effect"", ""hascompare"", ""opname"", ""opmap"",
           ""HAVE_ARGUMENT"", ""EXTENDED_ARG"", ""hasarg"", ""hasconst"", ""hasname"",
           ""hasjump"", ""hasjrel"", ""hasjabs"", ""hasfree"", ""haslocal"", ""hasexc""]
# __all__ defines the public interface of the module, listing the names that should
# be imported when 'from module import *' is used.

import _opcode
from _opcode import stack_effect
# Importing the internal _opcode module, which contains functions to interact with the bytecode.
# The stack_effect function is imported specifically, likely used for analyzing how operations 
# affect the stack.

from _opcode_metadata import (_specializations, _specialized_opmap, opmap,  # noqa: F401
                              HAVE_ARGUMENT, MIN_INSTRUMENTED_OPCODE)  # noqa: F401
# Importing metadata related to opcode operations. The # noqa: F401 suppresses unused import warnings.

EXTENDED_ARG = opmap['EXTENDED_ARG']
# Retrieves the opcode for ""EXTENDED_ARG"" from the opmap, used for handling extended argument instructions.

opname = ['<%r>' % (op,) for op in range(max(opmap.values()) + 1)]
for op, i in opmap.items():
    opname[i] = op
# This generates a list of opcode names ('opname') by iterating through the opmap values.
# Initially, the list is populated with formatted string representations of opcodes.
# Then, it maps the integer values in opmap to corresponding human-readable opcode names.

cmp_op = ('<', '<=', '==', '!=', '>', '>=')
# Defines comparison operations as a tuple of strings, likely corresponding to comparison bytecodes.

# These lists are documented as part of the dis module's API
hasarg = [op for op in opmap.values() if _opcode.has_arg(op)]
hasconst = [op for op in opmap.values() if _opcode.has_const(op)]
hasname = [op for op in opmap.values() if _opcode.has_name(op)]
hasjump = [op for op in opmap.values() if _opcode.has_jump(op)]
hasjrel = hasjump  # for backward compatibility
hasjabs = []
hasfree = [op for op in opmap.values() if _opcode.has_free(op)]
haslocal = [op for op in opmap.values() if _opcode.has_local(op)]
hasexc = [op for op in opmap.values() if _opcode.has_exc(op)]
# These lists are generated to categorize opcodes based on different properties.
# For example, `hasarg` lists all opcodes that require an argument, `hasconst` lists those
# that use constants, etc. These are used to optimize operations and checks.

_intrinsic_1_descs = _opcode.get_intrinsic1_descs()
_intrinsic_2_descs = _opcode.get_intrinsic2_descs()
_special_method_names = _opcode.get_special_method_names()
_common_constants = [AssertionError, NotImplementedError]
_nb_ops = _opcode.get_nb_ops()
# These variables are populated by calling functions from the _opcode module to retrieve
# intrinsic descriptions, special method names, common constants, and the number of opcodes.

hascompare = [opmap[""COMPARE_OP""]]
# Retrieves the opcode for the ""COMPARE_OP"" operation.

_cache_format = {
    ""LOAD_GLOBAL"": {
        ""counter"": 1,
        ""index"": 1,
        ""module_keys_version"": 1,
        ""builtin_keys_version"": 1,
    },
    ""BINARY_OP"": {
        ""counter"": 1,
    },
    ""UNPACK_SEQUENCE"": {
        ""counter"": 1,
    },
    ""COMPARE_OP"": {
        ""counter"": 1,
    },
    ""CONTAINS_OP"": {
        ""counter"": 1,
    },
    ""BINARY_SUBSCR"": {
        ""counter"": 1,
    },
    ""FOR_ITER"": {
        ""counter"": 1,
    },
    ""LOAD_SUPER_ATTR"": {
        ""counter"": 1,
    },
    ""LOAD_ATTR"": {
        ""counter"": 1,
        ""version"": 2,
        ""keys_version"": 2,
        ""descr"": 4,
    },
    ""STORE_ATTR"": {
        ""counter"": 1,
        ""version"": 2,
        ""index"": 1,
    },
    ""CALL"": {
        ""counter"": 1,
        ""func_version"": 2,
    },
    ""CALL_KW"": {
        ""counter"": 1,
        ""func_version"": 2,
    },
    ""STORE_SUBSCR"": {
        ""counter"": 1,
    },
    ""SEND"": {
        ""counter"": 1,
    },
    ""JUMP_BACKWARD"": {
        ""counter"": 1,
    },
    ""TO_BOOL"": {
        ""counter"": 1,
        ""version"": 2,
    },
    ""POP_JUMP_IF_TRUE"": {
        ""counter"": 1,
    },
    ""POP_JUMP_IF_FALSE"": {
        ""counter"": 1,
    },
    ""POP_JUMP_IF_NONE"": {
        ""counter"": 1,
    },
    ""POP_JUMP_IF_NOT_NONE"": {
        ""counter"": 1,
    },
}
# The _cache_format dictionary provides detailed metadata about various bytecode operations,
# specifying attributes like counters, versions, indices, etc. These attributes are likely
# used for bytecode optimization or instrumentation.

_inline_cache_entries = {
    name: sum(value.values()) for (name, value) in _cache_format.items()
}
# Creates a dictionary of inline cache entries by summing the values in the _cache_format dictionary.
# This could be used to track the total counts or properties related to different bytecode operations.
","This module provides essential tools for working with bytecodes, categorizing them, and collecting metadata to optimize or analyze bytecode execution. It's likely designed to be shared across different modules that need to interact with Python bytecode, such as disassemblers, peephole optimizers, or code instrumentation tools."
"class GetoptError(Exception):
    opt = ''  # This is a class variable for storing the option involved in the error.
    msg = ''  # This is a class variable for storing the error message.

    def __init__(self, msg, opt=''):
        # The constructor initializes the error message (msg) and the option (opt).
        self.msg = msg
        self.opt = opt
        Exception.__init__(self, msg, opt)  # Calls the base class constructor to initialize the exception.

    def __str__(self):
        # Returns the error message when the exception is printed.
        return self.msg
",The GetoptError class is a custom exception used in the getopt module. It is raised when an error occurs while parsing command-line options. The class inherits from Pythons built-in Exception class and allows setting an error message and the option involved in the error. The __str__ method returns the message associated with the exception when it is printed.
"def getopt(args, shortopts, longopts = []):
    """"""
    getopt(args, options[, long_options]) -> opts, args
    Parses command line options and returns a list of option-value pairs and the remaining arguments.
    """"""
    opts = []  # Initialize an empty list to store the option-value pairs.
    
    if isinstance(longopts, str):  # Check if longopts is a string; if so, convert to list.
        longopts = [longopts]
    else:
        longopts = list(longopts)  # Ensure longopts is a list.

    while args and args[0].startswith('-') and args[0] != '-':  # While there are arguments to parse.
        if args[0] == '--':  # End of options, so break out of the loop.
            args = args[1:]
            break
        if args[0].startswith('--'):  # Handle long options (e.g., --option).
            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])
        else:  # Handle short options (e.g., -x).
            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])
    
    return opts, args  # Return the parsed options and remaining arguments.
","The getopt function parses command-line options (short and long) from a list of arguments (args). It processes short options like -x and long options like --alpha. The function returns a tuple: the first element is a list of parsed options and their arguments, and the second element is the remaining non-option arguments. This function supports both short and long options and can handle option arguments."
"def gnu_getopt(args, shortopts, longopts = []):
    """"""
    gnu_getopt(args, options[, long_options]) -> opts, args
    Similar to getopt() but allows option and non-option arguments to be intermixed.
    """"""
    opts = []  # Initialize an empty list to store the option-value pairs.
    prog_args = []  # List to hold non-option arguments.

    if isinstance(longopts, str):  # Convert longopts to a list if it's a string.
        longopts = [longopts]
    else:
        longopts = list(longopts)

    return_in_order = False
    # Check if we need to return options after non-option arguments
    if shortopts.startswith('-'):
        shortopts = shortopts[1:]
        all_options_first = False
        return_in_order = True
    elif shortopts.startswith('+'):
        shortopts = shortopts[1:]
        all_options_first = True
    elif os.environ.get(""POSIXLY_CORRECT""):
        all_options_first = True
    else:
        all_options_first = False

    while args:  # Continue parsing until there are no more arguments.
        if args[0] == '--':  # End of options, so append remaining args.
            prog_args += args[1:]
            break
        if args[0][:2] == '--':  # Handle long options.
            if return_in_order and prog_args:
                opts.append((None, prog_args))  # Store non-option args if needed.
                prog_args = []
            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])
        elif args[0][:1] == '-' and args[0] != '-':  # Handle short options.
            if return_in_order and prog_args:
                opts.append((None, prog_args))  # Store non-option args if needed.
                prog_args = []
            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])
        else:
            # Handle non-option arguments.
            if all_options_first:
                prog_args += args
                break
            else:
                prog_args.append(args[0])
                args = args[1:]
    
    return opts, prog_args  # Return parsed options and non-option arguments.
","The gnu_getopt function is an extended version of getopt. It allows options and non-option arguments to be intermixed (e.g., -x file -y), unlike getopt, which stops parsing options as soon as a non-option argument is encountered. It provides additional behavior for handling command-line arguments based on the POSIX environment and can manage option argument ordering."
"def do_longs(opts, opt, longopts, args):
    """"""
    Processes a long option (e.g., --option).
    """"""
    try:
        i = opt.index('=')  # Check if the option has an argument (e.g., --opt=value).
    except ValueError:
        optarg = None  # No argument for the option.
    else:
        opt, optarg = opt[:i], opt[i+1:]  # Split option and argument.
    
    has_arg, opt = long_has_args(opt, longopts)  # Check if the option expects an argument.
    
    if has_arg:  # If the option requires an argument.
        if optarg is None and has_arg != '?':
            if not args:
                raise GetoptError(_('option --%s requires argument') % opt, opt)
            optarg, args = args[0], args[1:]
    elif optarg is not None:
        raise GetoptError(_('option --%s must not have an argument') % opt, opt)
    
    opts.append(('--' + opt, optarg or ''))  # Add the option and its argument to the opts list.
    return opts, args
","The do_longs function handles long command-line options, parsing them into option-argument pairs. It checks if the option requires an argument and handles errors accordingly. If the option has an argument, it attempts to retrieve it; if not, it raises an error. This function works with options of the form --opt=value and ensures that arguments are correctly matched."
"def long_has_args(opt, longopts):
    """"""
    Checks if the long option has an argument.
    """"""
    possibilities = [o for o in longopts if o.startswith(opt)]  # Find matching long options.
    
    if not possibilities:  # If no matching option, raise error.
        raise GetoptError(_('option --%s not recognized') % opt, opt)
    
    # Check for exact matches or options with arguments.
    if opt in possibilities:
        return False, opt
    elif opt + '=' in possibilities:
        return True, opt  # Option requires an argument.
    elif opt + '=?' in possibilities:
        return '?', opt  # Option accepts an optional argument.
    
    # If there are multiple matches, raise error.
    if len(possibilities) > 1:
        raise GetoptError(
            _(""option --%s not a unique prefix; possible options: %s"")
            % (opt, "", "".join(possibilities)),
            opt,
        )
    
    # If only one match, return whether it requires an argument.
    assert len(possibilities) == 1
    unique_match = possibilities[0]
    if unique_match.endswith('=?'):
        return '?', unique_match[:-2]
    has_arg = unique_match.endswith('=')
    if has_arg:
        unique_match = unique_match[:-1]
    return has_arg, unique_match
","The long_has_args function checks if a long option requires an argument. It matches the given option against the list of possible long options and verifies whether it requires an argument (e.g., --option=). If there are multiple matches or an unrecognized option, it raises a GetoptError. It returns a tuple indicating whether the option requires an argument."
"def do_shorts(opts, optstring, shortopts, args):
    """"""
    Processes a short option (e.g., -x).
    """"""
    while optstring != '':  # Continue until all characters in optstring are processed.
        opt, optstring = optstring[0], optstring[1:]  # Extract the current option character.
        optarg = None  # Default to no argument.

        has_arg, opt = short_has_args(opt, shortopts)  # Check if the option expects an argument.
        
        if has_arg == '?':  # If the option accepts an optional argument.
            if args and args[0][0] != '-':
                optarg = args[0]
                args = args[1:]
        
        if has_arg:  # If the option requires an argument.
            if optarg is None:
                if not args:
                    raise GetoptError(_('option -%s requires argument') % opt, opt)
                optarg, args = args[0], args[1:]
        
        opts.append(('-' + opt, optarg or ''))  # Add the option and its argument to the opts list.
    
    return opts, args
","The do_shorts function processes short options (e.g., -x). It checks if the option has an argument and handles it accordingly. If the option requires an argument, it attempts to retrieve it from the arguments list. The function handles both required and optional arguments for short options and appends the parsed option and argument pair to the opts list."
"def randbelow(exclusive_upper_bound):
    """"""Return a random int in the range [0, n).""""""
    if exclusive_upper_bound <= 0:
        raise ValueError(""Upper bound must be positive."")  # Ensure the upper bound is positive.
    return _sysrand._randbelow(exclusive_upper_bound)  # Generate a random integer below the given upper bound.
","The randbelow() function generates a random integer in the range [0, exclusive_upper_bound), i.e., between 0 (inclusive) and the specified upper bound (exclusive). If the upper bound is non-positive, it raises a ValueError. It leverages the SystemRandom class for cryptographically secure random number generation."
"def token_bytes(nbytes=None):
    """"""Return a random byte string containing *nbytes* bytes.

    If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.
    """"""
    if nbytes is None:  # If nbytes is not specified, set it to the default entropy size.
        nbytes = DEFAULT_ENTROPY
    return _sysrand.randbytes(nbytes)  # Generate a random byte string of the specified length.
","The token_bytes() function generates a secure random byte string of nbytes length. If no length is provided, it defaults to DEFAULT_ENTROPY (32 bytes). This function uses the randbytes() method of the SystemRandom class to ensure cryptographically strong randomness suitable for security-sensitive applications."
"def token_hex(nbytes=None):
    """"""Return a random text string, in hexadecimal.

    The string has *nbytes* random bytes, each byte converted to two
    hex digits.  If *nbytes* is ``None`` or not supplied, a reasonable
    default is used.
    """"""
    return token_bytes(nbytes).hex()  # Generate random bytes and convert them to a hexadecimal string.
","The token_hex() function returns a string of random bytes converted to hexadecimal format. Each byte is represented by two hexadecimal digits. The function relies on token_bytes() to generate the random bytes and then calls the .hex() method to convert them into a hex string. If no byte count is provided, it defaults to the same behavior as token_bytes()."
"def token_urlsafe(nbytes=None):
    """"""Return a random URL-safe text string, in Base64 encoding.

    The string has *nbytes* random bytes.  If *nbytes* is ``None``
    or not supplied, a reasonable default is used.
    """"""
    tok = token_bytes(nbytes)  # Generate the random byte string using token_bytes.
    return base64.urlsafe_b64encode(tok).rstrip(b'=').decode('ascii')  # Encode the byte string in Base64 URL-safe format.
","The token_urlsafe() function generates a random URL-safe Base64 encoded string. It first calls token_bytes() to generate random bytes and then uses base64.urlsafe_b64encode() to convert the byte string into a Base64 string. It also removes the trailing = padding for URL compatibility and decodes it into an ASCII string. If no length is specified, the function defaults to 32 bytes."
"def isleap(year):
    """"""Return True for leap years, False for non-leap years.""""""
    # Check if the year is divisible by 4
    # A year is a leap year if it is divisible by 4 and not divisible by 100,
    # unless it is also divisible by 400.
    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)","The isleap function determines whether a given year is a leap year according to the Gregorian calendar. A leap year occurs every four years to help synchronize the calendar year with the solar year. The logic implemented in this function checks if the input year is divisible by 4, which is the primary condition for a leap year. However, if the year is also divisible by 100, it must additionally be divisible by 400 to qualify as a leap year. This accounts for century years that are not leap years unless they can be evenly divided by 400. The function returns True if these conditions are met, indicating a leap year, and False otherwise."
"def leapdays(y1, y2):
    """"""Return number of leap years in range [y1, y2).
       Assume y1 <= y2.""""""
    # Decrement y1 by 1 to include the starting year in the calculation
    y1 -= 1
    # Decrement y2 by 1 to handle the exclusive range [y1, y2)
    y2 -= 1
    # Calculate the number of leap years using the formula:
    # (number of years divisible by 4) - (number of years divisible by 100)
    # + (number of years divisible by 400)
    return (y2 // 4 - y1 // 4) - (y2 // 100 - y1 // 100) + (y2 // 400 - y1 // 400)","The leapdays function calculates the number of leap years within a specified range of years [y1,y2), where y1 is inclusive and y2 is exclusive. The function assumes that y1?y2. To accurately compute the number of leap years, the function first decrements both y1 and y2 by one to adjust for zero-based calculations. It then applies a formula that counts the total number of leap years: it calculates the number of years divisible by 4, subtracts those divisible by 100 (since they are not leap years unless also divisible by 400), and finally adds back those divisible by 400. This approach efficiently determines the total leap years in the specified range."
"def weekday(year, month, day):
    """"""Return weekday (0-6 ~ Mon-Sun) for year, month (1-12), day (1-31).""""""
    # Check if the year is within the valid range defined by datetime
    if not datetime.MINYEAR <= year <= datetime.MAXYEAR:
        # Adjust the year to fall within a 400-year cycle if it is out of range
        year = 2000 + year % 400
    # Calculate the weekday for the given date and return it
    # The weekday() method returns an integer where Monday is 0 and Sunday is 6
    return Day(datetime.date(year, month, day).weekday())","The weekday function calculates the day of the week for a given date specified by year, month, and day. The function returns an integer from 0 to 6, corresponding to Monday through Sunday. It first checks if the provided year falls within the valid range defined by Python's datetime module. If the year is outside this range, it adjusts the year to fit within a 400-year cycle starting from 2000. This adjustment ensures compatibility with the Gregorian calendar's leap year rules. The function then uses the datetime.date method to create a date object and calls its weekday() method to determine the day of the week, which is wrapped in a call to Day to format or process it as needed."
"def _validate_month(month):
    # Check if the month is within the valid range (1 to 12)
    if not 1 <= month <= 12:
        # Raise an error if the month is not valid
        raise IllegalMonthError(month)","The _validate_month function ensures that a given month value is valid by checking if it falls within the range of 1 to 12, which corresponds to the months of January through December. If the month value is outside this range, the function raises an IllegalMonthError, passing the invalid month as an argument. This function is typically used to validate input data before performing operations that assume a valid month, helping to prevent errors and ensure data integrity in applications that handle date-related information."
"def monthrange(year, month):
    """"""Return weekday of first day of month (0-6 ~ Mon-Sun)
       and number of days (28-31) for year, month.""""""
    # Validate that the month is within the correct range (1 to 12)
    _validate_month(month)
    # Get the weekday of the first day of the month
    day1 = weekday(year, month, 1)
    # Calculate the number of days in the month
    # Add 1 to February's days if it's a leap year
    ndays = mdays[month] + (month == FEBRUARY and isleap(year))
    # Return the weekday of the first day and the total number of days in the month
    return day1, ndays","The monthrange function determines two key pieces of information about a given month and year: the weekday on which the first day of the month falls and the total number of days in that month. It first calls _validate_month to ensure that the provided month is between 1 and 12. It then uses the weekday function to find out which day of the week (0 for Monday through 6 for Sunday) corresponds to the first day of the specified month. The function calculates the number of days in the month by accessing a predefined list mdays that contains the standard number of days for each month. If February (the second month) is being evaluated and it is a leap year, an additional day is added. The function returns a tuple containing both the weekday of the first day and the total number of days in the month."
"def _monthlen(year, month):
    # Calculate the number of days in the given month of the specified year
    # Add 1 to February's days if it's a leap year
    return mdays[month] + (month == FEBRUARY and isleap(year))","The _monthlen function computes the number of days in a specified month for a given year. It uses a predefined list mdays, which contains the standard number of days for each month (e.g., 31 for January, 28 for February, etc.). The function adds an extra day to February's count if the year is a leap year, as determined by the isleap function. This adjustment ensures that February has 29 days in leap years. The function returns the total number of days for the specified month, accounting for leap years when applicable."
"def _prevmonth(year, month):
    # Check if the current month is January
    if month == 1:
        # If it is January, return the previous year and December as the previous month
        return year - 1, 12
    else:
        # Otherwise, return the same year and the previous month
        return year, month - 1","The _prevmonth function determines the previous month and its corresponding year for a given year and month. If the specified month is January (represented by 1), the function returns December (represented by 12) of the previous year. This accounts for the transition from one calendar year to another. For all other months, the function simply returns the same year with the month decremented by one. This utility function is useful for navigating dates and calculating periods that involve moving backward in time from a given date."
"def _nextmonth(year, month):
    # Check if the current month is December
    if month == 12:
        # If it is December, return the next year and January as the next month
        return year + 1, 1
    else:
        # Otherwise, return the same year and the next month
        return year, month + 1","The _nextmonth function calculates the subsequent month and its corresponding year for a given year and month. If the specified month is December (represented by 12), the function returns January (represented by 1) of the following year. This handles the transition from one calendar year to the next. For all other months, the function simply returns the same year with the month incremented by one. This function is useful for date calculations that require moving forward in time from a given date."
"def splitdoc(doc):
    """"""Split a doc string into a synopsis line (if any) and the rest.""""""
    # Strip leading and trailing whitespace and split the docstring into lines
    lines = doc.strip().split('\n')
    # Check if the docstring consists of only one line
    if len(lines) == 1:
        # Return the single line as the synopsis and an empty string for the rest
        return lines[0], ''
    # Check if there are at least two lines and the second line is empty
    elif len(lines) >= 2 and not lines[1].rstrip():
        # Return the first line as the synopsis and join the rest from the third line
        return lines[0], '\n'.join(lines[2:])
    # If there is no clear synopsis, return an empty synopsis and the entire docstring
    return '', '\n'.join(lines)","The splitdoc function processes a given docstring by dividing it into two parts: a synopsis line and the remaining content. It begins by removing any leading or trailing whitespace from the doc string and splits it into individual lines. If the docstring contains only one line, that line is returned as the synopsis, with an empty string for the rest. If there are multiple lines and the second line is blank, it considers the first line as the synopsis, returning it along with the subsequent lines (from the third onward) as additional content. In cases where no clear synopsis can be determined, it returns an empty string for the synopsis and includes all lines as additional content. This function is useful for extracting concise summaries from longer documentation strings."
"def replace(text, *pairs):
    """"""Do a series of global replacements on a string.""""""
    # Iterate over the pairs of old and new substrings
    while pairs:
        # Replace all occurrences of the old substring (pairs[0]) with the new substring (pairs[1])
        text = pairs[1].join(text.split(pairs[0]))
        # Move to the next pair of substrings by slicing the list
        pairs = pairs[2:]
    # Return the modified text after all replacements are done
    return text","The replace function performs a series of global replacements on a given string text. It takes an arbitrary number of arguments in pairs, where each pair consists of an ""old"" substring and a ""new"" substring. The function iterates through these pairs, replacing every occurrence of the ""old"" substring with the ""new"" substring throughout the entire text. This is achieved by splitting the text at each occurrence of the ""old"" substring and then joining it back together using the ""new"" substring. The function processes each pair sequentially, updating the text with each replacement before moving to the next pair. This method allows for multiple replacements in one function call, effectively transforming the input string based on specified patterns."
"def pathdirs():
    """"""Convert sys.path into a list of absolute, existing, unique paths.""""""
    # Initialize lists to store directories and their normalized versions
    dirs = []
    normdirs = []
    # Iterate over each directory in sys.path
    for dir in sys.path:
        # Convert the directory to an absolute path, using '.' if it is empty
        dir = os.path.abspath(dir or '.')
        # Normalize the directory path for case-insensitive comparison
        normdir = os.path.normcase(dir)
        # Check if the normalized directory is not already in the list and if it exists
        if normdir not in normdirs and os.path.isdir(dir):
            # Add the directory to the list of unique directories
            dirs.append(dir)
            # Add the normalized directory to the list to track uniqueness
            normdirs.append(normdir)
    # Return the list of unique, existing directories
    return dirs","The pathdirs function processes sys.path, which is a list of directories that Python searches for modules, to produce a list of absolute, existing, and unique directory paths. It initializes two lists: dirs for storing the final directory paths and normdirs for storing normalized versions of these paths to ensure case-insensitive uniqueness. The function iterates over each entry in sys.path, converting each to an absolute path using os.path.abspath. If a directory string is empty, it defaults to the current directory ('.'). It then normalizes each path using os.path.normcase to handle case differences across filesystems. The function checks if each normalized path is not already in normdirs and if it corresponds to an existing directory (os.path.isdir). If both conditions are met, it adds the path to dirs and its normalized counterpart to normdirs. Finally, it returns the list of unique, existing directories. This function ensures that only valid and distinct paths are included, which can be useful for module searching or configuration tasks."
"def _findclass(func):
    # Retrieve the module object where the function is defined
    cls = sys.modules.get(func.__module__)
    # Return None if the module is not found
    if cls is None:
        return None
    # Traverse the qualified name of the function to locate the class
    for name in func.__qualname__.split('.')[:-1]:
        cls = getattr(cls, name)
    # Check if the final object is a class, return None if not
    if not inspect.isclass(cls):
        return None","The _findclass function attempts to locate and return the class object to which a given function func belongs. It begins by retrieving the module object from sys.modules using the function's __module__ attribute. If the module cannot be found, it returns None. The function then traverses through the components of the function's __qualname__, which includes the fully qualified name of the function, split by periods. This traversal excludes the last part, which is the function's own name, allowing it to navigate through nested classes or modules to reach the class definition. For each component in this path, it uses getattr to access attributes of the current object (initially set to the module). Finally, it checks whether the resulting object is indeed a class using inspect.isclass. If it is not a class, it returns None. This utility is useful for introspection tasks where identifying a function's enclosing class is needed."
"def _getowndoc(obj):
    """"""Get the documentation string for an object if it is not
    inherited from its class.""""""
    try:
        # Retrieve the __doc__ attribute of the object
        doc = object.__getattribute__(obj, '__doc__')
        # Return None if the docstring is None
        if doc is None:
            return None
        # Check if the object is not a type object
        if obj is not type:
            # Retrieve the docstring of the object's type
            typedoc = type(obj).__doc__
            # Compare the object's docstring with its type's docstring
            # Return None if they are identical, indicating inheritance
            if isinstance(typedoc, str) and typedoc == doc:
                return None
        # Return the object's own docstring if it is not inherited
        return doc
    except AttributeError:
        # Return None if there is an AttributeError accessing __doc__
        return None","The _getowndoc function retrieves the documentation string (docstring) for an object obj, but only if this docstring is not inherited from the object's class. It attempts to access the __doc__ attribute of obj using object.__getattribute__. If no docstring exists (None), it returns None. For objects that are not of type type, it further checks whether the object's docstring matches that of its class (obtained via type(obj).__doc__). If they are identical, suggesting that the docstring is inherited, it returns None. Otherwise, it returns the object's own docstring. This function is useful for distinguishing between original documentation provided at an instance level and inherited documentation from a class level, aiding in more precise documentation analysis or display."
"def _getdoc(object):
    """"""Get the documentation string for an object.

    All tabs are expanded to spaces. To clean up docstrings that are
    indented to line up with blocks of code, any whitespace that can be
    uniformly removed from the second line onwards is removed.""""""
    # Attempt to retrieve the object's own docstring
    doc = _getowndoc(object)
    # If the object's own docstring is not found, try to find a docstring elsewhere
    if doc is None:
        try:
            doc = _finddoc(object)
        except (AttributeError, TypeError):
            return None
    # Ensure the retrieved docstring is a string
    if not isinstance(doc, str):
        return None
    # Clean up the docstring by expanding tabs and removing uniform indentation
    return inspect.cleandoc(doc)","The _getdoc function retrieves and processes the documentation string (docstring) for a given object. It first attempts to get the object's own docstring using _getowndoc. If this fails (returns None), it tries to find a relevant docstring using _finddoc, handling potential AttributeError or TypeError exceptions by returning None. Once a docstring is obtained, it checks whether it is a string. If it is not, the function returns None. For valid string docstrings, it uses inspect.cleandoc to clean up the text: tabs are expanded into spaces, and any uniform indentation from the second line onwards is removed. This ensures that the returned documentation is neatly formatted and free of unnecessary whitespace, making it more readable and suitable for display or further processing."
"def getdoc(object):
    """"""Get the doc string or comments for an object.""""""
    # Attempt to retrieve the documentation string using _getdoc
    result = _getdoc(object) or inspect.getcomments(object)
    # Clean up the result by removing leading empty lines and trailing whitespace
    return result and re.sub('^ *\n', '', result.rstrip()) or ''","The getdoc function retrieves documentation for a given object, either from its docstring or from comments associated with it. It first attempts to obtain the docstring using _getdoc. If _getdoc returns None, it falls back on using inspect.getcomments to retrieve comments that precede the object's definition in the source code. The function then processes the resulting text by removing any leading empty lines and trimming trailing whitespace using a regular expression with re.sub. This ensures that the returned documentation is clean and free of unnecessary formatting artifacts. If no documentation is found, it returns an empty string. This function is useful for extracting human-readable documentation from Python objects, whether it is embedded as a docstring or provided as comments."
"def isdata(object):
    """"""Check if an object is of a type that probably means it's data.""""""
    # Return True if the object is not a module, class, routine, frame, traceback, or code object
    return not (inspect.ismodule(object) or inspect.isclass(object) or
                inspect.isroutine(object) or inspect.isframe(object) or
                inspect.istraceback(object) or inspect.iscode(object))","The isdata function determines whether a given object is likely to be data rather than a structural or executable component of Python code. It uses the inspect module to check if the object is any of the following types: module, class, routine (function or method), frame, traceback, or code object. If the object does not match any of these types, the function returns True, indicating that it is probably data. Otherwise, it returns False. This function helps distinguish between objects that represent data (such as numbers, strings, lists, and dictionaries) and those that represent code structure or execution elements in Python programs."
"def cram(text, maxlen):
    """"""Omit part of a string if needed to make it fit in a maximum length.""""""
    # Check if the text length exceeds the maximum allowed length
    if len(text) > maxlen:
        # Calculate the number of characters to keep at the start of the text
        pre = max(0, (maxlen - 3) // 2)
        # Calculate the number of characters to keep at the end of the text
        post = max(0, maxlen - 3 - pre)
        # Return the truncated text with ellipsis in the middle
        return text[:pre] + '...' + text[len(text) - post:]
    # Return the original text if it fits within the maximum length
    return text","The cram function shortens a given text to fit within a specified maximum length (maxlen), inserting an ellipsis (...) in the middle if truncation is necessary. If the text exceeds maxlen, it calculates how many characters to retain from both the start and end of the string, ensuring that together with the ellipsis, they do not exceed maxlen. The number of characters retained from the start (pre) and end (post) are computed such that they balance around the ellipsis. This results in a string that preserves as much context as possible from both ends while clearly indicating truncation. If the text is already within maxlen, it is returned unchanged. This function is particularly useful for displaying shortened versions of strings in user interfaces or logs where space is limited."
"def _is_bound_method(fn):
    """"""
    Returns True if fn is a bound method, regardless of whether
    fn was implemented in Python or in C.
    """"""
    # Check if the function is a Python method
    if inspect.ismethod(fn):
        return True
    # Check if the function is a built-in method
    if inspect.isbuiltin(fn):
        # Retrieve the __self__ attribute to determine if it's bound
        self = getattr(fn, '__self__', None)
        # Return True if __self__ is not a module and is not None, indicating it's bound
        return not (inspect.ismodule(self) or (self is None))
    # Return False if the function is neither a method nor a bound built-in
    return False","The _is_bound_method function checks whether a given function fn is a bound method. A bound method is one that is associated with an instance of a class, meaning it has access to the instance's attributes and methods. The function first checks if fn is a Python method using inspect.ismethod, returning True if it is. If not, it checks if fn is a built-in method using inspect.isbuiltin. For built-in methods, it examines the __self__ attribute to determine if the method is bound. If __self__ exists and is not a module or None, this indicates that the method is bound to an instance, and the function returns True. Otherwise, it returns False. This utility helps in distinguishing between bound and unbound methods, which can be crucial for introspection and dynamic behavior analysis in Python applications."
"def ispackage(path):
    """"""Guess whether a path refers to a package directory.""""""
    # Issue a deprecation warning indicating that this function is deprecated
    warnings.warn('The pydoc.ispackage() function is deprecated',
                  DeprecationWarning, stacklevel=2)
    # Check if the given path is a directory
    if os.path.isdir(path):
        # Check for the presence of an __init__ file with .py or .pyc extension
        for ext in ('.py', '.pyc'):
            if os.path.isfile(os.path.join(path, '__init__' + ext)):
                # Return True if an __init__ file is found, indicating a package
                return True
    # Return False if no __init__ file is found or if the path is not a directory
    return False","The ispackage function attempts to determine whether a given path refers to a Python package directory. A package directory typically contains an __init__.py file, which can be either a source file (.py) or a compiled bytecode file (.pyc). The function first issues a deprecation warning using warnings.warn, indicating that this functionality is deprecated and may be removed in future versions. It then checks if the specified path is a directory using os.path.isdir. If it is, the function looks for the presence of an __init__ file with either .py or .pyc extension within that directory using os.path.isfile. If such a file is found, it returns True, suggesting that the directory is indeed a package. If no __init__ file is found or if the path is not a directory, it returns False. This function helps identify directories that are structured as Python packages."
"def source_synopsis(file):
    # Read the first line of the file
    line = file.readline()
    # Skip lines that are comments or empty
    while line[:1] == '#' or not line.strip():
        line = file.readline()
        if not line:
            break
    # Strip leading and trailing whitespace from the line
    line = line.strip()
    # Handle raw docstring starting with r""""""
    if line[:4] == 'r""""""':
        line = line[1:]
    # Check if the line starts with a docstring delimiter """"""
    if line[:3] == '""""""':
        # Remove the starting """"""
        line = line[3:]
        # Remove a trailing backslash if present
        if line[-1:] == '\\':
            line = line[:-1]
        # Continue reading lines until a non-empty one is found
        while not line.strip():
            line = file.readline()
            if not line:
                break
        # Extract the first part of the docstring before another """"""
        result = line.split('""""""')[0].strip()
    else:
        result = None
    return result","The source_synopsis function extracts a brief synopsis from the beginning of a source file, typically from its docstring. It reads lines from the provided file object, skipping over any lines that are comments (starting with #) or are empty. Once it finds a non-comment, non-empty line, it checks for a docstring, which starts with """""". If the docstring is prefixed with r"""""", indicating a raw string, it adjusts accordingly. The function then removes the initial """""" and any trailing backslash. It continues reading until it finds content within the docstring, capturing text up to another """""". This captured text is returned as the synopsis. If no valid docstring is found, it returns None. This function is useful for quickly summarizing what a Python file does by extracting its top-level documentation comment."
"def importfile(path):
    """"""Import a Python source file or compiled file given its path.""""""
    # Retrieve the magic number used to identify bytecode files
    magic = importlib.util.MAGIC_NUMBER
    # Open the file in binary read mode to check if it's a bytecode file
    with open(path, 'rb') as file:
        is_bytecode = magic == file.read(len(magic))
    # Extract the filename and its extension from the given path
    filename = os.path.basename(path)
    name, ext = os.path.splitext(filename)
    # Choose the appropriate loader based on whether the file is bytecode
    if is_bytecode:
        loader = importlib._bootstrap_external.SourcelessFileLoader(name, path)
    else:
        loader = importlib._bootstrap_external.SourceFileLoader(name, path)
    # Create a module specification from the file location and loader
    spec = importlib.util.spec_from_file_location(name, path, loader=loader)
    try:
        # Load and return the module using the specification
        return importlib._bootstrap._load(spec)
    except BaseException as err:
        # Raise a custom error if there is an issue during import
        raise ErrorDuringImport(path, err)","The importfile function imports a Python module from a specified file path, handling both source files (.py) and compiled bytecode files (.pyc). It begins by checking if the file is a bytecode file by comparing its initial bytes with Python's MAGIC_NUMBER, which identifies compiled files. It reads these bytes in binary mode. Based on this check, it selects either SourcelessFileLoader for bytecode or SourceFileLoader for source files to load the module. The function constructs a module specification using importlib.util.spec_from_file_location, which includes the module's name and path along with the chosen loader. It then attempts to load the module using this specification via importlib._bootstrap._load. If any error occurs during this process, it raises an ErrorDuringImport, providing details about the path and error encountered. This function facilitates dynamic loading of Python modules from arbitrary paths, useful in scenarios such as plugins or script execution environments."
